{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bba52-982a-4cd1-bc36-6db68e47553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA = Path(\"data/raw\")\n",
    "  # adjust if your notebook sits in notebooks/\n",
    "\n",
    "hi_small_trans = pd.read_csv(DATA/\"HI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "hi_small_accts = pd.read_csv(DATA/\"HI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "\n",
    "print(hi_small_trans.shape, hi_small_accts.shape)\n",
    "display(hi_small_trans.head(3))\n",
    "display(hi_small_accts.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca5f79-7937-4a7c-8bfe-d3bdcacac2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/raw\")  # or ../data/raw if notebook is inside notebooks/\n",
    "print(\"Path exists:\", DATA.exists())\n",
    "print(\"Files:\", list(DATA.glob(\"*.csv\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b2ad0-1539-4a77-9f3e-e32d476058f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA = Path(\"data/raw\")      # <-- if notebook is under notebooks/\n",
    "SAVE = Path(\"data/interim\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA exists:\", DATA.exists())\n",
    "print(\"CSV files:\", sorted([p.name for p in DATA.glob(\"*.csv\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf037f-6b53-4c1c-bcc4-6c2665324988",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_opts = dict(engine=\"pyarrow\")\n",
    "\n",
    "datasets = {\n",
    "    \"HI_Small_Trans\": DATA/\"HI-Small_Trans.csv\",\n",
    "    \"HI_Small_Accts\": DATA/\"HI-Small_accounts.csv\",\n",
    "    \"LI_Small_Trans\": DATA/\"LI-Small_Trans.csv\",\n",
    "    \"LI_Small_Accts\": DATA/\"LI-Small_accounts.csv\",\n",
    "    \"HI_Med_Trans\":   DATA/\"HI-Medium_Trans.csv\",\n",
    "    \"HI_Med_Accts\":   DATA/\"HI-Medium_accounts.csv\",\n",
    "    \"LI_Med_Trans\":   DATA/\"LI-Medium_Trans.csv\",\n",
    "    \"LI_Med_Accts\":   DATA/\"LI-Medium_accounts.csv\",\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for name, path in datasets.items():\n",
    "    print(f\"Loading {name} ...\")\n",
    "    df = pd.read_csv(path, **read_opts)\n",
    "    dfs[name] = df\n",
    "    print(f\"  → {df.shape[0]:,} rows × {df.shape[1]} cols\")\n",
    "print(\"✅ loaded all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a44b70-5ade-4bfa-8b17-ae42a1f4403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/raw\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def csv_to_parquet_chunked(SRC, DST, chunksize=1_000_000, usecols=None, parse_dates=None):\n",
    "    \"\"\"Stream a big CSV to a single Parquet file with minimal memory use.\"\"\"\n",
    "    first = True\n",
    "    pq_parts = []\n",
    "    for i, chunk in enumerate(pd.read_csv(SRC, chunksize=chunksize, usecols=usecols,\n",
    "                                          parse_dates=parse_dates, low_memory=True)):\n",
    "        # optional downcasting (shrinks memory a lot)\n",
    "        for col in chunk.select_dtypes(include=\"float\").columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast=\"float\")\n",
    "        for col in chunk.select_dtypes(include=\"integer\").columns:\n",
    "            chunk[col] = pd.to_numeric(chunk[col], downcast=\"integer\")\n",
    "\n",
    "        part_path = SAVE / f\"{DST.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(part_path, index=False)   # write part\n",
    "        pq_parts.append(part_path)\n",
    "\n",
    "    # stitch parts into one (optional – or keep partitioned)\n",
    "    # If you’re fine with multiple parts, skip this stitching step.\n",
    "    if len(pq_parts) > 1:\n",
    "        import pyarrow as pa, pyarrow.parquet as pq\n",
    "        tables = [pq.read_table(p) for p in pq_parts]\n",
    "        table = pa.concat_tables(tables)\n",
    "        pq.write_table(table, DST)\n",
    "        for p in pq_parts: p.unlink()  # remove parts\n",
    "    elif len(pq_parts) == 1:\n",
    "        pq_parts[0].rename(DST)\n",
    "\n",
    "# === Choose columns you actually need (speeds up and reduces RAM) ===\n",
    "USECOLS_TRANS = None  # or e.g. [\"Timestamp\",\"From Bank\",\"From Account\",\"To Bank\",\"To Account\",\"Amount Paid\"]\n",
    "USECOLS_ACCTS = None  # or e.g. [\"Account ID\",\"Entity ID\",\"Entity Name\",\"Bank Name\"]\n",
    "\n",
    "# HI-Medium\n",
    "csv_to_parquet_chunked(\n",
    "    SRC=DATA/\"HI-Medium_Trans.csv\",\n",
    "    DST=SAVE/\"HI_Medium_Trans.parquet\",\n",
    "    chunksize=1_000_000,\n",
    "    usecols=USECOLS_TRANS,\n",
    "    parse_dates=None  # set to your timestamp column if you already know it\n",
    ")\n",
    "csv_to_parquet_chunked(\n",
    "    SRC=DATA/\"HI-Medium_accounts.csv\",\n",
    "    DST=SAVE/\"HI_Medium_accounts.parquet\",\n",
    "    chunksize=500_000,\n",
    "    usecols=USECOLS_ACCTS\n",
    ")\n",
    "\n",
    "# LI-Medium (repeat when ready)\n",
    "# csv_to_parquet_chunked(DATA/\"LI-Medium_Trans.csv\", SAVE/\"LI_Medium_Trans.parquet\", chunksize=1_000_000, usecols=USECOLS_TRANS)\n",
    "# csv_to_parquet_chunked(DATA/\"LI-Medium_accounts.csv\", SAVE/\"LI_Medium_accounts.parquet\", chunksize=500_000, usecols=USECOLS_ACCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afdfe2-a803-4e14-ace2-53fcfa190c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_med_t = pd.read_parquet(\"../data/interim/HI_Medium_Trans.parquet\")\n",
    "hi_med_a = pd.read_parquet(\"../data/interim/HI_Medium_accounts.parquet\")\n",
    "print(hi_med_t.shape, hi_med_a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1a39b-5231-4d9a-b037-5fa4bb0441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INT = Path(\"../data/interim\")\n",
    "print(\"Exists:\", INT.exists())\n",
    "print(\"Contents:\", [f.name for f in INT.glob(\"*\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08aa7b3-bfb4-42b5-ad1f-7311a599d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "DATA = Path(\"data/raw\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def csv_to_parquet_chunked(SRC, DST, chunksize=1_000_000):\n",
    "    pq_parts = []\n",
    "    for i, chunk in enumerate(pd.read_csv(SRC, chunksize=chunksize, low_memory=True)):\n",
    "        print(f\"Chunk {i+1} ...\", end=\" \")\n",
    "        part_path = SAVE / f\"{DST.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(part_path, index=False)\n",
    "        pq_parts.append(part_path)\n",
    "        print(\"saved\")\n",
    "\n",
    "    if len(pq_parts) > 1:\n",
    "        tables = [pq.read_table(p) for p in pq_parts]\n",
    "        pq.write_table(pa.concat_tables(tables), DST)\n",
    "        for p in pq_parts: p.unlink()\n",
    "    else:\n",
    "        pq_parts[0].rename(DST)\n",
    "    print(\"✅ Done:\", DST.name)\n",
    "\n",
    "# Convert only HI-Medium\n",
    "csv_to_parquet_chunked(DATA/\"HI-Medium_Trans.csv\", SAVE/\"HI_Medium_Trans.parquet\")\n",
    "csv_to_parquet_chunked(DATA/\"HI-Medium_accounts.csv\", SAVE/\"HI_Medium_accounts.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f062de-b53b-49c3-8e2c-c3c41bd3fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f.name for f in Path(\"../data/interim\").glob(\"*.parquet\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e7c55-b611-4b79-b2ae-ffb13d07aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "print([f.name for f in Path(\"data/interim\").glob(\"*.parquet\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5311b29-c7a0-4623-92d7-b712828272e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_med_t = pd.read_parquet(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "hi_med_a = pd.read_parquet(\"data/interim/HI_Medium_accounts.parquet\")\n",
    "print(hi_med_t.shape, hi_med_a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0bbdb-7acb-49c4-8029-718c240bce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_report(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"memory (MB):\", round(df.memory_usage(deep=True).sum()/1e6, 2))\n",
    "    print(\"columns:\", list(df.columns))\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"missing ratio (top 10):\")\n",
    "    display(miss.head(10))\n",
    "    display(df.head(3))\n",
    "\n",
    "for k in [\"HI_Small_Trans\",\"HI_Small_Accts\",\"LI_Small_Trans\",\"LI_Small_Accts\"]:\n",
    "    quick_report(dfs[k], k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdb508-c443-4579-9572-554e05be69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_med_t\n",
    "hi_med_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc35e75-0bac-454e-8208-ee83454fcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_report(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"memory (MB):\", round(df.memory_usage(deep=True).sum()/1e6, 2))\n",
    "    print(\"columns:\", list(df.columns))\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"missing ratio (top 10):\")\n",
    "    display(miss.head(10))\n",
    "    display(df.head(3))\n",
    "\n",
    "# Run quick reports for your loaded Medium sets\n",
    "quick_report(hi_med_t, \"HI-Medium Transactions\")\n",
    "quick_report(hi_med_a, \"HI-Medium Accounts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cd12e-0d25-40c5-8292-3b1cd83110bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df):\n",
    "    cols = df.columns\n",
    "    candidates = {\n",
    "        \"tx_id\": [c for c in cols if \"trans\" in c.lower() and \"id\" in c.lower()] + \\\n",
    "                 [c for c in cols if c.lower() in {\"txid\",\"tx_id\",\"id\"}],\n",
    "        \"acct_id\": [c for c in cols if (\"acct\" in c.lower() or \"account\" in c.lower()) and \"id\" in c.lower()],\n",
    "        \"time\": [c for c in cols if (\"time\" in c.lower()) or (\"date\" in c.lower())],\n",
    "        \"amount\": [c for c in cols if (\"amount\" in c.lower()) or (c.lower() in {\"amt\",\"value\",\"transactionamount\"})],\n",
    "        \"label\": [c for c in cols if c.lower() in {\"is_illicit\",\"isfraud\",\"fraud\",\"label\",\"y\",\"target\"}],\n",
    "    }\n",
    "    return {k: v[:3] for k,v in candidates.items()}  # show top few\n",
    "\n",
    "for k in [\"HI_Small_Trans\",\"HI_Small_Accts\"]:\n",
    "    print(k, detect_columns(dfs[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33a667-571f-462e-a10e-6b78579f60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Quick report helper\n",
    "def quick_report(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"memory (MB):\", round(df.memory_usage(deep=True).sum()/1e6, 2))\n",
    "    print(\"columns:\", list(df.columns))\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"missing ratio (top 10):\")\n",
    "    display(miss.head(10))\n",
    "    display(df.head(3))\n",
    "\n",
    "\n",
    "# --- Reload all four pairs if needed ---\n",
    "# (Skip this section if they're already loaded in memory)\n",
    "DATA = Path(\"data/raw\")\n",
    "hi_small_trans = pd.read_csv(DATA/\"HI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "hi_small_accts = pd.read_csv(DATA/\"HI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "li_small_trans = pd.read_csv(DATA/\"LI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "li_small_accts = pd.read_csv(DATA/\"LI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "\n",
    "hi_med_t = pd.read_parquet(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "hi_med_a = pd.read_parquet(\"data/interim/HI_Medium_accounts.parquet\")\n",
    "# (If you later convert LI-Medium, you can add them too)\n",
    "\n",
    "\n",
    "# --- Run quick reports for all four ---\n",
    "quick_report(hi_small_trans, \"HI-Small Transactions\")\n",
    "quick_report(hi_small_accts, \"HI-Small Accounts\")\n",
    "quick_report(li_small_trans, \"LI-Small Transactions\")\n",
    "quick_report(li_small_accts, \"LI-Small Accounts\")\n",
    "quick_report(hi_med_t, \"HI-Medium Transactions\")\n",
    "quick_report(hi_med_a, \"HI-Medium Accounts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfedaa1-722f-43ba-bbd8-c089a5099dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df):\n",
    "    cols = df.columns\n",
    "    candidates = {\n",
    "        \"tx_id\": [c for c in cols if \"trans\" in c.lower() and \"id\" in c.lower()] + \\\n",
    "                 [c for c in cols if c.lower() in {\"txid\",\"tx_id\",\"id\"}],\n",
    "        \"acct_id\": [c for c in cols if (\"acct\" in c.lower() or \"account\" in c.lower()) and \"id\" in c.lower()],\n",
    "        \"time\": [c for c in cols if (\"time\" in c.lower()) or (\"date\" in c.lower())],\n",
    "        \"amount\": [c for c in cols if (\"amount\" in c.lower()) or (c.lower() in {\"amt\",\"value\",\"transactionamount\"})],\n",
    "        \"label\": [c for c in cols if c.lower() in {\"is_illicit\",\"isfraud\",\"fraud\",\"label\",\"y\",\"target\"}],\n",
    "    }\n",
    "    return {k: v[:3] for k,v in candidates.items()}\n",
    "\n",
    "# Run on one sample to check naming conventions\n",
    "print(\"HI-Small Transactions:\", detect_columns(hi_small_trans))\n",
    "print(\"HI-Small Accounts:\", detect_columns(hi_small_accts))\n",
    "print(\"LI-Small Transactions:\", detect_columns(li_small_trans))\n",
    "print(\"LI-Small Accounts:\", detect_columns(li_small_accts))\n",
    "print(\"HI-Medium Transactions:\", detect_columns(hi_med_t))\n",
    "print(\"HI-Medium Accounts:\", detect_columns(hi_med_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf4340-1a6c-41dd-bc25-a25b8b561f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Define helper function\n",
    "def quick_report(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Memory (MB):\", round(df.memory_usage(deep=True).sum() / 1_000_000, 2))\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    \n",
    "    # Missing value ratios\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"Missing ratio (top 10):\")\n",
    "    display(miss.head(10))\n",
    "    \n",
    "    # Show a quick preview\n",
    "    display(df.head(3))\n",
    "\n",
    "\n",
    "# --- Reload if needed (skip if already loaded) ---\n",
    "from pathlib import Path\n",
    "DATA = Path(\"data/raw\")\n",
    "INT = Path(\"data/interim\")\n",
    "\n",
    "# Uncomment if you need to reload\n",
    "# hi_small_trans = pd.read_csv(DATA/\"HI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "# hi_small_accts = pd.read_csv(DATA/\"HI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "# li_small_trans = pd.read_csv(DATA/\"LI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "# li_small_accts = pd.read_csv(DATA/\"LI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "# hi_med_t = pd.read_parquet(INT/\"HI_Medium_Trans.parquet\")\n",
    "# hi_med_a = pd.read_parquet(INT/\"HI_Medium_accounts.parquet\")\n",
    "\n",
    "# --- Run quick reports for all ---\n",
    "quick_report(hi_small_trans, \"HI-Small Transactions\")\n",
    "quick_report(hi_small_accts, \"HI-Small Accounts\")\n",
    "quick_report(li_small_trans, \"LI-Small Transactions\")\n",
    "quick_report(li_small_accts, \"LI-Small Accounts\")\n",
    "quick_report(hi_med_t, \"HI-Medium Transactions\")\n",
    "quick_report(hi_med_a, \"HI-Medium Accounts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28327e2-db12-4c87-bf78-24e39796a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df):\n",
    "    cols = df.columns\n",
    "    candidates = {\n",
    "        \"tx_id\": [c for c in cols if \"trans\" in c.lower() and \"id\" in c.lower()] + \\\n",
    "                 [c for c in cols if c.lower() in {\"txid\",\"tx_id\",\"id\"}],\n",
    "        \"acct_id\": [c for c in cols if (\"acct\" in c.lower() or \"account\" in c.lower()) and \"id\" in c.lower()],\n",
    "        \"time\": [c for c in cols if (\"time\" in c.lower()) or (\"date\" in c.lower())],\n",
    "        \"amount\": [c for c in cols if (\"amount\" in c.lower()) or (c.lower() in {\"amt\",\"value\",\"transactionamount\"})],\n",
    "        \"label\": [c for c in cols if c.lower() in {\"is_illicit\",\"isfraud\",\"fraud\",\"label\",\"y\",\"target\"}],\n",
    "    }\n",
    "    return {k: v[:3] for k,v in candidates.items()}\n",
    "\n",
    "# Run on one sample to check naming conventions\n",
    "print(\"HI-Small Transactions:\", detect_columns(hi_small_trans))\n",
    "print(\"HI-Small Accounts:\", detect_columns(hi_small_accts))\n",
    "print(\"LI-Small Transactions:\", detect_columns(li_small_trans))\n",
    "print(\"LI-Small Accounts:\", detect_columns(li_small_accts))\n",
    "print(\"HI-Medium Transactions:\", detect_columns(hi_med_t))\n",
    "print(\"HI-Medium Accounts:\", detect_columns(hi_med_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93075d77-e702-492a-8fb2-8e1f317f2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_ID   = \"Transaction ID\"      # change if your output differs\n",
    "ACCT_ID = \"Account ID\"          # e.g., \"Acct_ID\" or similar\n",
    "DATECOL = \"Timestamp\"           # e.g., \"Date\", \"Transaction Date\"\n",
    "AMOUNT  = \"Amount Paid\"         # e.g., \"Amount\", \"Transaction Amount\"\n",
    "LABEL   = None                  # set automatically if a label column exists later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff968ccd-cb15-49ec-83c1-f3bf462ec39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_t = hi_small_trans.copy()\n",
    "\n",
    "if DATECOL in hi_t.columns:\n",
    "    hi_t[DATECOL] = pd.to_datetime(hi_t[DATECOL], errors=\"coerce\")\n",
    "if TX_ID in hi_t.columns:\n",
    "    hi_t[TX_ID] = hi_t[TX_ID].astype(str)\n",
    "if ACCT_ID in hi_t.columns:\n",
    "    hi_t[ACCT_ID] = hi_t[ACCT_ID].astype(str)\n",
    "if AMOUNT in hi_t.columns:\n",
    "    hi_t[AMOUNT] = pd.to_numeric(hi_t[AMOUNT], errors=\"coerce\")\n",
    "\n",
    "# detect label if exists\n",
    "for c in hi_t.columns:\n",
    "    if c.lower() in {\"is_illicit\",\"isfraud\",\"fraud\",\"label\",\"y\",\"target\"}:\n",
    "        LABEL = c\n",
    "        break\n",
    "\n",
    "print(\"Using columns →\", dict(TX_ID=TX_ID, ACCT_ID=ACCT_ID, DATECOL=DATECOL, AMOUNT=AMOUNT, LABEL=LABEL))\n",
    "quick_report(hi_t, \"HI-Small Trans (typed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5464b-72b6-4e66-9993-2c8449db6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_a = hi_small_accts.copy()\n",
    "\n",
    "if ACCT_ID in hi_a.columns:\n",
    "    hi_a[ACCT_ID] = hi_a[ACCT_ID].astype(str)\n",
    "\n",
    "# Prefix account columns to avoid name collisions\n",
    "acct_pref = hi_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={f\"acct_{ACCT_ID}\": ACCT_ID})\n",
    "\n",
    "# Merge on account id\n",
    "hi = hi_t.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(hi, \"HI-Small merged (Trans + Accounts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371e680-43c0-4802-9563-bca617dca680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HI-Small Transactions columns:\")\n",
    "print(list(hi_t.columns))\n",
    "\n",
    "print(\"\\nHI-Small Accounts columns:\")\n",
    "print(list(hi_a.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa82fb-7b85-4141-96b3-1370f23763a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCT_ID = \"Acct ID\"         # change this to match your actual column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d0078-414e-42c4-8cf9-fc9cb6e4ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ACCT_ID in hi_a.columns:\n",
    "    hi_a[ACCT_ID] = hi_a[ACCT_ID].astype(str)\n",
    "\n",
    "acct_pref = hi_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={f\"acct_{ACCT_ID}\": ACCT_ID})\n",
    "\n",
    "hi = hi_t.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(hi, \"HI-Small merged (Trans + Accounts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99204c7-224b-4147-9c53-259130eef99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HI-Small Transactions columns:\")\n",
    "print(list(hi_t.columns))\n",
    "\n",
    "print(\"\\nHI-Small Accounts columns:\")\n",
    "print(list(hi_a.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb16b2c-9857-4edb-b2ed-5a720e1b026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix duplicate column names\n",
    "hi_t.columns = ['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "                'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "                'Payment Currency', 'Payment Format', 'Is Laundering']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e6cf4-4c4a-4805-afa4-4bd921b74a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_ID   = None                   # not present in this dataset, we’ll create one\n",
    "ACCT_ID = \"From Account\"         # matches sender\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT  = \"Amount Paid\"\n",
    "LABEL   = \"Is Laundering\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d677243-7f09-4555-a1ea-3e9150030855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transaction ID\n",
    "hi_t[\"Transaction ID\"] = range(1, len(hi_t) + 1)\n",
    "TX_ID = \"Transaction ID\"\n",
    "\n",
    "# Convert dtypes\n",
    "hi_t[DATECOL] = pd.to_datetime(hi_t[DATECOL], errors=\"coerce\")\n",
    "hi_t[AMOUNT] = pd.to_numeric(hi_t[AMOUNT], errors=\"coerce\")\n",
    "hi_t[LABEL] = hi_t[LABEL].astype(int)\n",
    "\n",
    "# Cast join column to str\n",
    "hi_t[ACCT_ID] = hi_t[ACCT_ID].astype(str)\n",
    "hi_a[\"Account Number\"] = hi_a[\"Account Number\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d7682-ec3e-48d1-9f9e-b3a9ac4464dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_pref = hi_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "hi = hi_t.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(hi, \"HI-Small merged (Trans + Accounts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f0e5a-5b07-4120-8a00-2de73d04e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = Path(\"data/interim\")\n",
    "hi_clean = hi.copy()\n",
    "hi_clean.to_parquet(SAVE/\"HI_Small_merged.parquet\", index=False)\n",
    "print(\"✅ Saved:\", (SAVE/\"HI_Small_merged.parquet\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a83885-35e5-4d78-b94b-b2840a159782",
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_ID   = \"Transaction ID\"   # synthetic we created earlier\n",
    "ACCT_ID = \"From Account\"\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT  = \"Amount Paid\"\n",
    "LABEL   = \"Is Laundering\"\n",
    "SAVE = Path(\"data/interim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29187a9a-6663-4cd6-a6f3-c8059ec1e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload LI-Small if not already loaded\n",
    "DATA = Path(\"data/raw\")\n",
    "li_small_trans = pd.read_csv(DATA/\"LI-Small_Trans.csv\", engine=\"pyarrow\")\n",
    "li_small_accts = pd.read_csv(DATA/\"LI-Small_accounts.csv\", engine=\"pyarrow\")\n",
    "\n",
    "# Fix duplicate column names (same issue as HI-Small)\n",
    "li_small_trans.columns = ['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "                          'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "                          'Payment Currency', 'Payment Format', 'Is Laundering']\n",
    "\n",
    "# Add synthetic transaction ID\n",
    "li_small_trans[\"Transaction ID\"] = range(1, len(li_small_trans) + 1)\n",
    "\n",
    "# Type conversions\n",
    "li_small_trans[DATECOL] = pd.to_datetime(li_small_trans[DATECOL], errors=\"coerce\")\n",
    "li_small_trans[AMOUNT] = pd.to_numeric(li_small_trans[AMOUNT], errors=\"coerce\")\n",
    "li_small_trans[LABEL] = li_small_trans[LABEL].astype(int)\n",
    "li_small_trans[ACCT_ID] = li_small_trans[ACCT_ID].astype(str)\n",
    "li_small_accts[\"Account Number\"] = li_small_accts[\"Account Number\"].astype(str)\n",
    "\n",
    "# Prefix account columns\n",
    "acct_pref = li_small_accts.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "# Merge\n",
    "li = li_small_trans.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(li, \"LI-Small merged\")\n",
    "\n",
    "# Save\n",
    "li.to_parquet(SAVE/\"LI_Small_merged.parquet\", index=False)\n",
    "print(\"✅ Saved:\", (SAVE/\"LI_Small_merged.parquet\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94e728-0b5a-4724-aa72-a01b97a92b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_med_t = pd.read_parquet(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "hi_med_a = pd.read_parquet(\"data/interim/HI_Medium_accounts.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b21be1-0a94-494c-8599-1ea3845ef3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names (they are the same structure)\n",
    "hi_med_t.columns = ['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "                    'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "                    'Payment Currency', 'Payment Format', 'Is Laundering']\n",
    "\n",
    "hi_med_t[\"Transaction ID\"] = range(1, len(hi_med_t) + 1)\n",
    "\n",
    "hi_med_t[DATECOL] = pd.to_datetime(hi_med_t[DATECOL], errors=\"coerce\")\n",
    "hi_med_t[AMOUNT] = pd.to_numeric(hi_med_t[AMOUNT], errors=\"coerce\")\n",
    "hi_med_t[LABEL] = hi_med_t[LABEL].astype(int)\n",
    "hi_med_t[ACCT_ID] = hi_med_t[ACCT_ID].astype(str)\n",
    "hi_med_a[\"Account Number\"] = hi_med_a[\"Account Number\"].astype(str)\n",
    "\n",
    "acct_pref = hi_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "hi_med = hi_med_t.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(hi_med, \"HI-Medium merged\")\n",
    "\n",
    "hi_med.to_parquet(SAVE/\"HI_Medium_merged.parquet\", index=False)\n",
    "print(\"✅ Saved:\", (SAVE/\"HI_Medium_merged.parquet\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6b3212-f87f-42f5-a7aa-6f3cdf7458f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "# Paths\n",
    "DATA = Path(\"data/raw\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Standard column references (we’ll reuse across all)\n",
    "TX_ID   = \"Transaction ID\"\n",
    "ACCT_ID = \"From Account\"\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT  = \"Amount Paid\"\n",
    "LABEL   = \"Is Laundering\"\n",
    "\n",
    "# Quick report helper\n",
    "def quick_report(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"memory (MB):\", round(df.memory_usage(deep=True).sum()/1e6, 2))\n",
    "    print(\"columns:\", list(df.columns))\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"missing ratio (top 10):\")\n",
    "    display(miss.head(10))\n",
    "    display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c4bedd-1e77-4cde-9df9-b4b75720bf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (31898238, 11) (2087786, 5)\n"
     ]
    }
   ],
   "source": [
    "hi_med_t = pd.read_parquet(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "hi_med_a = pd.read_parquet(\"data/interim/HI_Medium_accounts.parquet\")\n",
    "\n",
    "print(\"Loaded:\", hi_med_t.shape, hi_med_a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34999a63-6789-4093-ae51-92b3fb62029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_med_t.columns = [\n",
    "    'Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "    'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "    'Payment Currency', 'Payment Format', 'Is Laundering'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be130472-1abf-478e-8646-2e41d4a3eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synthetic transaction ID\n",
    "hi_med_t[\"Transaction ID\"] = range(1, len(hi_med_t) + 1)\n",
    "\n",
    "# Type conversions\n",
    "hi_med_t[DATECOL] = pd.to_datetime(hi_med_t[DATECOL], errors=\"coerce\")\n",
    "hi_med_t[AMOUNT] = pd.to_numeric(hi_med_t[AMOUNT], errors=\"coerce\")\n",
    "hi_med_t[LABEL] = hi_med_t[LABEL].astype(int)\n",
    "hi_med_t[ACCT_ID] = hi_med_t[ACCT_ID].astype(str)\n",
    "hi_med_a[\"Account Number\"] = hi_med_a[\"Account Number\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f46d705-4fad-4c7b-8ece-0c2dfd1915ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.19 GiB for an array with shape (5, 31898510) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m acct_pref = hi_med_a.add_prefix(\u001b[33m\"\u001b[39m\u001b[33macct_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m acct_pref = acct_pref.rename(columns={\u001b[33m\"\u001b[39m\u001b[33macct_Account Number\u001b[39m\u001b[33m\"\u001b[39m: ACCT_ID})\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m hi_med = \u001b[43mhi_med_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43macct_pref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mACCT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m quick_report(hi_med, \u001b[33m\"\u001b[39m\u001b[33mHI-Medium merged\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m hi_med.to_parquet(SAVE/\u001b[33m\"\u001b[39m\u001b[33mHI_Medium_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10859\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10840\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10841\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10842\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10855\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m ) -> DataFrame:\n\u001b[32m  10857\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10860\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10868\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10869\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10873\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    877\u001b[39m left.columns = llabels\n\u001b[32m    878\u001b[39m right.columns = rlabels\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:623\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    620\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1807\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1802\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1809\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2288\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2286\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2287\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2288\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2289\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2291\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2320\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2317\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2319\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2321\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2323\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.19 GiB for an array with shape (5, 31898510) and data type object"
     ]
    }
   ],
   "source": [
    "acct_pref = hi_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "hi_med = hi_med_t.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "quick_report(hi_med, \"HI-Medium merged\")\n",
    "\n",
    "hi_med.to_parquet(SAVE/\"HI_Medium_merged.parquet\", index=False)\n",
    "print(\"✅ Saved:\", (SAVE/\"HI_Medium_merged.parquet\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d373ef72-0837-49b6-9115-dfb11bf90c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_table() got an unexpected keyword argument 'chunksize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Open a ParquetWriter (efficient appending)\u001b[39;00m\n\u001b[32m     14\u001b[39m writer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi_med_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔹 Merging chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     chunk[ACCT_ID] = chunk[ACCT_ID].astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[31mTypeError\u001b[39m: read_table() got an unexpected keyword argument 'chunksize'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# We’ll read transactions in chunks of ~1M rows and merge iteratively\n",
    "chunk_size = 1_000_000\n",
    "SRC = \"data/interim/HI_Medium_Trans.parquet\"\n",
    "DST = SAVE / \"HI_Medium_merged.parquet\"\n",
    "\n",
    "# Preprocess accounts only once\n",
    "acct_pref = hi_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "acct_pref[ACCT_ID] = acct_pref[ACCT_ID].astype(str)\n",
    "\n",
    "# Open a ParquetWriter (efficient appending)\n",
    "writer = None\n",
    "for i, chunk in enumerate(pd.read_parquet(SRC, engine=\"pyarrow\", columns=hi_med_t.columns, use_threads=True, chunksize=chunk_size)):\n",
    "    print(f\"🔹 Merging chunk {i+1} ...\")\n",
    "\n",
    "    chunk[ACCT_ID] = chunk[ACCT_ID].astype(str)\n",
    "    merged = chunk.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    # Write incrementally\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(DST, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    # Free memory\n",
    "    del chunk, merged, table\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ Done: {DST.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4356fd42-9712-4b83-bb0d-7dd8f0e37e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Processing batch 1 ...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'From Account'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'From Account'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Convert Arrow batch to pandas\u001b[39;00m\n\u001b[32m     32\u001b[39m chunk = batch.to_pandas()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m chunk[ACCT_ID] = \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mACCT_ID\u001b[49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Merge with accounts\u001b[39;00m\n\u001b[32m     36\u001b[39m merged = chunk.merge(acct_pref, on=ACCT_ID, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'From Account'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "ACCT_ID = \"From Account\"\n",
    "\n",
    "# Load accounts (small)\n",
    "acct_pref = hi_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "acct_pref[ACCT_ID] = acct_pref[ACCT_ID].astype(str)\n",
    "\n",
    "# Source parquet for transactions\n",
    "SRC = DATA / \"HI_Medium_Trans.parquet\"\n",
    "DST = SAVE / \"HI_Medium_merged.parquet\"\n",
    "\n",
    "# Create PyArrow dataset to stream over row groups\n",
    "dataset = ds.dataset(SRC, format=\"parquet\")\n",
    "\n",
    "writer = None\n",
    "chunk_counter = 0\n",
    "\n",
    "for batch in dataset.to_batches():\n",
    "    chunk_counter += 1\n",
    "    print(f\"🔹 Processing batch {chunk_counter} ...\")\n",
    "    \n",
    "    # Convert Arrow batch to pandas\n",
    "    chunk = batch.to_pandas()\n",
    "    chunk[ACCT_ID] = chunk[ACCT_ID].astype(str)\n",
    "\n",
    "    # Merge with accounts\n",
    "    merged = chunk.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    # Write out incrementally\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(DST, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    # Free memory explicitly\n",
    "    del chunk, merged, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ Done safely: {DST.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66917ac-1454-4bf6-b5bc-0565b32fc833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format', 'Is Laundering']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "tbl = pq.read_table(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "print(tbl.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7800f12d-1c7a-447b-af38-a5d134e028de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns fixed and parquet updated:\n",
      "['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format', 'Is Laundering']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the medium transactions parquet\n",
    "hi_med_t = pd.read_parquet(\"data/interim/HI_Medium_Trans.parquet\")\n",
    "\n",
    "# Rename duplicate Account columns to match the small datasets\n",
    "hi_med_t.columns = [\n",
    "    'Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "    'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "    'Payment Currency', 'Payment Format', 'Is Laundering'\n",
    "]\n",
    "\n",
    "# Save back the fixed version\n",
    "hi_med_t.to_parquet(\"data/interim/HI_Medium_Trans.parquet\", index=False)\n",
    "\n",
    "print(\"✅ Columns fixed and parquet updated:\")\n",
    "print(list(hi_med_t.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c345cf-f0bb-4d68-bed6-db4b8beb166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Processing batch 1 ...\n",
      "🔹 Processing batch 2 ...\n",
      "🔹 Processing batch 3 ...\n",
      "🔹 Processing batch 4 ...\n",
      "🔹 Processing batch 5 ...\n",
      "🔹 Processing batch 6 ...\n",
      "🔹 Processing batch 7 ...\n",
      "🔹 Processing batch 8 ...\n",
      "🔹 Processing batch 9 ...\n",
      "🔹 Processing batch 10 ...\n",
      "🔹 Processing batch 11 ...\n",
      "🔹 Processing batch 12 ...\n",
      "🔹 Processing batch 13 ...\n",
      "🔹 Processing batch 14 ...\n",
      "🔹 Processing batch 15 ...\n",
      "🔹 Processing batch 16 ...\n",
      "🔹 Processing batch 17 ...\n",
      "🔹 Processing batch 18 ...\n",
      "🔹 Processing batch 19 ...\n",
      "🔹 Processing batch 20 ...\n",
      "🔹 Processing batch 21 ...\n",
      "🔹 Processing batch 22 ...\n",
      "🔹 Processing batch 23 ...\n",
      "🔹 Processing batch 24 ...\n",
      "🔹 Processing batch 25 ...\n",
      "🔹 Processing batch 26 ...\n",
      "🔹 Processing batch 27 ...\n",
      "🔹 Processing batch 28 ...\n",
      "🔹 Processing batch 29 ...\n",
      "🔹 Processing batch 30 ...\n",
      "🔹 Processing batch 31 ...\n",
      "🔹 Processing batch 32 ...\n",
      "🔹 Processing batch 33 ...\n",
      "🔹 Processing batch 34 ...\n",
      "🔹 Processing batch 35 ...\n",
      "🔹 Processing batch 36 ...\n",
      "🔹 Processing batch 37 ...\n",
      "🔹 Processing batch 38 ...\n",
      "🔹 Processing batch 39 ...\n",
      "🔹 Processing batch 40 ...\n",
      "🔹 Processing batch 41 ...\n",
      "🔹 Processing batch 42 ...\n",
      "🔹 Processing batch 43 ...\n",
      "🔹 Processing batch 44 ...\n",
      "🔹 Processing batch 45 ...\n",
      "🔹 Processing batch 46 ...\n",
      "🔹 Processing batch 47 ...\n",
      "🔹 Processing batch 48 ...\n",
      "🔹 Processing batch 49 ...\n",
      "🔹 Processing batch 50 ...\n",
      "🔹 Processing batch 51 ...\n",
      "🔹 Processing batch 52 ...\n",
      "🔹 Processing batch 53 ...\n",
      "🔹 Processing batch 54 ...\n",
      "🔹 Processing batch 55 ...\n",
      "🔹 Processing batch 56 ...\n",
      "🔹 Processing batch 57 ...\n",
      "🔹 Processing batch 58 ...\n",
      "🔹 Processing batch 59 ...\n",
      "🔹 Processing batch 60 ...\n",
      "🔹 Processing batch 61 ...\n",
      "🔹 Processing batch 62 ...\n",
      "🔹 Processing batch 63 ...\n",
      "🔹 Processing batch 64 ...\n",
      "🔹 Processing batch 65 ...\n",
      "🔹 Processing batch 66 ...\n",
      "🔹 Processing batch 67 ...\n",
      "🔹 Processing batch 68 ...\n",
      "🔹 Processing batch 69 ...\n",
      "🔹 Processing batch 70 ...\n",
      "🔹 Processing batch 71 ...\n",
      "🔹 Processing batch 72 ...\n",
      "🔹 Processing batch 73 ...\n",
      "🔹 Processing batch 74 ...\n",
      "🔹 Processing batch 75 ...\n",
      "🔹 Processing batch 76 ...\n",
      "🔹 Processing batch 77 ...\n",
      "🔹 Processing batch 78 ...\n",
      "🔹 Processing batch 79 ...\n",
      "🔹 Processing batch 80 ...\n",
      "🔹 Processing batch 81 ...\n",
      "🔹 Processing batch 82 ...\n",
      "🔹 Processing batch 83 ...\n",
      "🔹 Processing batch 84 ...\n",
      "🔹 Processing batch 85 ...\n",
      "🔹 Processing batch 86 ...\n",
      "🔹 Processing batch 87 ...\n",
      "🔹 Processing batch 88 ...\n",
      "🔹 Processing batch 89 ...\n",
      "🔹 Processing batch 90 ...\n",
      "🔹 Processing batch 91 ...\n",
      "🔹 Processing batch 92 ...\n",
      "🔹 Processing batch 93 ...\n",
      "🔹 Processing batch 94 ...\n",
      "🔹 Processing batch 95 ...\n",
      "🔹 Processing batch 96 ...\n",
      "🔹 Processing batch 97 ...\n",
      "🔹 Processing batch 98 ...\n",
      "🔹 Processing batch 99 ...\n",
      "🔹 Processing batch 100 ...\n",
      "🔹 Processing batch 101 ...\n",
      "🔹 Processing batch 102 ...\n",
      "🔹 Processing batch 103 ...\n",
      "🔹 Processing batch 104 ...\n",
      "🔹 Processing batch 105 ...\n",
      "🔹 Processing batch 106 ...\n",
      "🔹 Processing batch 107 ...\n",
      "🔹 Processing batch 108 ...\n",
      "🔹 Processing batch 109 ...\n",
      "🔹 Processing batch 110 ...\n",
      "🔹 Processing batch 111 ...\n",
      "🔹 Processing batch 112 ...\n",
      "🔹 Processing batch 113 ...\n",
      "🔹 Processing batch 114 ...\n",
      "🔹 Processing batch 115 ...\n",
      "🔹 Processing batch 116 ...\n",
      "🔹 Processing batch 117 ...\n",
      "🔹 Processing batch 118 ...\n",
      "🔹 Processing batch 119 ...\n",
      "🔹 Processing batch 120 ...\n",
      "🔹 Processing batch 121 ...\n",
      "🔹 Processing batch 122 ...\n",
      "🔹 Processing batch 123 ...\n",
      "🔹 Processing batch 124 ...\n",
      "🔹 Processing batch 125 ...\n",
      "🔹 Processing batch 126 ...\n",
      "🔹 Processing batch 127 ...\n",
      "🔹 Processing batch 128 ...\n",
      "🔹 Processing batch 129 ...\n",
      "🔹 Processing batch 130 ...\n",
      "🔹 Processing batch 131 ...\n",
      "🔹 Processing batch 132 ...\n",
      "🔹 Processing batch 133 ...\n",
      "🔹 Processing batch 134 ...\n",
      "🔹 Processing batch 135 ...\n",
      "🔹 Processing batch 136 ...\n",
      "🔹 Processing batch 137 ...\n",
      "🔹 Processing batch 138 ...\n",
      "🔹 Processing batch 139 ...\n",
      "🔹 Processing batch 140 ...\n",
      "🔹 Processing batch 141 ...\n",
      "🔹 Processing batch 142 ...\n",
      "🔹 Processing batch 143 ...\n",
      "🔹 Processing batch 144 ...\n",
      "🔹 Processing batch 145 ...\n",
      "🔹 Processing batch 146 ...\n",
      "🔹 Processing batch 147 ...\n",
      "🔹 Processing batch 148 ...\n",
      "🔹 Processing batch 149 ...\n",
      "🔹 Processing batch 150 ...\n",
      "🔹 Processing batch 151 ...\n",
      "🔹 Processing batch 152 ...\n",
      "🔹 Processing batch 153 ...\n",
      "🔹 Processing batch 154 ...\n",
      "🔹 Processing batch 155 ...\n",
      "🔹 Processing batch 156 ...\n",
      "🔹 Processing batch 157 ...\n",
      "🔹 Processing batch 158 ...\n",
      "🔹 Processing batch 159 ...\n",
      "🔹 Processing batch 160 ...\n",
      "🔹 Processing batch 161 ...\n",
      "🔹 Processing batch 162 ...\n",
      "🔹 Processing batch 163 ...\n",
      "🔹 Processing batch 164 ...\n",
      "🔹 Processing batch 165 ...\n",
      "🔹 Processing batch 166 ...\n",
      "🔹 Processing batch 167 ...\n",
      "🔹 Processing batch 168 ...\n",
      "🔹 Processing batch 169 ...\n",
      "🔹 Processing batch 170 ...\n",
      "🔹 Processing batch 171 ...\n",
      "🔹 Processing batch 172 ...\n",
      "🔹 Processing batch 173 ...\n",
      "🔹 Processing batch 174 ...\n",
      "🔹 Processing batch 175 ...\n",
      "🔹 Processing batch 176 ...\n",
      "🔹 Processing batch 177 ...\n",
      "🔹 Processing batch 178 ...\n",
      "🔹 Processing batch 179 ...\n",
      "🔹 Processing batch 180 ...\n",
      "🔹 Processing batch 181 ...\n",
      "🔹 Processing batch 182 ...\n",
      "🔹 Processing batch 183 ...\n",
      "🔹 Processing batch 184 ...\n",
      "🔹 Processing batch 185 ...\n",
      "🔹 Processing batch 186 ...\n",
      "🔹 Processing batch 187 ...\n",
      "🔹 Processing batch 188 ...\n",
      "🔹 Processing batch 189 ...\n",
      "🔹 Processing batch 190 ...\n",
      "🔹 Processing batch 191 ...\n",
      "🔹 Processing batch 192 ...\n",
      "🔹 Processing batch 193 ...\n",
      "🔹 Processing batch 194 ...\n",
      "🔹 Processing batch 195 ...\n",
      "🔹 Processing batch 196 ...\n",
      "🔹 Processing batch 197 ...\n",
      "🔹 Processing batch 198 ...\n",
      "🔹 Processing batch 199 ...\n",
      "🔹 Processing batch 200 ...\n",
      "🔹 Processing batch 201 ...\n",
      "🔹 Processing batch 202 ...\n",
      "🔹 Processing batch 203 ...\n",
      "🔹 Processing batch 204 ...\n",
      "🔹 Processing batch 205 ...\n",
      "🔹 Processing batch 206 ...\n",
      "🔹 Processing batch 207 ...\n",
      "🔹 Processing batch 208 ...\n",
      "🔹 Processing batch 209 ...\n",
      "🔹 Processing batch 210 ...\n",
      "🔹 Processing batch 211 ...\n",
      "🔹 Processing batch 212 ...\n",
      "🔹 Processing batch 213 ...\n",
      "🔹 Processing batch 214 ...\n",
      "🔹 Processing batch 215 ...\n",
      "🔹 Processing batch 216 ...\n",
      "🔹 Processing batch 217 ...\n",
      "🔹 Processing batch 218 ...\n",
      "🔹 Processing batch 219 ...\n",
      "🔹 Processing batch 220 ...\n",
      "🔹 Processing batch 221 ...\n",
      "🔹 Processing batch 222 ...\n",
      "🔹 Processing batch 223 ...\n",
      "🔹 Processing batch 224 ...\n",
      "🔹 Processing batch 225 ...\n",
      "🔹 Processing batch 226 ...\n",
      "🔹 Processing batch 227 ...\n",
      "🔹 Processing batch 228 ...\n",
      "🔹 Processing batch 229 ...\n",
      "🔹 Processing batch 230 ...\n",
      "🔹 Processing batch 231 ...\n",
      "🔹 Processing batch 232 ...\n",
      "🔹 Processing batch 233 ...\n",
      "🔹 Processing batch 234 ...\n",
      "🔹 Processing batch 235 ...\n",
      "🔹 Processing batch 236 ...\n",
      "🔹 Processing batch 237 ...\n",
      "🔹 Processing batch 238 ...\n",
      "🔹 Processing batch 239 ...\n",
      "🔹 Processing batch 240 ...\n",
      "🔹 Processing batch 241 ...\n",
      "🔹 Processing batch 242 ...\n",
      "🔹 Processing batch 243 ...\n",
      "🔹 Processing batch 244 ...\n",
      "✅ Done safely: data/interim/HI_Medium_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "ACCT_ID = \"From Account\"\n",
    "\n",
    "# Load accounts parquet\n",
    "hi_med_a = pd.read_parquet(\"data/interim/HI_Medium_accounts.parquet\")\n",
    "acct_pref = hi_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "acct_pref[ACCT_ID] = acct_pref[ACCT_ID].astype(str)\n",
    "\n",
    "SRC = DATA / \"HI_Medium_Trans.parquet\"\n",
    "DST = SAVE / \"HI_Medium_merged.parquet\"\n",
    "\n",
    "dataset = ds.dataset(SRC, format=\"parquet\")\n",
    "\n",
    "writer = None\n",
    "chunk_counter = 0\n",
    "\n",
    "for batch in dataset.to_batches():\n",
    "    chunk_counter += 1\n",
    "    print(f\"🔹 Processing batch {chunk_counter} ...\")\n",
    "    \n",
    "    chunk = batch.to_pandas()\n",
    "    chunk[ACCT_ID] = chunk[ACCT_ID].astype(str)\n",
    "\n",
    "    merged = chunk.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(DST, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    del chunk, merged, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ Done safely: {DST.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b581575-ad83-48a2-89c0-063dff8a2059",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\interim\\\\LI_Medium_accounts.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m LABEL = \u001b[33m\"\u001b[39m\u001b[33mIs Laundering\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# === Step 1: Load accounts (small) ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m li_med_a = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLI_Medium_accounts.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m acct_pref = li_med_a.add_prefix(\u001b[33m\"\u001b[39m\u001b[33macct_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m acct_pref = acct_pref.rename(columns={\u001b[33m\"\u001b[39m\u001b[33macct_Account Number\u001b[39m\u001b[33m\"\u001b[39m: ACCT_ID})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\interim\\\\LI_Medium_accounts.parquet'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths and parameters ===\n",
    "DATA = Path(\"data/interim\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "ACCT_ID = \"From Account\"\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT = \"Amount Paid\"\n",
    "LABEL = \"Is Laundering\"\n",
    "\n",
    "# === Step 1: Load accounts (small) ===\n",
    "li_med_a = pd.read_parquet(DATA / \"LI_Medium_accounts.parquet\")\n",
    "acct_pref = li_med_a.add_prefix(\"acct_\")\n",
    "acct_pref = acct_pref.rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "acct_pref[ACCT_ID] = acct_pref[ACCT_ID].astype(str)\n",
    "\n",
    "# === Step 2: Load and rename transaction parquet ===\n",
    "li_med_t = pd.read_parquet(DATA / \"LI_Medium_Trans.parquet\")\n",
    "\n",
    "# Fix column names if still generic\n",
    "li_med_t.columns = [\n",
    "    'Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account',\n",
    "    'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
    "    'Payment Currency', 'Payment Format', 'Is Laundering'\n",
    "]\n",
    "\n",
    "# Overwrite parquet to ensure consistent schema\n",
    "li_med_t.to_parquet(DATA / \"LI_Medium_Trans.parquet\", index=False)\n",
    "print(\"✅ Column names fixed and updated for LI-Medium transactions\")\n",
    "\n",
    "# === Step 3: Chunked merge ===\n",
    "SRC = DATA / \"LI_Medium_Trans.parquet\"\n",
    "DST = SAVE / \"LI_Medium_merged.parquet\"\n",
    "\n",
    "dataset = ds.dataset(SRC, format=\"parquet\")\n",
    "\n",
    "writer = None\n",
    "batch_num = 0\n",
    "\n",
    "for batch in dataset.to_batches():\n",
    "    batch_num += 1\n",
    "    print(f\"🔹 Processing batch {batch_num} ...\")\n",
    "\n",
    "    # Convert batch to pandas DataFrame\n",
    "    chunk = batch.to_pandas()\n",
    "    chunk[\"Transaction ID\"] = range(1, len(chunk) + 1)\n",
    "    chunk[DATECOL] = pd.to_datetime(chunk[DATECOL], errors=\"coerce\")\n",
    "    chunk[AMOUNT] = pd.to_numeric(chunk[AMOUNT], errors=\"coerce\")\n",
    "    chunk[LABEL] = chunk[LABEL].astype(int)\n",
    "    chunk[ACCT_ID] = chunk[ACCT_ID].astype(str)\n",
    "\n",
    "    # Merge with accounts\n",
    "    merged = chunk.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    # Write to Parquet incrementally\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(DST, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    del chunk, merged, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ Successfully merged and saved: {DST.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54cbc98-5901-411b-83b5-dc030a809ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 ... saved\n",
      "Chunk 2 ... saved\n",
      "Chunk 3 ... saved\n",
      "Chunk 4 ... saved\n",
      "Chunk 5 ... saved\n",
      "Chunk 6 ... saved\n",
      "Chunk 7 ... saved\n",
      "Chunk 8 ... saved\n",
      "Chunk 9 ... saved\n",
      "Chunk 10 ... saved\n",
      "Chunk 11 ... saved\n",
      "Chunk 12 ... saved\n",
      "Chunk 13 ... saved\n",
      "Chunk 14 ... saved\n",
      "Chunk 15 ... saved\n",
      "Chunk 16 ... saved\n",
      "Chunk 17 ... saved\n",
      "Chunk 18 ... saved\n",
      "Chunk 19 ... saved\n",
      "Chunk 20 ... saved\n",
      "Chunk 21 ... saved\n",
      "Chunk 22 ... saved\n",
      "Chunk 23 ... saved\n",
      "Chunk 24 ... saved\n",
      "Chunk 25 ... saved\n",
      "Chunk 26 ... saved\n",
      "Chunk 27 ... saved\n",
      "Chunk 28 ... saved\n",
      "Chunk 29 ... saved\n",
      "Chunk 30 ... saved\n",
      "Chunk 31 ... saved\n",
      "Chunk 32 ... saved\n",
      "✅ Done: LI_Medium_Trans.parquet\n",
      "Chunk 1 ... saved\n",
      "Chunk 2 ... saved\n",
      "Chunk 3 ... saved\n",
      "✅ Done: LI_Medium_accounts.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_RAW = Path(\"data/raw\")\n",
    "SAVE = Path(\"data/interim\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def csv_to_parquet_chunked(SRC, DST, chunksize=1_000_000):\n",
    "    parts = []\n",
    "    for i, chunk in enumerate(pd.read_csv(SRC, chunksize=chunksize, low_memory=True)):\n",
    "        print(f\"Chunk {i+1} ...\", end=\" \")\n",
    "        part_path = SAVE / f\"{DST.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(part_path, index=False)\n",
    "        parts.append(part_path)\n",
    "        print(\"saved\")\n",
    "\n",
    "    # Combine all small parts\n",
    "    if len(parts) > 1:\n",
    "        tables = [pq.read_table(p) for p in parts]\n",
    "        pq.write_table(pa.concat_tables(tables), DST)\n",
    "        for p in parts: p.unlink()\n",
    "    else:\n",
    "        parts[0].rename(DST)\n",
    "    print(\"✅ Done:\", DST.name)\n",
    "\n",
    "# Convert LI-Medium CSVs\n",
    "csv_to_parquet_chunked(DATA_RAW/\"LI-Medium_Trans.csv\", SAVE/\"LI_Medium_Trans.parquet\")\n",
    "csv_to_parquet_chunked(DATA_RAW/\"LI-Medium_accounts.csv\", SAVE/\"LI_Medium_accounts.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc508a95-f269-4cc6-a15a-c00683afeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small ===\n",
      "(5078395, 16)\n",
      "['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format']\n",
      "\n",
      "Amount summary for HI_Small:\n",
      "count    5.078395e+06\n",
      "mean     5.988667e+06\n",
      "std      1.037178e+09\n",
      "min      1.000000e-06\n",
      "1%       1.680976e-02\n",
      "5%       6.850000e+00\n",
      "50%      1.411000e+03\n",
      "95%      6.498872e+05\n",
      "99%      1.475123e+07\n",
      "max      1.046302e+12\n",
      "Name: Amount Received, dtype: float64\n",
      "\n",
      "Label distribution for HI_Small:\n",
      "Is Laundering\n",
      "0    0.998981\n",
      "1    0.001019\n",
      "Name: ratio, dtype: float64\n",
      "\n",
      "=== LI_Small ===\n",
      "(6924074, 16)\n",
      "['Timestamp', 'From Bank', 'From Account', 'To Bank', 'To Account', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format']\n",
      "\n",
      "Amount summary for LI_Small:\n",
      "count    6.924074e+06\n",
      "mean     6.324044e+06\n",
      "std      2.105367e+09\n",
      "min      1.000000e-06\n",
      "1%       9.488000e-03\n",
      "5%       2.950000e+00\n",
      "50%      1.397620e+03\n",
      "95%      6.235947e+05\n",
      "99%      1.336818e+07\n",
      "max      3.644854e+12\n",
      "Name: Amount Received, dtype: float64\n",
      "\n",
      "Label distribution for LI_Small:\n",
      "Is Laundering\n",
      "0    0.999485\n",
      "1    0.000515\n",
      "Name: ratio, dtype: float64\n",
      "\n",
      "=== HI_Medium ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "datasets = {\n",
    "    \"HI_Small\": DATA/\"HI_Small_merged.parquet\",\n",
    "    \"LI_Small\": DATA/\"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium\": DATA/\"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium\": DATA/\"LI_Medium_merged.parquet\",\n",
    "}\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    df = pd.read_parquet(path)\n",
    "    print(df.shape)\n",
    "    print(df.columns.tolist()[:10])  # show sample columns\n",
    "    \n",
    "    # Amount column\n",
    "    amt_col = next((c for c in df.columns if \"amount\" in c.lower()), None)\n",
    "    if amt_col:\n",
    "        print(f\"\\nAmount summary for {name}:\")\n",
    "        print(df[amt_col].describe(percentiles=[.01, .05, .5, .95, .99]))\n",
    "    \n",
    "    # Label column\n",
    "    label_col = next((c for c in df.columns if \"launder\" in c.lower() or \"label\" in c.lower()), None)\n",
    "    if label_col:\n",
    "        print(f\"\\nLabel distribution for {name}:\")\n",
    "        print(df[label_col].value_counts(normalize=True).rename(\"ratio\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd50d8dd-004a-487c-b9d9-e356c3be81c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HI_Medium_accounts.parquet', 'HI_Medium_merged.parquet', 'HI_Medium_Trans.parquet', 'HI_Small_merged.parquet', 'LI_Medium_accounts.parquet', 'LI_Medium_Trans.parquet', 'LI_Small_merged.parquet']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "DATA = Path(\"data/interim\")\n",
    "print([f.name for f in DATA.glob(\"*.parquet\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46ae760-d3f0-45bb-9f80-a581bb43091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 5,078,395\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998981\n",
      "1    0.001019\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 50 ⚠️\n",
      "\n",
      "=== LI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 6,924,074\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.999485\n",
      "1    0.000515\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 25 ⚠️\n",
      "\n",
      "=== HI_Medium ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 31,898,510\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998896\n",
      "1    0.001104\n",
      "Name: ratio, dtype: float64\n",
      "• Transaction ID column not found (OK if you skipped it on small sets)\n",
      "\n",
      "=== LI_Medium ===\n",
      "❌ MISSING: LI_Medium_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "# what we expect to have after Step 5\n",
    "targets = {\n",
    "    \"HI_Small\":  DATA/\"HI_Small_merged.parquet\",\n",
    "    \"LI_Small\":  DATA/\"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium\": DATA/\"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium\": DATA/\"LI_Medium_merged.parquet\",\n",
    "}\n",
    "\n",
    "def verify_merged(name, path: Path):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    if not path.exists():\n",
    "        print(\"❌ MISSING:\", path.name); return\n",
    "\n",
    "    # Read schema only (no data) to decide minimal columns to load\n",
    "    cols = pq.read_schema(path).names\n",
    "    req = [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"]\n",
    "    have = [c for c in req if c in cols]\n",
    "    acct_col = next((c for c in cols if c.startswith(\"acct_\")), None)\n",
    "    txid_col = \"Transaction ID\" if \"Transaction ID\" in cols else None\n",
    "\n",
    "    # Load only the needed columns\n",
    "    read_cols = [c for c in (have + ([acct_col] if acct_col else []) + ([txid_col] if txid_col else [])) if c]\n",
    "    df = pd.read_parquet(path, columns=read_cols)\n",
    "\n",
    "    # 1) required columns present?\n",
    "    missing = [c for c in req if c not in cols]\n",
    "    print(\"• Required columns present:\", \"✅\" if not missing else f\"❌ missing {missing}\")\n",
    "\n",
    "    # 2) basic shape (only for loaded cols)\n",
    "    total_rows = len(df)\n",
    "    print(f\"• Rows (sampled cols): {total_rows:,}\")\n",
    "\n",
    "    # 3) join coverage (did the accounts merge work?)\n",
    "    if acct_col:\n",
    "        join_cov = 1 - df[acct_col].isna().mean()\n",
    "        print(f\"• Accounts join coverage via {acct_col}: {join_cov:.2%} {'✅' if join_cov>0 else '⚠️'}\")\n",
    "    else:\n",
    "        print(\"• Accounts join coverage: ⚠️ no acct_* column found (merge may not have added account fields)\")\n",
    "\n",
    "    # 4) label sanity\n",
    "    if \"Is Laundering\" in df.columns:\n",
    "        ratios = df[\"Is Laundering\"].value_counts(normalize=True, dropna=False).rename(\"ratio\")\n",
    "        print(\"• Label ratio (0/1):\"); print(ratios)\n",
    "        if 0.0 in ratios.values or 1.0 in ratios.values:\n",
    "            print(\"  ⚠️ label looks degenerate; double-check\")\n",
    "    else:\n",
    "        print(\"• Label column missing\")\n",
    "\n",
    "    # 5) duplicate TX_ID check (if present)\n",
    "    if txid_col:\n",
    "        dups = df.duplicated(subset=[txid_col]).sum()\n",
    "        print(f\"• Duplicate '{txid_col}' count:\", dups, \"✅\" if dups==0 else \"⚠️\")\n",
    "    else:\n",
    "        print(\"• Transaction ID column not found (OK if you skipped it on small sets)\")\n",
    "\n",
    "for n,p in targets.items():\n",
    "    verify_merged(n, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8960492b-e3a2-4d8d-bca3-ac286116196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI_Small_merged: removed 50 duplicates → saved clean file ✅\n",
      "LI_Small_merged: removed 25 duplicates → saved clean file ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "for name in [\"HI_Small_merged\", \"LI_Small_merged\"]:\n",
    "    path = DATA / f\"{name}.parquet\"\n",
    "    df = pd.read_parquet(path)\n",
    "    before = len(df)\n",
    "    if \"Transaction ID\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"Transaction ID\"], keep=\"first\")\n",
    "    after = len(df)\n",
    "    df.to_parquet(path, index=False)\n",
    "    print(f\"{name}: removed {before - after} duplicates → saved clean file ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5284ea-c693-4a6f-8f9b-ebec9a2ef8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 715. MiB for an array with shape (3, 31251483) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(expected) != \u001b[38;5;28mset\u001b[39m(li_med_t.columns):\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# common raw header shape: ['Timestamp','From Bank','Account','To Bank','Account.1',...]\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAccount\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m li_med_t.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAccount.1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m li_med_t.columns:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         li_med_t = \u001b[43mli_med_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAccount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFrom Account\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAccount.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTo Account\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# also normalize exact expected order (safe if names match)\u001b[39;00m\n\u001b[32m     72\u001b[39m     li_med_t = li_med_t[[\n\u001b[32m     73\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mFrom Bank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mFrom Account\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mTo Bank\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mTo Account\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     74\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAmount Received\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mReceiving Currency\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mAmount Paid\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     75\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mPayment Currency\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mPayment Format\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mIs Laundering\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     76\u001b[39m     ]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5789\u001b[39m, in \u001b[36mDataFrame.rename\u001b[39m\u001b[34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[39m\n\u001b[32m   5658\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrename\u001b[39m(\n\u001b[32m   5659\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5660\u001b[39m     mapper: Renamer | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5668\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5669\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5670\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5671\u001b[39m \u001b[33;03m    Rename columns or index labels.\u001b[39;00m\n\u001b[32m   5672\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5787\u001b[39m \u001b[33;03m    4  3  6\u001b[39;00m\n\u001b[32m   5788\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_rename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5790\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5791\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5793\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5795\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5797\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5798\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:1108\u001b[39m, in \u001b[36mNDFrame._rename\u001b[39m\u001b[34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[39m\n\u001b[32m   1105\u001b[39m         index = mapper\n\u001b[32m   1107\u001b[39m \u001b[38;5;28mself\u001b[39m._check_inplace_and_allows_duplicate_labels(inplace)\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m result = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis_no, replacements \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m((index, columns)):\n\u001b[32m   1111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m replacements \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6833\u001b[39m, in \u001b[36mNDFrame.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6684\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   6685\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Self:\n\u001b[32m   6686\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6687\u001b[39m \u001b[33;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[32m   6688\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6831\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   6832\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6833\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6834\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n\u001b[32m   6835\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6836\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6837\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:612\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    610\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m res.axes = new_axes\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    616\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    820\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 715. MiB for an array with shape (3, 31251483) and data type int64"
     ]
    }
   ],
   "source": [
    "# === Build LI_Medium_merged.parquet (memory-safe, end-to-end) ===\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ---- Paths & constants ----\n",
    "RAW  = Path(\"data/raw\")\n",
    "INT  = Path(\"data/interim\")\n",
    "INT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACCT_ID = \"From Account\"\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT  = \"Amount Paid\"\n",
    "LABEL   = \"Is Laundering\"\n",
    "\n",
    "# ---- Helper: CSV -> Parquet (chunked) if parquet missing ----\n",
    "def csv_to_parquet_chunked(src_csv: Path, dst_parquet: Path, chunksize=1_000_000):\n",
    "    parts = []\n",
    "    for i, chunk in enumerate(pd.read_csv(src_csv, chunksize=chunksize, low_memory=True)):\n",
    "        part = dst_parquet.with_name(f\"{dst_parquet.stem}_part{i:03d}.parquet\")\n",
    "        chunk.to_parquet(part, index=False)\n",
    "        parts.append(part)\n",
    "        print(f\"  • saved part {i+1}\")\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        parts[0].rename(dst_parquet)\n",
    "    else:\n",
    "        tables = [pq.read_table(p) for p in parts]\n",
    "        pq.write_table(pa.concat_tables(tables), dst_parquet)\n",
    "        for p in parts: p.unlink()\n",
    "    print(f\"✅ combined → {dst_parquet.name}\")\n",
    "\n",
    "# ---- Ensure LI-Medium base parquets exist ----\n",
    "li_accts_parq = INT / \"LI_Medium_accounts.parquet\"\n",
    "li_trans_parq = INT / \"LI_Medium_Trans.parquet\"\n",
    "\n",
    "if not li_accts_parq.exists():\n",
    "    print(\"Converting LI-Medium_accounts.csv → parquet ...\")\n",
    "    csv_to_parquet_chunked(RAW/\"LI-Medium_accounts.csv\", li_accts_parq)\n",
    "\n",
    "if not li_trans_parq.exists():\n",
    "    print(\"Converting LI-Medium_Trans.csv → parquet ...\")\n",
    "    csv_to_parquet_chunked(RAW/\"LI-Medium_Trans.csv\", li_trans_parq)\n",
    "\n",
    "# ---- Load & standardize schema (transactions & accounts) ----\n",
    "# Accounts (small)\n",
    "li_med_a = pd.read_parquet(li_accts_parq)\n",
    "# ensure string join key\n",
    "if \"Account Number\" not in li_med_a.columns:\n",
    "    raise RuntimeError(\"Expected column 'Account Number' in LI_Medium_accounts.parquet\")\n",
    "\n",
    "li_med_a[\"Account Number\"] = li_med_a[\"Account Number\"].astype(str)\n",
    "acct_pref = li_med_a.add_prefix(\"acct_\").rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "# Transactions (may have generic 'Account' columns → fix)\n",
    "li_med_t = pd.read_parquet(li_trans_parq)\n",
    "# If two \"Account\" columns exist, rename them; otherwise keep if already correct\n",
    "expected = ['Timestamp','From Bank','From Account','To Bank','To Account',\n",
    "            'Amount Received','Receiving Currency','Amount Paid',\n",
    "            'Payment Currency','Payment Format','Is Laundering']\n",
    "if set(expected) != set(li_med_t.columns):\n",
    "    # common raw header shape: ['Timestamp','From Bank','Account','To Bank','Account.1',...]\n",
    "    if \"Account\" in li_med_t.columns and \"Account.1\" in li_med_t.columns:\n",
    "        li_med_t = li_med_t.rename(columns={\n",
    "            \"Account\": \"From Account\",\n",
    "            \"Account.1\": \"To Account\"\n",
    "        })\n",
    "    # also normalize exact expected order (safe if names match)\n",
    "    li_med_t = li_med_t[[\n",
    "        'Timestamp','From Bank','From Account','To Bank','To Account',\n",
    "        'Amount Received','Receiving Currency','Amount Paid',\n",
    "        'Payment Currency','Payment Format','Is Laundering'\n",
    "    ]]\n",
    "    # overwrite parquet so downstream batches see the fixed schema\n",
    "    li_med_t.to_parquet(li_trans_parq, index=False)\n",
    "    print(\"✅ normalized LI_Medium_Trans.parquet column names\")\n",
    "\n",
    "# ---- Chunked merge: stream row-groups to keep RAM low ----\n",
    "dst = INT / \"LI_Medium_merged.parquet\"\n",
    "dataset = ds.dataset(li_trans_parq, format=\"parquet\")\n",
    "\n",
    "writer = None\n",
    "batch_no = 0\n",
    "for batch in dataset.to_batches():       # streams parquet row-groups\n",
    "    batch_no += 1\n",
    "    print(f\"🔹 processing batch {batch_no} ...\")\n",
    "    chunk = batch.to_pandas()\n",
    "\n",
    "    # minimal typing & helper columns\n",
    "    chunk[\"Transaction ID\"] = range(1, len(chunk)+1)\n",
    "    chunk[DATECOL] = pd.to_datetime(chunk[DATECOL], errors=\"coerce\")\n",
    "    chunk[AMOUNT]  = pd.to_numeric(chunk[AMOUNT], errors=\"coerce\")\n",
    "    chunk[LABEL]   = chunk[LABEL].astype(int)\n",
    "    chunk[ACCT_ID] = chunk[ACCT_ID].astype(str)\n",
    "\n",
    "    merged = chunk.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(dst, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    # free memory each loop\n",
    "    del chunk, merged, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ finished: {dst.as_posix()}\")\n",
    "\n",
    "# ---- Quick verification (reads only needed columns) ----\n",
    "schema = pq.read_schema(dst).names\n",
    "take = [c for c in [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"] if c in schema]\n",
    "acct_col = next((c for c in schema if c.startswith(\"acct_\")), None)\n",
    "txid_col = \"Transaction ID\" if \"Transaction ID\" in schema else None\n",
    "read_cols = [*take, *( [acct_col] if acct_col else [] ), *( [txid_col] if txid_col else [] )]\n",
    "\n",
    "df_chk = pd.read_parquet(dst, columns=read_cols)\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(\"rows:\", len(df_chk))\n",
    "print(\"required columns present:\",\n",
    "      all(c in schema for c in [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"]))\n",
    "if acct_col:\n",
    "    print(\"accounts join coverage:\",\n",
    "          f\"{(1 - df_chk[acct_col].isna().mean()):.2%}\")\n",
    "if txid_col:\n",
    "    print(\"duplicate TX_IDs:\", df_chk.duplicated(subset=[txid_col]).sum())\n",
    "print(\"label ratio:\")\n",
    "print(df_chk[\"Is Laundering\"].value_counts(normalize=True).rename(\"ratio\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1dd071b-148f-4688-b2dc-0d1d713a3477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 processing batch 1 ...\n",
      "🔹 processing batch 2 ...\n",
      "🔹 processing batch 3 ...\n",
      "🔹 processing batch 4 ...\n",
      "🔹 processing batch 5 ...\n",
      "🔹 processing batch 6 ...\n",
      "🔹 processing batch 7 ...\n",
      "🔹 processing batch 8 ...\n",
      "🔹 processing batch 9 ...\n",
      "🔹 processing batch 10 ...\n",
      "🔹 processing batch 11 ...\n",
      "🔹 processing batch 12 ...\n",
      "🔹 processing batch 13 ...\n",
      "🔹 processing batch 14 ...\n",
      "🔹 processing batch 15 ...\n",
      "🔹 processing batch 16 ...\n",
      "🔹 processing batch 17 ...\n",
      "🔹 processing batch 18 ...\n",
      "🔹 processing batch 19 ...\n",
      "🔹 processing batch 20 ...\n",
      "🔹 processing batch 21 ...\n",
      "🔹 processing batch 22 ...\n",
      "🔹 processing batch 23 ...\n",
      "🔹 processing batch 24 ...\n",
      "🔹 processing batch 25 ...\n",
      "🔹 processing batch 26 ...\n",
      "🔹 processing batch 27 ...\n",
      "🔹 processing batch 28 ...\n",
      "🔹 processing batch 29 ...\n",
      "🔹 processing batch 30 ...\n",
      "🔹 processing batch 31 ...\n",
      "🔹 processing batch 32 ...\n",
      "🔹 processing batch 33 ...\n",
      "🔹 processing batch 34 ...\n",
      "🔹 processing batch 35 ...\n",
      "🔹 processing batch 36 ...\n",
      "🔹 processing batch 37 ...\n",
      "🔹 processing batch 38 ...\n",
      "🔹 processing batch 39 ...\n",
      "🔹 processing batch 40 ...\n",
      "🔹 processing batch 41 ...\n",
      "🔹 processing batch 42 ...\n",
      "🔹 processing batch 43 ...\n",
      "🔹 processing batch 44 ...\n",
      "🔹 processing batch 45 ...\n",
      "🔹 processing batch 46 ...\n",
      "🔹 processing batch 47 ...\n",
      "🔹 processing batch 48 ...\n",
      "🔹 processing batch 49 ...\n",
      "🔹 processing batch 50 ...\n",
      "🔹 processing batch 51 ...\n",
      "🔹 processing batch 52 ...\n",
      "🔹 processing batch 53 ...\n",
      "🔹 processing batch 54 ...\n",
      "🔹 processing batch 55 ...\n",
      "🔹 processing batch 56 ...\n",
      "🔹 processing batch 57 ...\n",
      "🔹 processing batch 58 ...\n",
      "🔹 processing batch 59 ...\n",
      "🔹 processing batch 60 ...\n",
      "🔹 processing batch 61 ...\n",
      "🔹 processing batch 62 ...\n",
      "🔹 processing batch 63 ...\n",
      "🔹 processing batch 64 ...\n",
      "🔹 processing batch 65 ...\n",
      "🔹 processing batch 66 ...\n",
      "🔹 processing batch 67 ...\n",
      "🔹 processing batch 68 ...\n",
      "🔹 processing batch 69 ...\n",
      "🔹 processing batch 70 ...\n",
      "🔹 processing batch 71 ...\n",
      "🔹 processing batch 72 ...\n",
      "🔹 processing batch 73 ...\n",
      "🔹 processing batch 74 ...\n",
      "🔹 processing batch 75 ...\n",
      "🔹 processing batch 76 ...\n",
      "🔹 processing batch 77 ...\n",
      "🔹 processing batch 78 ...\n",
      "🔹 processing batch 79 ...\n",
      "🔹 processing batch 80 ...\n",
      "🔹 processing batch 81 ...\n",
      "🔹 processing batch 82 ...\n",
      "🔹 processing batch 83 ...\n",
      "🔹 processing batch 84 ...\n",
      "🔹 processing batch 85 ...\n",
      "🔹 processing batch 86 ...\n",
      "🔹 processing batch 87 ...\n",
      "🔹 processing batch 88 ...\n",
      "🔹 processing batch 89 ...\n",
      "🔹 processing batch 90 ...\n",
      "🔹 processing batch 91 ...\n",
      "🔹 processing batch 92 ...\n",
      "🔹 processing batch 93 ...\n",
      "🔹 processing batch 94 ...\n",
      "🔹 processing batch 95 ...\n",
      "🔹 processing batch 96 ...\n",
      "🔹 processing batch 97 ...\n",
      "🔹 processing batch 98 ...\n",
      "🔹 processing batch 99 ...\n",
      "🔹 processing batch 100 ...\n",
      "🔹 processing batch 101 ...\n",
      "🔹 processing batch 102 ...\n",
      "🔹 processing batch 103 ...\n",
      "🔹 processing batch 104 ...\n",
      "🔹 processing batch 105 ...\n",
      "🔹 processing batch 106 ...\n",
      "🔹 processing batch 107 ...\n",
      "🔹 processing batch 108 ...\n",
      "🔹 processing batch 109 ...\n",
      "🔹 processing batch 110 ...\n",
      "🔹 processing batch 111 ...\n",
      "🔹 processing batch 112 ...\n",
      "🔹 processing batch 113 ...\n",
      "🔹 processing batch 114 ...\n",
      "🔹 processing batch 115 ...\n",
      "🔹 processing batch 116 ...\n",
      "🔹 processing batch 117 ...\n",
      "🔹 processing batch 118 ...\n",
      "🔹 processing batch 119 ...\n",
      "🔹 processing batch 120 ...\n",
      "🔹 processing batch 121 ...\n",
      "🔹 processing batch 122 ...\n",
      "🔹 processing batch 123 ...\n",
      "🔹 processing batch 124 ...\n",
      "🔹 processing batch 125 ...\n",
      "🔹 processing batch 126 ...\n",
      "🔹 processing batch 127 ...\n",
      "🔹 processing batch 128 ...\n",
      "🔹 processing batch 129 ...\n",
      "🔹 processing batch 130 ...\n",
      "🔹 processing batch 131 ...\n",
      "🔹 processing batch 132 ...\n",
      "🔹 processing batch 133 ...\n",
      "🔹 processing batch 134 ...\n",
      "🔹 processing batch 135 ...\n",
      "🔹 processing batch 136 ...\n",
      "🔹 processing batch 137 ...\n",
      "🔹 processing batch 138 ...\n",
      "🔹 processing batch 139 ...\n",
      "🔹 processing batch 140 ...\n",
      "🔹 processing batch 141 ...\n",
      "🔹 processing batch 142 ...\n",
      "🔹 processing batch 143 ...\n",
      "🔹 processing batch 144 ...\n",
      "🔹 processing batch 145 ...\n",
      "🔹 processing batch 146 ...\n",
      "🔹 processing batch 147 ...\n",
      "🔹 processing batch 148 ...\n",
      "🔹 processing batch 149 ...\n",
      "🔹 processing batch 150 ...\n",
      "🔹 processing batch 151 ...\n",
      "🔹 processing batch 152 ...\n",
      "🔹 processing batch 153 ...\n",
      "🔹 processing batch 154 ...\n",
      "🔹 processing batch 155 ...\n",
      "🔹 processing batch 156 ...\n",
      "🔹 processing batch 157 ...\n",
      "🔹 processing batch 158 ...\n",
      "🔹 processing batch 159 ...\n",
      "🔹 processing batch 160 ...\n",
      "🔹 processing batch 161 ...\n",
      "🔹 processing batch 162 ...\n",
      "🔹 processing batch 163 ...\n",
      "🔹 processing batch 164 ...\n",
      "🔹 processing batch 165 ...\n",
      "🔹 processing batch 166 ...\n",
      "🔹 processing batch 167 ...\n",
      "🔹 processing batch 168 ...\n",
      "🔹 processing batch 169 ...\n",
      "🔹 processing batch 170 ...\n",
      "🔹 processing batch 171 ...\n",
      "🔹 processing batch 172 ...\n",
      "🔹 processing batch 173 ...\n",
      "🔹 processing batch 174 ...\n",
      "🔹 processing batch 175 ...\n",
      "🔹 processing batch 176 ...\n",
      "🔹 processing batch 177 ...\n",
      "🔹 processing batch 178 ...\n",
      "🔹 processing batch 179 ...\n",
      "🔹 processing batch 180 ...\n",
      "🔹 processing batch 181 ...\n",
      "🔹 processing batch 182 ...\n",
      "🔹 processing batch 183 ...\n",
      "🔹 processing batch 184 ...\n",
      "🔹 processing batch 185 ...\n",
      "🔹 processing batch 186 ...\n",
      "🔹 processing batch 187 ...\n",
      "🔹 processing batch 188 ...\n",
      "🔹 processing batch 189 ...\n",
      "🔹 processing batch 190 ...\n",
      "🔹 processing batch 191 ...\n",
      "🔹 processing batch 192 ...\n",
      "🔹 processing batch 193 ...\n",
      "🔹 processing batch 194 ...\n",
      "🔹 processing batch 195 ...\n",
      "🔹 processing batch 196 ...\n",
      "🔹 processing batch 197 ...\n",
      "🔹 processing batch 198 ...\n",
      "🔹 processing batch 199 ...\n",
      "🔹 processing batch 200 ...\n",
      "🔹 processing batch 201 ...\n",
      "🔹 processing batch 202 ...\n",
      "🔹 processing batch 203 ...\n",
      "🔹 processing batch 204 ...\n",
      "🔹 processing batch 205 ...\n",
      "🔹 processing batch 206 ...\n",
      "🔹 processing batch 207 ...\n",
      "🔹 processing batch 208 ...\n",
      "🔹 processing batch 209 ...\n",
      "🔹 processing batch 210 ...\n",
      "🔹 processing batch 211 ...\n",
      "🔹 processing batch 212 ...\n",
      "🔹 processing batch 213 ...\n",
      "🔹 processing batch 214 ...\n",
      "🔹 processing batch 215 ...\n",
      "🔹 processing batch 216 ...\n",
      "🔹 processing batch 217 ...\n",
      "🔹 processing batch 218 ...\n",
      "🔹 processing batch 219 ...\n",
      "🔹 processing batch 220 ...\n",
      "🔹 processing batch 221 ...\n",
      "🔹 processing batch 222 ...\n",
      "🔹 processing batch 223 ...\n",
      "🔹 processing batch 224 ...\n",
      "🔹 processing batch 225 ...\n",
      "🔹 processing batch 226 ...\n",
      "🔹 processing batch 227 ...\n",
      "🔹 processing batch 228 ...\n",
      "🔹 processing batch 229 ...\n",
      "🔹 processing batch 230 ...\n",
      "🔹 processing batch 231 ...\n",
      "🔹 processing batch 232 ...\n",
      "🔹 processing batch 233 ...\n",
      "🔹 processing batch 234 ...\n",
      "🔹 processing batch 235 ...\n",
      "🔹 processing batch 236 ...\n",
      "🔹 processing batch 237 ...\n",
      "🔹 processing batch 238 ...\n",
      "🔹 processing batch 239 ...\n",
      "✅ finished: data/interim/LI_Medium_merged.parquet\n",
      "\n",
      "--- Verification ---\n",
      "rows: 31251818\n",
      "required columns present: True\n",
      "accounts join coverage: 100.00%\n",
      "duplicate TX_IDs: 335\n",
      "label ratio:\n",
      "Is Laundering\n",
      "0    0.999487\n",
      "1    0.000513\n",
      "Name: ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Build LI_Medium_merged.parquet without loading whole file ===\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "INT = Path(\"data/interim\")\n",
    "RAW = Path(\"data/raw\")\n",
    "INT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACCT_ID = \"From Account\"\n",
    "DATECOL = \"Timestamp\"\n",
    "AMOUNT  = \"Amount Paid\"\n",
    "LABEL   = \"Is Laundering\"\n",
    "\n",
    "li_accts_parq = INT / \"LI_Medium_accounts.parquet\"\n",
    "li_trans_parq = INT / \"LI_Medium_Trans.parquet\"\n",
    "dst           = INT / \"LI_Medium_merged.parquet\"\n",
    "\n",
    "# 1) Accounts (small)\n",
    "li_med_a = pd.read_parquet(li_accts_parq)\n",
    "li_med_a[\"Account Number\"] = li_med_a[\"Account Number\"].astype(str)\n",
    "acct_pref = li_med_a.add_prefix(\"acct_\").rename(columns={\"acct_Account Number\": ACCT_ID})\n",
    "\n",
    "# 2) Stream transactions row-groups and rename IN-LOOP (no big copies)\n",
    "dataset = ds.dataset(li_trans_parq, format=\"parquet\")\n",
    "writer = None\n",
    "batch_no = 0\n",
    "global_txid = 0  # monotonically increasing across batches\n",
    "\n",
    "expected_cols = [\n",
    "    'Timestamp','From Bank','From Account','To Bank','To Account',\n",
    "    'Amount Received','Receiving Currency','Amount Paid',\n",
    "    'Payment Currency','Payment Format','Is Laundering'\n",
    "]\n",
    "\n",
    "for batch in dataset.to_batches():\n",
    "    batch_no += 1\n",
    "    print(f\"🔹 processing batch {batch_no} ...\")\n",
    "    df = batch.to_pandas()  # small-ish chunk only\n",
    "\n",
    "    # If this batch has generic names, rename by position (cheap)\n",
    "    if \"Account\" in df.columns and \"Account.1\" in df.columns and len(df.columns) == 11:\n",
    "        df.columns = expected_cols  # assigns header only; no data copy\n",
    "    # If already correct, nothing happens.\n",
    "\n",
    "    # Types / helpers\n",
    "    df[DATECOL] = pd.to_datetime(df[DATECOL], errors=\"coerce\")\n",
    "    df[AMOUNT]  = pd.to_numeric(df[AMOUNT], errors=\"coerce\")\n",
    "    df[LABEL]   = df[LABEL].astype(int)\n",
    "    df[ACCT_ID] = df[ACCT_ID].astype(str)\n",
    "\n",
    "    # Global Transaction ID across batches\n",
    "    start = global_txid + 1\n",
    "    global_txid += len(df)\n",
    "    df[\"Transaction ID\"] = range(start, global_txid + 1)\n",
    "\n",
    "    # Merge with accounts (small dimension)\n",
    "    merged = df.merge(acct_pref, on=ACCT_ID, how=\"left\")\n",
    "\n",
    "    # Append to parquet\n",
    "    table = pa.Table.from_pandas(merged)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(dst, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    del df, merged, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"✅ finished: {dst.as_posix()}\")\n",
    "\n",
    "# 3) Quick verification (lightweight)\n",
    "schema = pq.read_schema(dst).names\n",
    "take = [c for c in [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\",\"Transaction ID\"] if c in schema]\n",
    "acct_col = next((c for c in schema if c.startswith(\"acct_\")), None)\n",
    "df_chk = pd.read_parquet(dst, columns=[*take, *( [acct_col] if acct_col else [] )])\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(\"rows:\", len(df_chk))\n",
    "print(\"required columns present:\",\n",
    "      all(c in schema for c in [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"]))\n",
    "if acct_col:\n",
    "    print(\"accounts join coverage:\", f\"{(1 - df_chk[acct_col].isna().mean()):.2%}\")\n",
    "print(\"duplicate TX_IDs:\", df_chk.duplicated(subset=[\"Transaction ID\"]).sum())\n",
    "print(\"label ratio:\")\n",
    "print(df_chk[\"Is Laundering\"].value_counts(normalize=True).rename(\"ratio\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edae0d3-9b31-4723-81c0-f1cee5878986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 5,078,345\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998981\n",
      "1    0.001019\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 0 ✅\n",
      "\n",
      "=== LI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 6,924,049\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.999485\n",
      "1    0.000515\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 0 ✅\n",
      "\n",
      "=== HI_Medium ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 31,898,510\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998896\n",
      "1    0.001104\n",
      "Name: ratio, dtype: float64\n",
      "• Transaction ID not present (OK if you skipped it)\n",
      "\n",
      "=== LI_Medium ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 31,251,818\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.999487\n",
      "1    0.000513\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 335 ⚠️\n"
     ]
    }
   ],
   "source": [
    "# === Step 5 Verification for all four merged datasets ===\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "targets = {\n",
    "    \"HI_Small\":  DATA/\"HI_Small_merged.parquet\",\n",
    "    \"LI_Small\":  DATA/\"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium\": DATA/\"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium\": DATA/\"LI_Medium_merged.parquet\",\n",
    "}\n",
    "\n",
    "def verify_merged(name, path: Path):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    if not path.exists():\n",
    "        print(\"❌ MISSING:\", path.name); return\n",
    "\n",
    "    # Read schema only (no data)\n",
    "    cols = pq.read_schema(path).names\n",
    "\n",
    "    # Required core columns\n",
    "    req = [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"]\n",
    "    have = [c for c in req if c in cols]\n",
    "    acct_col = next((c for c in cols if c.startswith(\"acct_\")), None)\n",
    "    txid_col = \"Transaction ID\" if \"Transaction ID\" in cols else None\n",
    "\n",
    "    # Read just the needed columns\n",
    "    read_cols = [*have, *( [acct_col] if acct_col else [] ), *( [txid_col] if txid_col else [] )]\n",
    "    df = pd.read_parquet(path, columns=read_cols)\n",
    "\n",
    "    # 1) required columns present\n",
    "    missing = [c for c in req if c not in cols]\n",
    "    print(\"• Required columns present:\", \"✅\" if not missing else f\"❌ missing {missing}\")\n",
    "\n",
    "    # 2) row count\n",
    "    print(f\"• Rows (sampled cols): {len(df):,}\")\n",
    "\n",
    "    # 3) accounts join coverage\n",
    "    if acct_col:\n",
    "        join_cov = 1 - df[acct_col].isna().mean()\n",
    "        print(f\"• Accounts join coverage via {acct_col}: {join_cov:.2%} {'✅' if join_cov>0 else '⚠️'}\")\n",
    "    else:\n",
    "        print(\"• Accounts join coverage: ⚠️ no acct_* column (merge may be missing)\")\n",
    "\n",
    "    # 4) label sanity\n",
    "    if \"Is Laundering\" in df.columns:\n",
    "        ratios = df[\"Is Laundering\"].value_counts(normalize=True, dropna=False).rename(\"ratio\")\n",
    "        print(\"• Label ratio (0/1):\"); print(ratios)\n",
    "    else:\n",
    "        print(\"• Label column missing\")\n",
    "\n",
    "    # 5) duplicate TX_ID (if present)\n",
    "    if txid_col:\n",
    "        dups = df.duplicated(subset=[txid_col]).sum()\n",
    "        print(f\"• Duplicate '{txid_col}' count:\", dups, \"✅\" if dups==0 else \"⚠️\")\n",
    "    else:\n",
    "        print(\"• Transaction ID not present (OK if you skipped it)\")\n",
    "\n",
    "for n,p in targets.items():\n",
    "    verify_merged(n, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d46c7-0859-4caf-887f-9764304f965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "file = DATA / \"LI_Medium_merged.parquet\"\n",
    "\n",
    "df = pd.read_parquet(file)\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Transaction ID\"], keep=\"first\")\n",
    "after = len(df)\n",
    "df.to_parquet(file, index=False)\n",
    "print(f\"✅ Cleaned LI_Medium_merged.parquet — removed {before - after} duplicates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e8a8a6-be96-4e5f-af61-45deb547a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 rewriting batch 1 ...\n",
      "🔹 rewriting batch 2 ...\n",
      "🔹 rewriting batch 3 ...\n",
      "🔹 rewriting batch 4 ...\n",
      "🔹 rewriting batch 5 ...\n",
      "🔹 rewriting batch 6 ...\n",
      "🔹 rewriting batch 7 ...\n",
      "🔹 rewriting batch 8 ...\n",
      "🔹 rewriting batch 9 ...\n",
      "🔹 rewriting batch 10 ...\n",
      "🔹 rewriting batch 11 ...\n",
      "🔹 rewriting batch 12 ...\n",
      "🔹 rewriting batch 13 ...\n",
      "🔹 rewriting batch 14 ...\n",
      "🔹 rewriting batch 15 ...\n",
      "🔹 rewriting batch 16 ...\n",
      "🔹 rewriting batch 17 ...\n",
      "🔹 rewriting batch 18 ...\n",
      "🔹 rewriting batch 19 ...\n",
      "🔹 rewriting batch 20 ...\n",
      "🔹 rewriting batch 21 ...\n",
      "🔹 rewriting batch 22 ...\n",
      "🔹 rewriting batch 23 ...\n",
      "🔹 rewriting batch 24 ...\n",
      "🔹 rewriting batch 25 ...\n",
      "🔹 rewriting batch 26 ...\n",
      "🔹 rewriting batch 27 ...\n",
      "🔹 rewriting batch 28 ...\n",
      "🔹 rewriting batch 29 ...\n",
      "🔹 rewriting batch 30 ...\n",
      "🔹 rewriting batch 31 ...\n",
      "🔹 rewriting batch 32 ...\n",
      "🔹 rewriting batch 33 ...\n",
      "🔹 rewriting batch 34 ...\n",
      "🔹 rewriting batch 35 ...\n",
      "🔹 rewriting batch 36 ...\n",
      "🔹 rewriting batch 37 ...\n",
      "🔹 rewriting batch 38 ...\n",
      "🔹 rewriting batch 39 ...\n",
      "🔹 rewriting batch 40 ...\n",
      "🔹 rewriting batch 41 ...\n",
      "🔹 rewriting batch 42 ...\n",
      "🔹 rewriting batch 43 ...\n",
      "🔹 rewriting batch 44 ...\n",
      "🔹 rewriting batch 45 ...\n",
      "🔹 rewriting batch 46 ...\n",
      "🔹 rewriting batch 47 ...\n",
      "🔹 rewriting batch 48 ...\n",
      "🔹 rewriting batch 49 ...\n",
      "🔹 rewriting batch 50 ...\n",
      "🔹 rewriting batch 51 ...\n",
      "🔹 rewriting batch 52 ...\n",
      "🔹 rewriting batch 53 ...\n",
      "🔹 rewriting batch 54 ...\n",
      "🔹 rewriting batch 55 ...\n",
      "🔹 rewriting batch 56 ...\n",
      "🔹 rewriting batch 57 ...\n",
      "🔹 rewriting batch 58 ...\n",
      "🔹 rewriting batch 59 ...\n",
      "🔹 rewriting batch 60 ...\n",
      "🔹 rewriting batch 61 ...\n",
      "🔹 rewriting batch 62 ...\n",
      "🔹 rewriting batch 63 ...\n",
      "🔹 rewriting batch 64 ...\n",
      "🔹 rewriting batch 65 ...\n",
      "🔹 rewriting batch 66 ...\n",
      "🔹 rewriting batch 67 ...\n",
      "🔹 rewriting batch 68 ...\n",
      "🔹 rewriting batch 69 ...\n",
      "🔹 rewriting batch 70 ...\n",
      "🔹 rewriting batch 71 ...\n",
      "🔹 rewriting batch 72 ...\n",
      "🔹 rewriting batch 73 ...\n",
      "🔹 rewriting batch 74 ...\n",
      "🔹 rewriting batch 75 ...\n",
      "🔹 rewriting batch 76 ...\n",
      "🔹 rewriting batch 77 ...\n",
      "🔹 rewriting batch 78 ...\n",
      "🔹 rewriting batch 79 ...\n",
      "🔹 rewriting batch 80 ...\n",
      "🔹 rewriting batch 81 ...\n",
      "🔹 rewriting batch 82 ...\n",
      "🔹 rewriting batch 83 ...\n",
      "🔹 rewriting batch 84 ...\n",
      "🔹 rewriting batch 85 ...\n",
      "🔹 rewriting batch 86 ...\n",
      "🔹 rewriting batch 87 ...\n",
      "🔹 rewriting batch 88 ...\n",
      "🔹 rewriting batch 89 ...\n",
      "🔹 rewriting batch 90 ...\n",
      "🔹 rewriting batch 91 ...\n",
      "🔹 rewriting batch 92 ...\n",
      "🔹 rewriting batch 93 ...\n",
      "🔹 rewriting batch 94 ...\n",
      "🔹 rewriting batch 95 ...\n",
      "🔹 rewriting batch 96 ...\n",
      "🔹 rewriting batch 97 ...\n",
      "🔹 rewriting batch 98 ...\n",
      "🔹 rewriting batch 99 ...\n",
      "🔹 rewriting batch 100 ...\n",
      "🔹 rewriting batch 101 ...\n",
      "🔹 rewriting batch 102 ...\n",
      "🔹 rewriting batch 103 ...\n",
      "🔹 rewriting batch 104 ...\n",
      "🔹 rewriting batch 105 ...\n",
      "🔹 rewriting batch 106 ...\n",
      "🔹 rewriting batch 107 ...\n",
      "🔹 rewriting batch 108 ...\n",
      "🔹 rewriting batch 109 ...\n",
      "🔹 rewriting batch 110 ...\n",
      "🔹 rewriting batch 111 ...\n",
      "🔹 rewriting batch 112 ...\n",
      "🔹 rewriting batch 113 ...\n",
      "🔹 rewriting batch 114 ...\n",
      "🔹 rewriting batch 115 ...\n",
      "🔹 rewriting batch 116 ...\n",
      "🔹 rewriting batch 117 ...\n",
      "🔹 rewriting batch 118 ...\n",
      "🔹 rewriting batch 119 ...\n",
      "🔹 rewriting batch 120 ...\n",
      "🔹 rewriting batch 121 ...\n",
      "🔹 rewriting batch 122 ...\n",
      "🔹 rewriting batch 123 ...\n",
      "🔹 rewriting batch 124 ...\n",
      "🔹 rewriting batch 125 ...\n",
      "🔹 rewriting batch 126 ...\n",
      "🔹 rewriting batch 127 ...\n",
      "🔹 rewriting batch 128 ...\n",
      "🔹 rewriting batch 129 ...\n",
      "🔹 rewriting batch 130 ...\n",
      "🔹 rewriting batch 131 ...\n",
      "🔹 rewriting batch 132 ...\n",
      "🔹 rewriting batch 133 ...\n",
      "🔹 rewriting batch 134 ...\n",
      "🔹 rewriting batch 135 ...\n",
      "🔹 rewriting batch 136 ...\n",
      "🔹 rewriting batch 137 ...\n",
      "🔹 rewriting batch 138 ...\n",
      "🔹 rewriting batch 139 ...\n",
      "🔹 rewriting batch 140 ...\n",
      "🔹 rewriting batch 141 ...\n",
      "🔹 rewriting batch 142 ...\n",
      "🔹 rewriting batch 143 ...\n",
      "🔹 rewriting batch 144 ...\n",
      "🔹 rewriting batch 145 ...\n",
      "🔹 rewriting batch 146 ...\n",
      "🔹 rewriting batch 147 ...\n",
      "🔹 rewriting batch 148 ...\n",
      "🔹 rewriting batch 149 ...\n",
      "🔹 rewriting batch 150 ...\n",
      "🔹 rewriting batch 151 ...\n",
      "🔹 rewriting batch 152 ...\n",
      "🔹 rewriting batch 153 ...\n",
      "🔹 rewriting batch 154 ...\n",
      "🔹 rewriting batch 155 ...\n",
      "🔹 rewriting batch 156 ...\n",
      "🔹 rewriting batch 157 ...\n",
      "🔹 rewriting batch 158 ...\n",
      "🔹 rewriting batch 159 ...\n",
      "🔹 rewriting batch 160 ...\n",
      "🔹 rewriting batch 161 ...\n",
      "🔹 rewriting batch 162 ...\n",
      "🔹 rewriting batch 163 ...\n",
      "🔹 rewriting batch 164 ...\n",
      "🔹 rewriting batch 165 ...\n",
      "🔹 rewriting batch 166 ...\n",
      "🔹 rewriting batch 167 ...\n",
      "🔹 rewriting batch 168 ...\n",
      "🔹 rewriting batch 169 ...\n",
      "🔹 rewriting batch 170 ...\n",
      "🔹 rewriting batch 171 ...\n",
      "🔹 rewriting batch 172 ...\n",
      "🔹 rewriting batch 173 ...\n",
      "🔹 rewriting batch 174 ...\n",
      "🔹 rewriting batch 175 ...\n",
      "🔹 rewriting batch 176 ...\n",
      "🔹 rewriting batch 177 ...\n",
      "🔹 rewriting batch 178 ...\n",
      "🔹 rewriting batch 179 ...\n",
      "🔹 rewriting batch 180 ...\n",
      "🔹 rewriting batch 181 ...\n",
      "🔹 rewriting batch 182 ...\n",
      "🔹 rewriting batch 183 ...\n",
      "🔹 rewriting batch 184 ...\n",
      "🔹 rewriting batch 185 ...\n",
      "🔹 rewriting batch 186 ...\n",
      "🔹 rewriting batch 187 ...\n",
      "🔹 rewriting batch 188 ...\n",
      "🔹 rewriting batch 189 ...\n",
      "🔹 rewriting batch 190 ...\n",
      "🔹 rewriting batch 191 ...\n",
      "🔹 rewriting batch 192 ...\n",
      "🔹 rewriting batch 193 ...\n",
      "🔹 rewriting batch 194 ...\n",
      "🔹 rewriting batch 195 ...\n",
      "🔹 rewriting batch 196 ...\n",
      "🔹 rewriting batch 197 ...\n",
      "🔹 rewriting batch 198 ...\n",
      "🔹 rewriting batch 199 ...\n",
      "🔹 rewriting batch 200 ...\n",
      "🔹 rewriting batch 201 ...\n",
      "🔹 rewriting batch 202 ...\n",
      "🔹 rewriting batch 203 ...\n",
      "🔹 rewriting batch 204 ...\n",
      "🔹 rewriting batch 205 ...\n",
      "🔹 rewriting batch 206 ...\n",
      "🔹 rewriting batch 207 ...\n",
      "🔹 rewriting batch 208 ...\n",
      "🔹 rewriting batch 209 ...\n",
      "🔹 rewriting batch 210 ...\n",
      "🔹 rewriting batch 211 ...\n",
      "🔹 rewriting batch 212 ...\n",
      "🔹 rewriting batch 213 ...\n",
      "🔹 rewriting batch 214 ...\n",
      "🔹 rewriting batch 215 ...\n",
      "🔹 rewriting batch 216 ...\n",
      "🔹 rewriting batch 217 ...\n",
      "🔹 rewriting batch 218 ...\n",
      "🔹 rewriting batch 219 ...\n",
      "🔹 rewriting batch 220 ...\n",
      "🔹 rewriting batch 221 ...\n",
      "🔹 rewriting batch 222 ...\n",
      "🔹 rewriting batch 223 ...\n",
      "🔹 rewriting batch 224 ...\n",
      "🔹 rewriting batch 225 ...\n",
      "🔹 rewriting batch 226 ...\n",
      "🔹 rewriting batch 227 ...\n",
      "🔹 rewriting batch 228 ...\n",
      "🔹 rewriting batch 229 ...\n",
      "🔹 rewriting batch 230 ...\n",
      "🔹 rewriting batch 231 ...\n",
      "🔹 rewriting batch 232 ...\n",
      "🔹 rewriting batch 233 ...\n",
      "🔹 rewriting batch 234 ...\n",
      "🔹 rewriting batch 235 ...\n",
      "🔹 rewriting batch 236 ...\n",
      "🔹 rewriting batch 237 ...\n",
      "🔹 rewriting batch 238 ...\n",
      "🔹 rewriting batch 239 ...\n",
      "🔹 rewriting batch 240 ...\n",
      "🔹 rewriting batch 241 ...\n",
      "🔹 rewriting batch 242 ...\n",
      "🔹 rewriting batch 243 ...\n",
      "🔹 rewriting batch 244 ...\n",
      "🔹 rewriting batch 245 ...\n",
      "🔹 rewriting batch 246 ...\n",
      "🔹 rewriting batch 247 ...\n",
      "🔹 rewriting batch 248 ...\n",
      "🔹 rewriting batch 249 ...\n",
      "🔹 rewriting batch 250 ...\n",
      "🔹 rewriting batch 251 ...\n",
      "🔹 rewriting batch 252 ...\n",
      "🔹 rewriting batch 253 ...\n",
      "🔹 rewriting batch 254 ...\n",
      "🔹 rewriting batch 255 ...\n",
      "🔹 rewriting batch 256 ...\n",
      "🔹 rewriting batch 257 ...\n",
      "🔹 rewriting batch 258 ...\n",
      "🔹 rewriting batch 259 ...\n",
      "🔹 rewriting batch 260 ...\n",
      "🔹 rewriting batch 261 ...\n",
      "🔹 rewriting batch 262 ...\n",
      "🔹 rewriting batch 263 ...\n",
      "🔹 rewriting batch 264 ...\n",
      "🔹 rewriting batch 265 ...\n",
      "🔹 rewriting batch 266 ...\n",
      "🔹 rewriting batch 267 ...\n",
      "🔹 rewriting batch 268 ...\n",
      "🔹 rewriting batch 269 ...\n",
      "🔹 rewriting batch 270 ...\n",
      "🔹 rewriting batch 271 ...\n",
      "🔹 rewriting batch 272 ...\n",
      "🔹 rewriting batch 273 ...\n",
      "🔹 rewriting batch 274 ...\n",
      "🔹 rewriting batch 275 ...\n",
      "🔹 rewriting batch 276 ...\n",
      "🔹 rewriting batch 277 ...\n",
      "🔹 rewriting batch 278 ...\n",
      "🔹 rewriting batch 279 ...\n",
      "🔹 rewriting batch 280 ...\n",
      "🔹 rewriting batch 281 ...\n",
      "🔹 rewriting batch 282 ...\n",
      "🔹 rewriting batch 283 ...\n",
      "🔹 rewriting batch 284 ...\n",
      "🔹 rewriting batch 285 ...\n",
      "🔹 rewriting batch 286 ...\n",
      "🔹 rewriting batch 287 ...\n",
      "🔹 rewriting batch 288 ...\n",
      "🔹 rewriting batch 289 ...\n",
      "🔹 rewriting batch 290 ...\n",
      "🔹 rewriting batch 291 ...\n",
      "🔹 rewriting batch 292 ...\n",
      "🔹 rewriting batch 293 ...\n",
      "🔹 rewriting batch 294 ...\n",
      "🔹 rewriting batch 295 ...\n",
      "🔹 rewriting batch 296 ...\n",
      "🔹 rewriting batch 297 ...\n",
      "🔹 rewriting batch 298 ...\n",
      "🔹 rewriting batch 299 ...\n",
      "🔹 rewriting batch 300 ...\n",
      "🔹 rewriting batch 301 ...\n",
      "🔹 rewriting batch 302 ...\n",
      "🔹 rewriting batch 303 ...\n",
      "🔹 rewriting batch 304 ...\n",
      "🔹 rewriting batch 305 ...\n",
      "🔹 rewriting batch 306 ...\n",
      "🔹 rewriting batch 307 ...\n",
      "🔹 rewriting batch 308 ...\n",
      "🔹 rewriting batch 309 ...\n",
      "🔹 rewriting batch 310 ...\n",
      "🔹 rewriting batch 311 ...\n",
      "✅ Rewrote LI_Medium_merged.parquet with unique Transaction ID\n",
      "Duplicate Transaction ID count: 0\n"
     ]
    }
   ],
   "source": [
    "# Reassign unique Transaction IDs for LI_Medium_merged.parquet (memory-safe)\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "INT = Path(\"data/interim\")\n",
    "SRC = INT / \"LI_Medium_merged.parquet\"\n",
    "TMP = INT / \"LI_Medium_merged_fixed.parquet\"\n",
    "\n",
    "dataset = ds.dataset(SRC, format=\"parquet\")\n",
    "\n",
    "writer = None\n",
    "batch_no = 0\n",
    "global_txid = 0  # will assign 1..N across all batches\n",
    "\n",
    "for batch in dataset.to_batches():\n",
    "    batch_no += 1\n",
    "    print(f\"🔹 rewriting batch {batch_no} ...\")\n",
    "    df = batch.to_pandas()               # small chunk only\n",
    "\n",
    "    # assign fresh, strictly unique TX IDs\n",
    "    start = global_txid + 1\n",
    "    global_txid += len(df)\n",
    "    df[\"Transaction ID\"] = range(start, global_txid + 1)\n",
    "\n",
    "    # append to new parquet\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(TMP, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "    del df, table, batch\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "# Replace old file with the fixed one\n",
    "SRC.unlink()\n",
    "TMP.rename(SRC)\n",
    "print(\"✅ Rewrote LI_Medium_merged.parquet with unique Transaction ID\")\n",
    "\n",
    "# Quick verification (reads only one column)\n",
    "dup_count = pd.read_parquet(SRC, columns=[\"Transaction ID\"])\\\n",
    "               .duplicated(subset=[\"Transaction ID\"]).sum()\n",
    "print(\"Duplicate Transaction ID count:\", dup_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7290c1-37df-4b33-8fe5-9f222bca87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 5,078,345\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998981\n",
      "1    0.001019\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 0 ✅\n",
      "\n",
      "=== LI_Small ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 6,924,049\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.999485\n",
      "1    0.000515\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 0 ✅\n",
      "\n",
      "=== HI_Medium ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 31,898,510\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.998896\n",
      "1    0.001104\n",
      "Name: ratio, dtype: float64\n",
      "• Transaction ID not present (OK if you skipped it)\n",
      "\n",
      "=== LI_Medium ===\n",
      "• Required columns present: ✅\n",
      "• Rows (sampled cols): 31,251,818\n",
      "• Accounts join coverage via acct_Bank Name: 100.00% ✅\n",
      "• Label ratio (0/1):\n",
      "Is Laundering\n",
      "0    0.999487\n",
      "1    0.000513\n",
      "Name: ratio, dtype: float64\n",
      "• Duplicate 'Transaction ID' count: 0 ✅\n"
     ]
    }
   ],
   "source": [
    "# === Step 5 Verification for all four merged datasets ===\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "targets = {\n",
    "    \"HI_Small\":  DATA/\"HI_Small_merged.parquet\",\n",
    "    \"LI_Small\":  DATA/\"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium\": DATA/\"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium\": DATA/\"LI_Medium_merged.parquet\",\n",
    "}\n",
    "\n",
    "def verify_merged(name, path: Path):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    if not path.exists():\n",
    "        print(\"❌ MISSING:\", path.name); return\n",
    "\n",
    "    # Read schema only (no data)\n",
    "    cols = pq.read_schema(path).names\n",
    "\n",
    "    # Required core columns\n",
    "    req = [\"Timestamp\",\"From Account\",\"Amount Paid\",\"Is Laundering\"]\n",
    "    have = [c for c in req if c in cols]\n",
    "    acct_col = next((c for c in cols if c.startswith(\"acct_\")), None)\n",
    "    txid_col = \"Transaction ID\" if \"Transaction ID\" in cols else None\n",
    "\n",
    "    # Read just the needed columns\n",
    "    read_cols = [*have, *( [acct_col] if acct_col else [] ), *( [txid_col] if txid_col else [] )]\n",
    "    df = pd.read_parquet(path, columns=read_cols)\n",
    "\n",
    "    # 1) required columns present\n",
    "    missing = [c for c in req if c not in cols]\n",
    "    print(\"• Required columns present:\", \"✅\" if not missing else f\"❌ missing {missing}\")\n",
    "\n",
    "    # 2) row count\n",
    "    print(f\"• Rows (sampled cols): {len(df):,}\")\n",
    "\n",
    "    # 3) accounts join coverage\n",
    "    if acct_col:\n",
    "        join_cov = 1 - df[acct_col].isna().mean()\n",
    "        print(f\"• Accounts join coverage via {acct_col}: {join_cov:.2%} {'✅' if join_cov>0 else '⚠️'}\")\n",
    "    else:\n",
    "        print(\"• Accounts join coverage: ⚠️ no acct_* column (merge may be missing)\")\n",
    "\n",
    "    # 4) label sanity\n",
    "    if \"Is Laundering\" in df.columns:\n",
    "        ratios = df[\"Is Laundering\"].value_counts(normalize=True, dropna=False).rename(\"ratio\")\n",
    "        print(\"• Label ratio (0/1):\"); print(ratios)\n",
    "    else:\n",
    "        print(\"• Label column missing\")\n",
    "\n",
    "    # 5) duplicate TX_ID (if present)\n",
    "    if txid_col:\n",
    "        dups = df.duplicated(subset=[txid_col]).sum()\n",
    "        print(f\"• Duplicate '{txid_col}' count:\", dups, \"✅\" if dups==0 else \"⚠️\")\n",
    "    else:\n",
    "        print(\"• Transaction ID not present (OK if you skipped it)\")\n",
    "\n",
    "for n,p in targets.items():\n",
    "    verify_merged(n, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bea7d58-5c15-49fb-b2c9-5de397a47a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small_merged.parquet ===\n",
      "count    5.078345e+06\n",
      "mean     5.988726e+06\n",
      "std      1.037183e+09\n",
      "min      1.000000e-06\n",
      "1%       1.680776e-02\n",
      "5%       6.850000e+00\n",
      "50%      1.411010e+03\n",
      "95%      6.499065e+05\n",
      "99%      1.475123e+07\n",
      "max      1.046302e+12\n",
      "Name: Amount Received, dtype: float64\n",
      "\n",
      "=== LI_Small_merged.parquet ===\n",
      "count    6.924049e+06\n",
      "mean     6.324067e+06\n",
      "std      2.105371e+09\n",
      "min      1.000000e-06\n",
      "1%       9.488000e-03\n",
      "5%       2.950000e+00\n",
      "50%      1.397620e+03\n",
      "95%      6.235947e+05\n",
      "99%      1.336849e+07\n",
      "max      3.644854e+12\n",
      "Name: Amount Received, dtype: float64\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚠️ No amount column found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHI_Small_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLI_Small_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHI_Medium_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLI_Medium_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mshow_amount_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mshow_amount_stats\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_amount_stats\u001b[39m(file):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     amt_col = \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c.lower()), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1898\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1885\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1886\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1887\u001b[39m         binary_type=binary_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1895\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1896\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1538\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1530\u001b[39m         index_columns = [\n\u001b[32m   1531\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1532\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1533\u001b[39m         ]\n\u001b[32m   1534\u001b[39m         columns = (\n\u001b[32m   1535\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1544\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:589\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3939\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "\n",
    "def show_amount_stats(file):\n",
    "    df = pd.read_parquet(DATA / file)\n",
    "    amt_col = next((c for c in df.columns if \"amount\" in c.lower()), None)\n",
    "    print(f\"\\n=== {file} ===\")\n",
    "    if amt_col:\n",
    "        print(df[amt_col].describe(percentiles=[.01, .05, .5, .95, .99]))\n",
    "    else:\n",
    "        print(\"⚠️ No amount column found\")\n",
    "\n",
    "for f in [\n",
    "    \"HI_Small_merged.parquet\",\n",
    "    \"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium_merged.parquet\"\n",
    "]:\n",
    "    show_amount_stats(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acb92ac-b3d3-484f-9810-dddafda41ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small ===\n",
      "Label ratio: {0: 0.998981, 1: 0.001019}\n",
      "count: 5,078,345\n",
      " mean: 4,509,273.367741\n",
      "  std: 869,772,830.919836\n",
      "  min: 0.000001\n",
      "   1%: 0.018632\n",
      "   5%: 7.090000\n",
      "  50%: 1,422.660000\n",
      "  95%: 647,407.290000\n",
      "  99%: 14,608,552.394300\n",
      "  max: 1,046,302,363,293.479980\n",
      "\n",
      "=== LI_Small ===\n",
      "Label ratio: {0: 0.999485, 1: 0.000515}\n",
      "count: 6,924,049\n",
      " mean: 4,676,035.972670\n",
      "  std: 1,544,098,506.684294\n",
      "  min: 0.000001\n",
      "   1%: 0.010000\n",
      "   5%: 3.400000\n",
      "  50%: 1,407.825000\n",
      "  95%: 603,494.249000\n",
      "  99%: 12,355,335.080000\n",
      "  max: 3,644,853,662,746.950195\n",
      "\n",
      "=== HI_Medium ===\n",
      "Label ratio: {0: 0.998896, 1: 0.001104}\n",
      "count: 31,898,510\n",
      " mean: 4,417,513.346213\n",
      "  std: 1,848,305,609.964206\n",
      "  min: 0.000001\n",
      "   1%: 0.040000\n",
      "   5%: 10.420000\n",
      "  50%: 1,483.170000\n",
      "  95%: 518,181.869000\n",
      "  99%: 10,757,310.816700\n",
      "  max: 8,158,609,321,727.610352\n",
      "\n",
      "=== LI_Medium ===\n",
      "Label ratio: {0: 0.999487, 1: 0.000513}\n",
      "count: 31,251,818\n",
      " mean: 5,392,182.547689\n",
      "  std: 1,298,672,282.658915\n",
      "  min: 0.000001\n",
      "   1%: 0.048536\n",
      "   5%: 10.649500\n",
      "  50%: 1,468.100000\n",
      "  95%: 551,553.630000\n",
      "  99%: 11,965,632.082800\n",
      "  max: 2,020,628,026,922.110107\n"
     ]
    }
   ],
   "source": [
    "# Memory-safe Step 6: amounts + labels (streaming, no full-table loads)\n",
    "from pathlib import Path\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "FILES = {\n",
    "    \"HI_Small\":  DATA/\"HI_Small_merged.parquet\",\n",
    "    \"LI_Small\":  DATA/\"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium\": DATA/\"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium\": DATA/\"LI_Medium_merged.parquet\",\n",
    "}\n",
    "\n",
    "def find_amount_col(path: Path):\n",
    "    cols = pq.read_schema(path).names\n",
    "    # prefer Paid, then Received, else any 'amount'\n",
    "    for c in cols:\n",
    "        if c.lower() == \"amount paid\":\n",
    "            return c\n",
    "    for c in cols:\n",
    "        if c.lower() == \"amount received\":\n",
    "            return c\n",
    "    for c in cols:\n",
    "        if \"amount\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def label_ratio_stream(path: Path, label_col=\"Is Laundering\"):\n",
    "    dset = ds.dataset(path, format=\"parquet\")\n",
    "    counts = {0:0, 1:0}\n",
    "    total = 0\n",
    "    for batch in dset.to_batches(columns=[label_col]):\n",
    "        s = batch.column(0).to_pandas()\n",
    "        vc = s.value_counts(dropna=True)\n",
    "        for k,v in vc.items():\n",
    "            if k in (0,1):\n",
    "                counts[int(k)] += int(v)\n",
    "        total += len(s)\n",
    "    if total == 0:\n",
    "        return None\n",
    "    ratios = {k: counts[k]/total for k in sorted(counts)}\n",
    "    return ratios\n",
    "\n",
    "def amount_stats_stream(path: Path, amt_col: str, sample_size=200_000):\n",
    "    \"\"\"\n",
    "    Streaming mean/std/min/max + approximate quantiles via reservoir sample.\n",
    "    \"\"\"\n",
    "    dset = ds.dataset(path, format=\"parquet\")\n",
    "\n",
    "    n = 0\n",
    "    mean = 0.0\n",
    "    M2 = 0.0\n",
    "    vmin = math.inf\n",
    "    vmax = -math.inf\n",
    "\n",
    "    # reservoir sample for quantiles\n",
    "    sample = []\n",
    "    k = sample_size\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    for batch in dset.to_batches(columns=[amt_col]):\n",
    "        arr = batch.column(0).to_pandas()\n",
    "        arr = pd.to_numeric(arr, errors=\"coerce\").dropna().to_numpy(dtype=\"float64\")\n",
    "        if arr.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Welford online updates\n",
    "        for x in arr:\n",
    "            n += 1\n",
    "            delta = x - mean\n",
    "            mean += delta / n\n",
    "            M2 += delta * (x - mean)\n",
    "        vmin = min(vmin, float(np.min(arr)))\n",
    "        vmax = max(vmax, float(np.max(arr)))\n",
    "\n",
    "        # Reservoir sampling\n",
    "        if len(sample) < k:\n",
    "            need = k - len(sample)\n",
    "            sample.extend(arr[:need].tolist())\n",
    "            start = need\n",
    "        else:\n",
    "            start = 0\n",
    "        for i in range(start, arr.size):\n",
    "            j = rng.randint(1, n)\n",
    "            if j <= k:\n",
    "                idx = rng.randint(0, k-1)\n",
    "                sample[idx] = float(arr[i])\n",
    "\n",
    "    if n == 0:\n",
    "        return None\n",
    "\n",
    "    std = math.sqrt(M2/(n-1)) if n > 1 else 0.0\n",
    "    qs = np.quantile(np.array(sample, dtype=\"float64\"),\n",
    "                     [0.01, 0.05, 0.50, 0.95, 0.99])\n",
    "    return {\n",
    "        \"count\": n,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"min\": vmin,\n",
    "        \"1%\":  qs[0],\n",
    "        \"5%\":  qs[1],\n",
    "        \"50%\": qs[2],\n",
    "        \"95%\": qs[3],\n",
    "        \"99%\": qs[4],\n",
    "        \"max\": vmax,\n",
    "    }\n",
    "\n",
    "for name, path in FILES.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    amt_col = find_amount_col(path)\n",
    "    if not amt_col:\n",
    "        print(\"⚠️ No amount-like column found.\")\n",
    "        continue\n",
    "\n",
    "    # labels (streamed)\n",
    "    ratios = label_ratio_stream(path, \"Is Laundering\")\n",
    "    if ratios is not None:\n",
    "        print(\"Label ratio:\", {k: round(v,6) for k,v in ratios.items()})\n",
    "    else:\n",
    "        print(\"No labels found\")\n",
    "\n",
    "    # amount stats (streamed; safe for 30M+ rows)\n",
    "    stats = amount_stats_stream(path, amt_col, sample_size=200_000)\n",
    "    if stats:\n",
    "        for k in [\"count\",\"mean\",\"std\",\"min\",\"1%\",\"5%\",\"50%\",\"95%\",\"99%\",\"max\"]:\n",
    "            print(f\"{k:>5}: {stats[k]:,.6f}\" if isinstance(stats[k], float) else f\"{k:>5}: {stats[k]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0233b69-0504-465d-8578-4a8956a3c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Processing: HI_Small_merged.parquet\n",
      "✅ Saved engineered file → data\\processed\\HI_Small_fe.parquet\n",
      "\n",
      "🔹 Processing: LI_Small_merged.parquet\n",
      "✅ Saved engineered file → data\\processed\\LI_Small_fe.parquet\n",
      "\n",
      "🔹 Processing: HI_Medium_merged.parquet\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 243. MiB for an array with shape (31898510,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHI_Small_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLI_Small_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHI_Medium_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLI_Medium_merged.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mfeature_engineer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mfeature_engineer\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# aggregate statistics per account (sender side)\u001b[39;00m\n\u001b[32m     19\u001b[39m acct_group = (\n\u001b[32m     20\u001b[39m     df.groupby(\u001b[33m\"\u001b[39m\u001b[33mFrom Account\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33mAmount Paid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m       .agg([\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     22\u001b[39m       .rename(columns=\u001b[38;5;28;01mlambda\u001b[39;00m c: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33macct_from_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43macct_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFrom Account\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# transaction value ratio to sender’s mean\u001b[39;00m\n\u001b[32m     27\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mratio_to_mean\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mAmount Paid\u001b[39m\u001b[33m\"\u001b[39m] / (df[\u001b[33m\"\u001b[39m\u001b[33macct_from_mean\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10784\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[39m\n\u001b[32m  10774\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  10775\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m  10776\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10777\u001b[39m             other,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10782\u001b[39m             validate=validate,\n\u001b[32m  10783\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10785\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10788\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10789\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10790\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10791\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10792\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10794\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10795\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m  10796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m result = \u001b[38;5;28mself\u001b[39m._reindex_and_concat(\n\u001b[32m    889\u001b[39m     join_index, left_indexer, right_indexer, copy=copy\n\u001b[32m    890\u001b[39m )\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1143\u001b[39m, in \u001b[36m_MergeOperation._get_join_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1138\u001b[39m     join_index, left_indexer, right_indexer = left_ax.join(\n\u001b[32m   1139\u001b[39m         right_ax, how=\u001b[38;5;28mself\u001b[39m.how, return_indexers=\u001b[38;5;28;01mTrue\u001b[39;00m, sort=\u001b[38;5;28mself\u001b[39m.sort\n\u001b[32m   1140\u001b[39m     )\n\u001b[32m   1142\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.how == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     join_index, left_indexer, right_indexer = \u001b[43m_left_join_on_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_ax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_ax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.left_index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.how == \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1148\u001b[39m     join_index, right_indexer, left_indexer = _left_join_on_index(\n\u001b[32m   1149\u001b[39m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m.right_join_keys, sort=\u001b[38;5;28mself\u001b[39m.sort\n\u001b[32m   1150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2386\u001b[39m, in \u001b[36m_left_join_on_index\u001b[39m\u001b[34m(left_ax, right_ax, join_keys, sort)\u001b[39m\n\u001b[32m   2383\u001b[39m     rkey = right_ax._values  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   2385\u001b[39m left_key, right_key, count = _factorize_keys(lkey, rkey, sort=sort)\n\u001b[32m-> \u001b[39m\u001b[32m2386\u001b[39m left_indexer, right_indexer = \u001b[43mlibjoin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleft_outer_join\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\n\u001b[32m   2388\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(left_ax) != \u001b[38;5;28mlen\u001b[39m(left_indexer):\n\u001b[32m   2391\u001b[39m     \u001b[38;5;66;03m# if asked to sort or there are 1-to-many matches\u001b[39;00m\n\u001b[32m   2392\u001b[39m     join_index = left_ax.take(left_indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/join.pyx:152\u001b[39m, in \u001b[36mpandas._libs.join.left_outer_join\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 243. MiB for an array with shape (31898510,) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "SAVE = Path(\"data/processed\")\n",
    "SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def feature_engineer(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    print(f\"\\n🔹 Processing: {path.name}\")\n",
    "\n",
    "    # basic temporal features\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "    df[\"hour\"] = df[\"Timestamp\"].dt.hour\n",
    "    df[\"day\"] = df[\"Timestamp\"].dt.day\n",
    "    df[\"weekday\"] = df[\"Timestamp\"].dt.weekday\n",
    "\n",
    "    # aggregate statistics per account (sender side)\n",
    "    acct_group = (\n",
    "        df.groupby(\"From Account\")[\"Amount Paid\"]\n",
    "          .agg([\"count\",\"sum\",\"mean\",\"std\",\"max\",\"min\"])\n",
    "          .rename(columns=lambda c: f\"acct_from_{c}\")\n",
    "    )\n",
    "    df = df.join(acct_group, on=\"From Account\", how=\"left\")\n",
    "\n",
    "    # transaction value ratio to sender’s mean\n",
    "    df[\"ratio_to_mean\"] = df[\"Amount Paid\"] / (df[\"acct_from_mean\"] + 1e-6)\n",
    "\n",
    "    # replace NaNs from low-activity accounts\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    out = SAVE / path.name.replace(\"_merged.parquet\", \"_fe.parquet\")\n",
    "    df.to_parquet(out, index=False)\n",
    "    print(f\"✅ Saved engineered file → {out}\")\n",
    "    return df\n",
    "\n",
    "for f in [\n",
    "    \"HI_Small_merged.parquet\",\n",
    "    \"LI_Small_merged.parquet\",\n",
    "    \"HI_Medium_merged.parquet\",\n",
    "    \"LI_Medium_merged.parquet\"\n",
    "]:\n",
    "    feature_engineer(DATA / f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59860df7-254c-450b-97b5-c95de8d04dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Streaming feature engineering for Medium sets ---\n",
    "import gc, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow as pa, pyarrow.dataset as ds, pyarrow.parquet as pq\n",
    "\n",
    "DATA = Path(\"data/interim\")\n",
    "PROC = Path(\"data/processed\"); PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACCT_COL = \"From Account\"\n",
    "AMT_COL  = \"Amount Paid\"\n",
    "TIME_COL = \"Timestamp\"\n",
    "\n",
    "def build_fromacct_stats(src_parquet: Path, stats_out: Path):\n",
    "    \"\"\"\n",
    "    Pass 1: read only From Account + Amount Paid in Arrow batches and\n",
    "    accumulate count/sum/sum_sq/min/max. Then write a compact stats parquet.\n",
    "    \"\"\"\n",
    "    acc = {}  # acct -> [count,sum,sum_sq,min,max]\n",
    "    dset = ds.dataset(src_parquet, format=\"parquet\")\n",
    "    for b in dset.to_batches(columns=[ACCT_COL, AMT_COL]):\n",
    "        df = b.to_pandas()[[ACCT_COL, AMT_COL]].copy()\n",
    "        df[ACCT_COL] = df[ACCT_COL].astype(str)\n",
    "        df[AMT_COL]  = pd.to_numeric(df[AMT_COL], errors=\"coerce\").fillna(0.0)\n",
    "        g = df.groupby(ACCT_COL)[AMT_COL].agg(['count','sum','mean','min','max'])  # mean only for speed\n",
    "        # also need sum of squares for std -> compute quickly:\n",
    "        sq = (df[AMT_COL]**2).groupby(df[ACCT_COL]).sum().rename('sum_sq')\n",
    "        g = g.join(sq)\n",
    "        for acct, row in g.iterrows():\n",
    "            c, s, _, mn, mx, sqsum = int(row['count']), float(row['sum']), row['mean'], float(row['min']), float(row['max']), float(row['sum_sq'])\n",
    "            if acct not in acc:\n",
    "                acc[acct] = [c, s, sqsum, mn, mx]\n",
    "            else:\n",
    "                C,S,S2,MN,MX = acc[acct]\n",
    "                acc[acct] = [C+c, S+s, S2+sqsum, min(MN,mn), max(MX,mx)]\n",
    "        del df, g, sq\n",
    "        gc.collect()\n",
    "\n",
    "    # finalize dataframe\n",
    "    rows = []\n",
    "    for acct,(C,S,S2,MN,MX) in acc.items():\n",
    "        mean = S / C if C else 0.0\n",
    "        var  = max((S2 / C) - mean**2, 0.0) if C else 0.0\n",
    "        std  = math.sqrt(var)\n",
    "        rows.append((acct,C,S,mean,std,MN,MX))\n",
    "    stats = pd.DataFrame(rows, columns=[ACCT_COL,'acct_from_count','acct_from_sum','acct_from_mean','acct_from_std','acct_from_min','acct_from_max'])\n",
    "    stats.to_parquet(stats_out, index=False)\n",
    "    print(f\"✅ wrote stats → {stats_out}  (unique accounts: {len(stats):,})\")\n",
    "    return stats\n",
    "\n",
    "def apply_stats_chunked(src_parquet: Path, stats_df: pd.DataFrame, dst_parquet: Path):\n",
    "    \"\"\"\n",
    "    Pass 2: iterate source in Arrow batches, add time features + ratio_to_mean,\n",
    "    merge with stats_df (small), and append to ParquetWriter.\n",
    "    \"\"\"\n",
    "    dset = ds.dataset(src_parquet, format=\"parquet\")\n",
    "    writer = None\n",
    "    for i, b in enumerate(dset.to_batches()):\n",
    "        df = b.to_pandas()\n",
    "        # light types\n",
    "        df[ACCT_COL] = df[ACCT_COL].astype(str)\n",
    "        df[AMT_COL]  = pd.to_numeric(df[AMT_COL], errors=\"coerce\").fillna(0.0)\n",
    "        # time features\n",
    "        df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
    "        df[\"hour\"]    = df[TIME_COL].dt.hour\n",
    "        df[\"day\"]     = df[TIME_COL].dt.day\n",
    "        df[\"weekday\"] = df[TIME_COL].dt.weekday\n",
    "        # merge small stats df\n",
    "        df = df.merge(stats_df, on=ACCT_COL, how=\"left\")\n",
    "        # derived ratio\n",
    "        df[\"ratio_to_mean\"] = df[AMT_COL] / (df[\"acct_from_mean\"].replace(0, np.nan) + 1e-6)\n",
    "        df[\"ratio_to_mean\"] = df[\"ratio_to_mean\"].fillna(0.0)\n",
    "\n",
    "        tbl = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(dst_parquet, schema=tbl.schema)\n",
    "        writer.write_table(tbl)\n",
    "        print(f\"  wrote batch {i+1}\")\n",
    "        del df, tbl, b\n",
    "        gc.collect()\n",
    "    if writer: writer.close()\n",
    "    print(f\"✅ finished → {dst_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce2fba11-cd65-4557-acb8-9b18fcd169e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote stats → data\\processed\\HI_Medium_fromacct_stats.parquet  (unique accounts: 2,013,627)\n",
      "  wrote batch 1\n",
      "  wrote batch 2\n",
      "  wrote batch 3\n",
      "  wrote batch 4\n",
      "  wrote batch 5\n",
      "  wrote batch 6\n",
      "  wrote batch 7\n",
      "  wrote batch 8\n",
      "  wrote batch 9\n",
      "  wrote batch 10\n",
      "  wrote batch 11\n",
      "  wrote batch 12\n",
      "  wrote batch 13\n",
      "  wrote batch 14\n",
      "  wrote batch 15\n",
      "  wrote batch 16\n",
      "  wrote batch 17\n",
      "  wrote batch 18\n",
      "  wrote batch 19\n",
      "  wrote batch 20\n",
      "  wrote batch 21\n",
      "  wrote batch 22\n",
      "  wrote batch 23\n",
      "  wrote batch 24\n",
      "  wrote batch 25\n",
      "  wrote batch 26\n",
      "  wrote batch 27\n",
      "  wrote batch 28\n",
      "  wrote batch 29\n",
      "  wrote batch 30\n",
      "  wrote batch 31\n",
      "  wrote batch 32\n",
      "  wrote batch 33\n",
      "  wrote batch 34\n",
      "  wrote batch 35\n",
      "  wrote batch 36\n",
      "  wrote batch 37\n",
      "  wrote batch 38\n",
      "  wrote batch 39\n",
      "  wrote batch 40\n",
      "  wrote batch 41\n",
      "  wrote batch 42\n",
      "  wrote batch 43\n",
      "  wrote batch 44\n",
      "  wrote batch 45\n",
      "  wrote batch 46\n",
      "  wrote batch 47\n",
      "  wrote batch 48\n",
      "  wrote batch 49\n",
      "  wrote batch 50\n",
      "  wrote batch 51\n",
      "  wrote batch 52\n",
      "  wrote batch 53\n",
      "  wrote batch 54\n",
      "  wrote batch 55\n",
      "  wrote batch 56\n",
      "  wrote batch 57\n",
      "  wrote batch 58\n",
      "  wrote batch 59\n",
      "  wrote batch 60\n",
      "  wrote batch 61\n",
      "  wrote batch 62\n",
      "  wrote batch 63\n",
      "  wrote batch 64\n",
      "  wrote batch 65\n",
      "  wrote batch 66\n",
      "  wrote batch 67\n",
      "  wrote batch 68\n",
      "  wrote batch 69\n",
      "  wrote batch 70\n",
      "  wrote batch 71\n",
      "  wrote batch 72\n",
      "  wrote batch 73\n",
      "  wrote batch 74\n",
      "  wrote batch 75\n",
      "  wrote batch 76\n",
      "  wrote batch 77\n",
      "  wrote batch 78\n",
      "  wrote batch 79\n",
      "  wrote batch 80\n",
      "  wrote batch 81\n",
      "  wrote batch 82\n",
      "  wrote batch 83\n",
      "  wrote batch 84\n",
      "  wrote batch 85\n",
      "  wrote batch 86\n",
      "  wrote batch 87\n",
      "  wrote batch 88\n",
      "  wrote batch 89\n",
      "  wrote batch 90\n",
      "  wrote batch 91\n",
      "  wrote batch 92\n",
      "  wrote batch 93\n",
      "  wrote batch 94\n",
      "  wrote batch 95\n",
      "  wrote batch 96\n",
      "  wrote batch 97\n",
      "  wrote batch 98\n",
      "  wrote batch 99\n",
      "  wrote batch 100\n",
      "  wrote batch 101\n",
      "  wrote batch 102\n",
      "  wrote batch 103\n",
      "  wrote batch 104\n",
      "  wrote batch 105\n",
      "  wrote batch 106\n",
      "  wrote batch 107\n",
      "  wrote batch 108\n",
      "  wrote batch 109\n",
      "  wrote batch 110\n",
      "  wrote batch 111\n",
      "  wrote batch 112\n",
      "  wrote batch 113\n",
      "  wrote batch 114\n",
      "  wrote batch 115\n",
      "  wrote batch 116\n",
      "  wrote batch 117\n",
      "  wrote batch 118\n",
      "  wrote batch 119\n",
      "  wrote batch 120\n",
      "  wrote batch 121\n",
      "  wrote batch 122\n",
      "  wrote batch 123\n",
      "  wrote batch 124\n",
      "  wrote batch 125\n",
      "  wrote batch 126\n",
      "  wrote batch 127\n",
      "  wrote batch 128\n",
      "  wrote batch 129\n",
      "  wrote batch 130\n",
      "  wrote batch 131\n",
      "  wrote batch 132\n",
      "  wrote batch 133\n",
      "  wrote batch 134\n",
      "  wrote batch 135\n",
      "  wrote batch 136\n",
      "  wrote batch 137\n",
      "  wrote batch 138\n",
      "  wrote batch 139\n",
      "  wrote batch 140\n",
      "  wrote batch 141\n",
      "  wrote batch 142\n",
      "  wrote batch 143\n",
      "  wrote batch 144\n",
      "  wrote batch 145\n",
      "  wrote batch 146\n",
      "  wrote batch 147\n",
      "  wrote batch 148\n",
      "  wrote batch 149\n",
      "  wrote batch 150\n",
      "  wrote batch 151\n",
      "  wrote batch 152\n",
      "  wrote batch 153\n",
      "  wrote batch 154\n",
      "  wrote batch 155\n",
      "  wrote batch 156\n",
      "  wrote batch 157\n",
      "  wrote batch 158\n",
      "  wrote batch 159\n",
      "  wrote batch 160\n",
      "  wrote batch 161\n",
      "  wrote batch 162\n",
      "  wrote batch 163\n",
      "  wrote batch 164\n",
      "  wrote batch 165\n",
      "  wrote batch 166\n",
      "  wrote batch 167\n",
      "  wrote batch 168\n",
      "  wrote batch 169\n",
      "  wrote batch 170\n",
      "  wrote batch 171\n",
      "  wrote batch 172\n",
      "  wrote batch 173\n",
      "  wrote batch 174\n",
      "  wrote batch 175\n",
      "  wrote batch 176\n",
      "  wrote batch 177\n",
      "  wrote batch 178\n",
      "  wrote batch 179\n",
      "  wrote batch 180\n",
      "  wrote batch 181\n",
      "  wrote batch 182\n",
      "  wrote batch 183\n",
      "  wrote batch 184\n",
      "  wrote batch 185\n",
      "  wrote batch 186\n",
      "  wrote batch 187\n",
      "  wrote batch 188\n",
      "  wrote batch 189\n",
      "  wrote batch 190\n",
      "  wrote batch 191\n",
      "  wrote batch 192\n",
      "  wrote batch 193\n",
      "  wrote batch 194\n",
      "  wrote batch 195\n",
      "  wrote batch 196\n",
      "  wrote batch 197\n",
      "  wrote batch 198\n",
      "  wrote batch 199\n",
      "  wrote batch 200\n",
      "  wrote batch 201\n",
      "  wrote batch 202\n",
      "  wrote batch 203\n",
      "  wrote batch 204\n",
      "  wrote batch 205\n",
      "  wrote batch 206\n",
      "  wrote batch 207\n",
      "  wrote batch 208\n",
      "  wrote batch 209\n",
      "  wrote batch 210\n",
      "  wrote batch 211\n",
      "  wrote batch 212\n",
      "  wrote batch 213\n",
      "  wrote batch 214\n",
      "  wrote batch 215\n",
      "  wrote batch 216\n",
      "  wrote batch 217\n",
      "  wrote batch 218\n",
      "  wrote batch 219\n",
      "  wrote batch 220\n",
      "  wrote batch 221\n",
      "  wrote batch 222\n",
      "  wrote batch 223\n",
      "  wrote batch 224\n",
      "  wrote batch 225\n",
      "  wrote batch 226\n",
      "  wrote batch 227\n",
      "  wrote batch 228\n",
      "  wrote batch 229\n",
      "  wrote batch 230\n",
      "  wrote batch 231\n",
      "  wrote batch 232\n",
      "  wrote batch 233\n",
      "  wrote batch 234\n",
      "  wrote batch 235\n",
      "  wrote batch 236\n",
      "  wrote batch 237\n",
      "  wrote batch 238\n",
      "  wrote batch 239\n",
      "  wrote batch 240\n",
      "  wrote batch 241\n",
      "  wrote batch 242\n",
      "  wrote batch 243\n",
      "  wrote batch 244\n",
      "  wrote batch 245\n",
      "  wrote batch 246\n",
      "  wrote batch 247\n",
      "  wrote batch 248\n",
      "  wrote batch 249\n",
      "  wrote batch 250\n",
      "  wrote batch 251\n",
      "  wrote batch 252\n",
      "  wrote batch 253\n",
      "  wrote batch 254\n",
      "  wrote batch 255\n",
      "  wrote batch 256\n",
      "  wrote batch 257\n",
      "  wrote batch 258\n",
      "  wrote batch 259\n",
      "  wrote batch 260\n",
      "  wrote batch 261\n",
      "  wrote batch 262\n",
      "  wrote batch 263\n",
      "  wrote batch 264\n",
      "  wrote batch 265\n",
      "  wrote batch 266\n",
      "  wrote batch 267\n",
      "  wrote batch 268\n",
      "  wrote batch 269\n",
      "  wrote batch 270\n",
      "  wrote batch 271\n",
      "  wrote batch 272\n",
      "  wrote batch 273\n",
      "  wrote batch 274\n",
      "  wrote batch 275\n",
      "  wrote batch 276\n",
      "  wrote batch 277\n",
      "  wrote batch 278\n",
      "  wrote batch 279\n",
      "  wrote batch 280\n",
      "  wrote batch 281\n",
      "  wrote batch 282\n",
      "  wrote batch 283\n",
      "  wrote batch 284\n",
      "  wrote batch 285\n",
      "  wrote batch 286\n",
      "  wrote batch 287\n",
      "  wrote batch 288\n",
      "  wrote batch 289\n",
      "  wrote batch 290\n",
      "  wrote batch 291\n",
      "  wrote batch 292\n",
      "  wrote batch 293\n",
      "  wrote batch 294\n",
      "  wrote batch 295\n",
      "  wrote batch 296\n",
      "  wrote batch 297\n",
      "  wrote batch 298\n",
      "  wrote batch 299\n",
      "  wrote batch 300\n",
      "  wrote batch 301\n",
      "  wrote batch 302\n",
      "  wrote batch 303\n",
      "✅ finished → data\\processed\\HI_Medium_fe.parquet\n",
      "✅ wrote stats → data\\processed\\LI_Medium_fromacct_stats.parquet  (unique accounts: 1,969,950)\n",
      "  wrote batch 1\n",
      "  wrote batch 2\n",
      "  wrote batch 3\n",
      "  wrote batch 4\n",
      "  wrote batch 5\n",
      "  wrote batch 6\n",
      "  wrote batch 7\n",
      "  wrote batch 8\n",
      "  wrote batch 9\n",
      "  wrote batch 10\n",
      "  wrote batch 11\n",
      "  wrote batch 12\n",
      "  wrote batch 13\n",
      "  wrote batch 14\n",
      "  wrote batch 15\n",
      "  wrote batch 16\n",
      "  wrote batch 17\n",
      "  wrote batch 18\n",
      "  wrote batch 19\n",
      "  wrote batch 20\n",
      "  wrote batch 21\n",
      "  wrote batch 22\n",
      "  wrote batch 23\n",
      "  wrote batch 24\n",
      "  wrote batch 25\n",
      "  wrote batch 26\n",
      "  wrote batch 27\n",
      "  wrote batch 28\n",
      "  wrote batch 29\n",
      "  wrote batch 30\n",
      "  wrote batch 31\n",
      "  wrote batch 32\n",
      "  wrote batch 33\n",
      "  wrote batch 34\n",
      "  wrote batch 35\n",
      "  wrote batch 36\n",
      "  wrote batch 37\n",
      "  wrote batch 38\n",
      "  wrote batch 39\n",
      "  wrote batch 40\n",
      "  wrote batch 41\n",
      "  wrote batch 42\n",
      "  wrote batch 43\n",
      "  wrote batch 44\n",
      "  wrote batch 45\n",
      "  wrote batch 46\n",
      "  wrote batch 47\n",
      "  wrote batch 48\n",
      "  wrote batch 49\n",
      "  wrote batch 50\n",
      "  wrote batch 51\n",
      "  wrote batch 52\n",
      "  wrote batch 53\n",
      "  wrote batch 54\n",
      "  wrote batch 55\n",
      "  wrote batch 56\n",
      "  wrote batch 57\n",
      "  wrote batch 58\n",
      "  wrote batch 59\n",
      "  wrote batch 60\n",
      "  wrote batch 61\n",
      "  wrote batch 62\n",
      "  wrote batch 63\n",
      "  wrote batch 64\n",
      "  wrote batch 65\n",
      "  wrote batch 66\n",
      "  wrote batch 67\n",
      "  wrote batch 68\n",
      "  wrote batch 69\n",
      "  wrote batch 70\n",
      "  wrote batch 71\n",
      "  wrote batch 72\n",
      "  wrote batch 73\n",
      "  wrote batch 74\n",
      "  wrote batch 75\n",
      "  wrote batch 76\n",
      "  wrote batch 77\n",
      "  wrote batch 78\n",
      "  wrote batch 79\n",
      "  wrote batch 80\n",
      "  wrote batch 81\n",
      "  wrote batch 82\n",
      "  wrote batch 83\n",
      "  wrote batch 84\n",
      "  wrote batch 85\n",
      "  wrote batch 86\n",
      "  wrote batch 87\n",
      "  wrote batch 88\n",
      "  wrote batch 89\n",
      "  wrote batch 90\n",
      "  wrote batch 91\n",
      "  wrote batch 92\n",
      "  wrote batch 93\n",
      "  wrote batch 94\n",
      "  wrote batch 95\n",
      "  wrote batch 96\n",
      "  wrote batch 97\n",
      "  wrote batch 98\n",
      "  wrote batch 99\n",
      "  wrote batch 100\n",
      "  wrote batch 101\n",
      "  wrote batch 102\n",
      "  wrote batch 103\n",
      "  wrote batch 104\n",
      "  wrote batch 105\n",
      "  wrote batch 106\n",
      "  wrote batch 107\n",
      "  wrote batch 108\n",
      "  wrote batch 109\n",
      "  wrote batch 110\n",
      "  wrote batch 111\n",
      "  wrote batch 112\n",
      "  wrote batch 113\n",
      "  wrote batch 114\n",
      "  wrote batch 115\n",
      "  wrote batch 116\n",
      "  wrote batch 117\n",
      "  wrote batch 118\n",
      "  wrote batch 119\n",
      "  wrote batch 120\n",
      "  wrote batch 121\n",
      "  wrote batch 122\n",
      "  wrote batch 123\n",
      "  wrote batch 124\n",
      "  wrote batch 125\n",
      "  wrote batch 126\n",
      "  wrote batch 127\n",
      "  wrote batch 128\n",
      "  wrote batch 129\n",
      "  wrote batch 130\n",
      "  wrote batch 131\n",
      "  wrote batch 132\n",
      "  wrote batch 133\n",
      "  wrote batch 134\n",
      "  wrote batch 135\n",
      "  wrote batch 136\n",
      "  wrote batch 137\n",
      "  wrote batch 138\n",
      "  wrote batch 139\n",
      "  wrote batch 140\n",
      "  wrote batch 141\n",
      "  wrote batch 142\n",
      "  wrote batch 143\n",
      "  wrote batch 144\n",
      "  wrote batch 145\n",
      "  wrote batch 146\n",
      "  wrote batch 147\n",
      "  wrote batch 148\n",
      "  wrote batch 149\n",
      "  wrote batch 150\n",
      "  wrote batch 151\n",
      "  wrote batch 152\n",
      "  wrote batch 153\n",
      "  wrote batch 154\n",
      "  wrote batch 155\n",
      "  wrote batch 156\n",
      "  wrote batch 157\n",
      "  wrote batch 158\n",
      "  wrote batch 159\n",
      "  wrote batch 160\n",
      "  wrote batch 161\n",
      "  wrote batch 162\n",
      "  wrote batch 163\n",
      "  wrote batch 164\n",
      "  wrote batch 165\n",
      "  wrote batch 166\n",
      "  wrote batch 167\n",
      "  wrote batch 168\n",
      "  wrote batch 169\n",
      "  wrote batch 170\n",
      "  wrote batch 171\n",
      "  wrote batch 172\n",
      "  wrote batch 173\n",
      "  wrote batch 174\n",
      "  wrote batch 175\n",
      "  wrote batch 176\n",
      "  wrote batch 177\n",
      "  wrote batch 178\n",
      "  wrote batch 179\n",
      "  wrote batch 180\n",
      "  wrote batch 181\n",
      "  wrote batch 182\n",
      "  wrote batch 183\n",
      "  wrote batch 184\n",
      "  wrote batch 185\n",
      "  wrote batch 186\n",
      "  wrote batch 187\n",
      "  wrote batch 188\n",
      "  wrote batch 189\n",
      "  wrote batch 190\n",
      "  wrote batch 191\n",
      "  wrote batch 192\n",
      "  wrote batch 193\n",
      "  wrote batch 194\n",
      "  wrote batch 195\n",
      "  wrote batch 196\n",
      "  wrote batch 197\n",
      "  wrote batch 198\n",
      "  wrote batch 199\n",
      "  wrote batch 200\n",
      "  wrote batch 201\n",
      "  wrote batch 202\n",
      "  wrote batch 203\n",
      "  wrote batch 204\n",
      "  wrote batch 205\n",
      "  wrote batch 206\n",
      "  wrote batch 207\n",
      "  wrote batch 208\n",
      "  wrote batch 209\n",
      "  wrote batch 210\n",
      "  wrote batch 211\n",
      "  wrote batch 212\n",
      "  wrote batch 213\n",
      "  wrote batch 214\n",
      "  wrote batch 215\n",
      "  wrote batch 216\n",
      "  wrote batch 217\n",
      "  wrote batch 218\n",
      "  wrote batch 219\n",
      "  wrote batch 220\n",
      "  wrote batch 221\n",
      "  wrote batch 222\n",
      "  wrote batch 223\n",
      "  wrote batch 224\n",
      "  wrote batch 225\n",
      "  wrote batch 226\n",
      "  wrote batch 227\n",
      "  wrote batch 228\n",
      "  wrote batch 229\n",
      "  wrote batch 230\n",
      "  wrote batch 231\n",
      "  wrote batch 232\n",
      "  wrote batch 233\n",
      "  wrote batch 234\n",
      "  wrote batch 235\n",
      "  wrote batch 236\n",
      "  wrote batch 237\n",
      "  wrote batch 238\n",
      "  wrote batch 239\n",
      "  wrote batch 240\n",
      "  wrote batch 241\n",
      "  wrote batch 242\n",
      "  wrote batch 243\n",
      "  wrote batch 244\n",
      "  wrote batch 245\n",
      "  wrote batch 246\n",
      "  wrote batch 247\n",
      "  wrote batch 248\n",
      "  wrote batch 249\n",
      "  wrote batch 250\n",
      "  wrote batch 251\n",
      "  wrote batch 252\n",
      "  wrote batch 253\n",
      "  wrote batch 254\n",
      "  wrote batch 255\n",
      "  wrote batch 256\n",
      "  wrote batch 257\n",
      "  wrote batch 258\n",
      "  wrote batch 259\n",
      "  wrote batch 260\n",
      "  wrote batch 261\n",
      "  wrote batch 262\n",
      "  wrote batch 263\n",
      "  wrote batch 264\n",
      "  wrote batch 265\n",
      "  wrote batch 266\n",
      "  wrote batch 267\n",
      "  wrote batch 268\n",
      "  wrote batch 269\n",
      "  wrote batch 270\n",
      "  wrote batch 271\n",
      "  wrote batch 272\n",
      "  wrote batch 273\n",
      "  wrote batch 274\n",
      "  wrote batch 275\n",
      "  wrote batch 276\n",
      "  wrote batch 277\n",
      "  wrote batch 278\n",
      "  wrote batch 279\n",
      "  wrote batch 280\n",
      "  wrote batch 281\n",
      "  wrote batch 282\n",
      "  wrote batch 283\n",
      "  wrote batch 284\n",
      "  wrote batch 285\n",
      "  wrote batch 286\n",
      "  wrote batch 287\n",
      "  wrote batch 288\n",
      "  wrote batch 289\n",
      "  wrote batch 290\n",
      "  wrote batch 291\n",
      "  wrote batch 292\n",
      "  wrote batch 293\n",
      "  wrote batch 294\n",
      "  wrote batch 295\n",
      "  wrote batch 296\n",
      "  wrote batch 297\n",
      "  wrote batch 298\n",
      "  wrote batch 299\n",
      "  wrote batch 300\n",
      "  wrote batch 301\n",
      "  wrote batch 302\n",
      "  wrote batch 303\n",
      "  wrote batch 304\n",
      "  wrote batch 305\n",
      "  wrote batch 306\n",
      "  wrote batch 307\n",
      "  wrote batch 308\n",
      "  wrote batch 309\n",
      "  wrote batch 310\n",
      "  wrote batch 311\n",
      "✅ finished → data\\processed\\LI_Medium_fe.parquet\n"
     ]
    }
   ],
   "source": [
    "# HI_Medium\n",
    "hi_src   = DATA / \"HI_Medium_merged.parquet\"\n",
    "hi_stats = PROC / \"HI_Medium_fromacct_stats.parquet\"\n",
    "hi_dst   = PROC / \"HI_Medium_fe.parquet\"\n",
    "\n",
    "stats_hi = build_fromacct_stats(hi_src, hi_stats)      # pass 1 (tiny output)\n",
    "apply_stats_chunked(hi_src, stats_hi, hi_dst)          # pass 2 (streaming writer)\n",
    "\n",
    "# LI_Medium\n",
    "li_src   = DATA / \"LI_Medium_merged.parquet\"\n",
    "li_stats = PROC / \"LI_Medium_fromacct_stats.parquet\"\n",
    "li_dst   = PROC / \"LI_Medium_fe.parquet\"\n",
    "\n",
    "stats_li = build_fromacct_stats(li_src, li_stats)\n",
    "apply_stats_chunked(li_src, stats_li, li_dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a18f1db-271f-40db-9330-f828be147a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 15\n",
      "['From Bank', 'To Bank', 'Amount Received', 'Amount Paid', 'acct_Bank ID', 'hour', 'day', 'weekday', 'acct_from_count', 'acct_from_sum', 'acct_from_mean', 'acct_from_std', 'acct_from_max', 'acct_from_min', 'ratio_to_mean'] ...\n",
      "Class ratio: {0: 0.9992716453067613, 1: 0.0007283546932386988}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROC = Path(\"data/processed\")\n",
    "\n",
    "hi = pd.read_parquet(PROC/\"HI_Small_fe.parquet\")\n",
    "li = pd.read_parquet(PROC/\"LI_Small_fe.parquet\")\n",
    "df = pd.concat([hi, li], ignore_index=True)   # combine both smalls\n",
    "\n",
    "label_col = \"Is Laundering\"\n",
    "id_like = {\"Transaction ID\", \"From Account\", \"To Account\", \"Timestamp\"}\n",
    "\n",
    "num_cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in id_like | {label_col}\n",
    "       and pd.api.types.is_numeric_dtype(df[c])\n",
    "]\n",
    "print(\"Features:\", len(num_cols))\n",
    "print(num_cols[:15], \"...\")\n",
    "print(\"Class ratio:\", df[label_col].value_counts(normalize=True).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee4a292-de50-4918-a57d-797ca98da15d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.07 GiB for an array with shape (9601915, 15) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m X_train, X_val, y_train, y_val = train_test_split(\n\u001b[32m      8\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_train_s = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m X_val_s   = scaler.transform(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1029\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1026\u001b[39m         \u001b[38;5;28mself\u001b[39m.n_samples_seen_ += X.shape[\u001b[32m0\u001b[39m] - np.isnan(X).sum(axis=\u001b[32m0\u001b[39m)\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m         \u001b[38;5;28mself\u001b[39m.mean_, \u001b[38;5;28mself\u001b[39m.var_, \u001b[38;5;28mself\u001b[39m.n_samples_seen_ = \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.ptp(\u001b[38;5;28mself\u001b[39m.n_samples_seen_) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1150\u001b[39m, in \u001b[36m_incremental_mean_and_var\u001b[39m\u001b[34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[39m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     T = new_sum / new_sample_count\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     temp = \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1152\u001b[39m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[32m   1153\u001b[39m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[32m   1154\u001b[39m         correction = _safe_accumulator_op(\n\u001b[32m   1155\u001b[39m             np.matmul, sample_weight, np.where(X_nan_mask, \u001b[32m0\u001b[39m, temp)\n\u001b[32m   1156\u001b[39m         )\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.07 GiB for an array with shape (9601915, 15) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[num_cols].fillna(0.0).values\n",
    "y = df[label_col].astype(int).values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c695240-587d-4313-98d3-9f0c96f49b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, roc_auc_score, confusion_matrix\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# === Load processed feature dataset ===\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# ✅ GPU-optimized incremental scaling + XGBoost model training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# === Load processed feature dataset ===\n",
    "df = pd.read_parquet(\"data/processed/HI_Small_fe.parquet\")\n",
    "label_col = \"Is Laundering\"\n",
    "\n",
    "# === Select numeric features ===\n",
    "num_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype in (\"int64\", \"float64\", \"float32\") and c != label_col\n",
    "]\n",
    "\n",
    "X = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "y = df[label_col].astype(int).values\n",
    "\n",
    "# === Train-validation split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Incremental normalization (memory-safe) ===\n",
    "scaler = StandardScaler()\n",
    "batch = 200_000\n",
    "n = X_train.shape[0]\n",
    "\n",
    "print(\"Fitting scaler in batches...\")\n",
    "for i in range(0, n, batch):\n",
    "    scaler.partial_fit(X_train[i:i+batch])\n",
    "\n",
    "print(\"Transforming data in batches...\")\n",
    "X_train_s = np.vstack([scaler.transform(X_train[i:i+batch]) for i in range(0, n, batch)])\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "# === GPU-based XGBoost training ===\n",
    "print(\"\\nTraining XGBoost on GPU...\")\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"gpu_hist\",     # GPU training\n",
    "    predictor=\"gpu_predictor\",\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),  # handle imbalance\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], verbose=100)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = (xgb.predict_proba(X_val_s)[:, 1] > 0.5).astype(int)\n",
    "print(\"\\n✅ Model Evaluation Results:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, xgb.predict_proba(X_val_s)[:, 1]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6886e9-4872-4136-b571-b94f96fbfddc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_has_cuda_support' from 'xgboost.core' (E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _has_cuda_support\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mXGBoost:\u001b[39m\u001b[33m\"\u001b[39m, xgboost.__version__)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPU available:\u001b[39m\u001b[33m\"\u001b[39m, _has_cuda_support())\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_has_cuda_support' from 'xgboost.core' (E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py)"
     ]
    }
   ],
   "source": [
    "import xgboost, numpy as np\n",
    "from xgboost.core import _has_cuda_support\n",
    "\n",
    "print(\"XGBoost:\", xgboost.__version__)\n",
    "print(\"GPU available:\", _has_cuda_support())\n",
    "\n",
    "# quick sanity fit on tiny data to confirm GPU path works\n",
    "from xgboost import XGBClassifier\n",
    "X = np.random.rand(1000, 10).astype(\"float32\")\n",
    "y = (X[:,0] + X[:,1] * 0.5 > 0.8).astype(int)\n",
    "clf = XGBClassifier(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", eval_metric=\"aucpr\")\n",
    "clf.fit(X, y)\n",
    "print(\"OK ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3d6a3b-0444-45e6-8780-e8eb7c54e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 2.1.1\n",
      "GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:16:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:16:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU training test successful!\n"
     ]
    }
   ],
   "source": [
    "import xgboost, numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"XGBoost version:\", xgboost.__version__)\n",
    "\n",
    "# GPU test model\n",
    "model = XGBClassifier(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\")\n",
    "print(\"GPU available:\", model.get_params()[\"tree_method\"] == \"gpu_hist\")\n",
    "\n",
    "# quick sanity fit\n",
    "X = np.random.rand(1000, 10).astype(\"float32\")\n",
    "y = (X[:, 0] + X[:, 1] * 0.5 > 0.8).astype(int)\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"✅ GPU training test successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0c92ba-dedc-47c4-9453-fa631b6d3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler in batches...\n",
      "Transforming data in batches...\n",
      "\n",
      "Training XGBoost on GPU...\n",
      "[0]\tvalidation_0-aucpr:0.00697\n",
      "[100]\tvalidation_0-aucpr:0.03995\n",
      "[200]\tvalidation_0-aucpr:0.05081\n",
      "[300]\tvalidation_0-aucpr:0.06049\n",
      "[400]\tvalidation_0-aucpr:0.06832\n",
      "[500]\tvalidation_0-aucpr:0.07859\n",
      "[599]\tvalidation_0-aucpr:0.08588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:18:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model Evaluation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9552    0.9770   1014634\n",
      "           1     0.0161    0.7169    0.0315      1035\n",
      "\n",
      "    accuracy                         0.9550   1015669\n",
      "   macro avg     0.5079    0.8361    0.5042   1015669\n",
      "weighted avg     0.9987    0.9550    0.9760   1015669\n",
      "\n",
      "ROC-AUC: 0.9439855592867503\n",
      "Confusion Matrix:\n",
      " [[969229  45405]\n",
      " [   293    742]]\n"
     ]
    }
   ],
   "source": [
    "# ✅ GPU-accelerated XGBoost training (v2.x syntax)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# === Load processed feature dataset ===\n",
    "df = pd.read_parquet(\"data/processed/HI_Small_fe.parquet\")\n",
    "label_col = \"Is Laundering\"\n",
    "\n",
    "# === Select numeric features ===\n",
    "num_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype in (\"int64\", \"float64\", \"float32\") and c != label_col\n",
    "]\n",
    "\n",
    "X = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "y = df[label_col].astype(int).values\n",
    "\n",
    "# === Train-validation split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Incremental normalization (memory-safe) ===\n",
    "scaler = StandardScaler()\n",
    "batch = 200_000\n",
    "n = X_train.shape[0]\n",
    "\n",
    "print(\"Fitting scaler in batches...\")\n",
    "for i in range(0, n, batch):\n",
    "    scaler.partial_fit(X_train[i:i+batch])\n",
    "\n",
    "print(\"Transforming data in batches...\")\n",
    "X_train_s = np.vstack([scaler.transform(X_train[i:i+batch]) for i in range(0, n, batch)])\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "# === GPU-based XGBoost training ===\n",
    "print(\"\\nTraining XGBoost on GPU...\")\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",      # updated syntax for XGBoost >=2.0\n",
    "    device=\"cuda\",           # enables GPU acceleration\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),  # handle imbalance\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], verbose=100)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = (xgb.predict_proba(X_val_s)[:, 1] > 0.5).astype(int)\n",
    "print(\"\\n✅ Model Evaluation Results:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, xgb.predict_proba(X_val_s)[:, 1]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894535fc-af87-48d5-b1e0-2c7a0044aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 0.08639565885121037\n",
      "→ Threshold 0.9879 | precision=0.200 | recall=0.056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9990    0.9998    0.9994   1014634\n",
      "           1     0.1993    0.0560    0.0875      1035\n",
      "\n",
      "    accuracy                         0.9988   1015669\n",
      "   macro avg     0.5992    0.5279    0.5434   1015669\n",
      "weighted avg     0.9982    0.9988    0.9985   1015669\n",
      "\n",
      "Confusion:\n",
      " [[1014401     233]\n",
      " [    977      58]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "proba = xgb.predict_proba(X_val_s)[:, 1]\n",
    "ap = average_precision_score(y_val, proba)\n",
    "print(\"PR-AUC:\", ap)\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_val, proba)\n",
    "\n",
    "def show_at_precision(target_p=0.2):  # e.g., 20% precision target\n",
    "    idx = np.where(prec >= target_p)[0]\n",
    "    if len(idx) == 0:\n",
    "        print(f\"No threshold reaches precision {target_p:.0%}. Max was {prec.max():.3f}\")\n",
    "        return None\n",
    "    j = idx[0]\n",
    "    t = thr[j-1] if j > 0 else 0.5\n",
    "    yhat = (proba >= t).astype(int)\n",
    "    print(f\"→ Threshold {t:.4f} | precision={prec[j]:.3f} | recall={rec[j]:.3f}\")\n",
    "    print(classification_report(y_val, yhat, digits=4))\n",
    "    print(\"Confusion:\\n\", confusion_matrix(y_val, yhat))\n",
    "    return t\n",
    "\n",
    "best_t = show_at_precision(0.20)   # try 10%, 20%, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e371d219-0bdb-482e-9afd-83407cabd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@0.10% = 0.172  (out of 1,015 alerts)\n",
      "P@0.20% = 0.138  (out of 2,031 alerts)\n",
      "P@0.50% = 0.090  (out of 5,078 alerts)\n",
      "P@1.00% = 0.057  (out of 10,156 alerts)\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(k_ratio=0.005):  # 0.5%\n",
    "    k = max(1, int(k_ratio * len(proba)))\n",
    "    idx = np.argsort(-proba)[:k]\n",
    "    p_at_k = y_val[idx].mean()\n",
    "    print(f\"P@{k_ratio*100:.2f}% = {p_at_k:.3f}  (out of {k:,} alerts)\")\n",
    "    return p_at_k\n",
    "\n",
    "for r in [0.001, 0.002, 0.005, 0.01]:\n",
    "    precision_at_k(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031b3c24-6432-4cad-9d6e-b0cbedd6735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\cupy\\_environment.py:215: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CuPy version: 13.6.0\n",
      "✅ CUDA device: b'NVIDIA GeForce RTX 5060 Ti'\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import xgboost\n",
    "\n",
    "print(\"✅ CuPy version:\", cp.__version__)\n",
    "print(\"✅ CUDA device:\", cp.cuda.runtime.getDeviceProperties(0)['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198f0bdd-d1fd-4911-84ba-4c992d673b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Fitting scaler incrementally...\n",
      "⚙️ Transforming batches...\n",
      "\n",
      "🚀 Training XGBoost on GPU...\n",
      "[0]\tvalidation_0-aucpr:0.00697\n",
      "[100]\tvalidation_0-aucpr:0.03995\n",
      "[200]\tvalidation_0-aucpr:0.05081\n",
      "[300]\tvalidation_0-aucpr:0.06049\n",
      "[400]\tvalidation_0-aucpr:0.06832\n",
      "[500]\tvalidation_0-aucpr:0.07859\n",
      "[599]\tvalidation_0-aucpr:0.08588\n",
      "\n",
      "🔮 Running GPU inference...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'inplace_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔮 Running GPU inference...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m Xv_gpu = cp.asarray(X_val_s)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m proba_gpu = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m(Xv_gpu)\n\u001b[32m     63\u001b[39m proba = cp.asnumpy(proba_gpu)  \u001b[38;5;66;03m# back to CPU numpy\u001b[39;00m\n\u001b[32m     64\u001b[39m y_pred = (proba > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'XGBClassifier' object has no attribute 'inplace_predict'"
     ]
    }
   ],
   "source": [
    "# ✅ Full GPU-accelerated ML pipeline (training + evaluation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# === Load processed dataset ===\n",
    "df = pd.read_parquet(\"data/processed/HI_Small_fe.parquet\")\n",
    "label_col = \"Is Laundering\"\n",
    "\n",
    "# === Select numeric features ===\n",
    "num_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype in (\"int64\", \"float64\", \"float32\") and c != label_col\n",
    "]\n",
    "\n",
    "X = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "y = df[label_col].astype(int).values\n",
    "\n",
    "# === Train/validation split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Incremental normalization ===\n",
    "scaler = StandardScaler()\n",
    "batch = 200_000\n",
    "n = X_train.shape[0]\n",
    "\n",
    "print(\"📊 Fitting scaler incrementally...\")\n",
    "for i in range(0, n, batch):\n",
    "    scaler.partial_fit(X_train[i:i+batch])\n",
    "\n",
    "print(\"⚙️ Transforming batches...\")\n",
    "X_train_s = np.vstack([scaler.transform(X_train[i:i+batch]) for i in range(0, n, batch)])\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "# === GPU-based XGBoost training ===\n",
    "print(\"\\n🚀 Training XGBoost on GPU...\")\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",         # use hist algorithm\n",
    "    device=\"cuda\",              # train on GPU\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),  # class imbalance\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], verbose=100)\n",
    "\n",
    "# === GPU-based inference using CuPy ===\n",
    "print(\"\\n🔮 Running GPU inference...\")\n",
    "Xv_gpu = cp.asarray(X_val_s)\n",
    "proba_gpu = xgb.inplace_predict(Xv_gpu)\n",
    "proba = cp.asnumpy(proba_gpu)  # back to CPU numpy\n",
    "y_pred = (proba > 0.5).astype(int)\n",
    "\n",
    "# === Evaluation ===\n",
    "print(\"\\n✅ Model Evaluation Results:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce6153a-4319-45a7-98f0-60ab3dff3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 Running GPU inference...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CuPy failed to load nvrtc64_120_0.dll: FileNotFoundError: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\_softlink.pyx:25\u001b[39m, in \u001b[36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ctypes\\__init__.py:379\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get booster handle and run GPU prediction\u001b[39;00m\n\u001b[32m      8\u001b[39m booster = xgb.get_booster()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m proba_gpu = \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXv_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprobability\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert result back to CPU\u001b[39;00m\n\u001b[32m     12\u001b[39m proba = cp.asnumpy(proba_gpu)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:2589\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2577\u001b[39m     interface_str = _cuda_array_interface(data)\n\u001b[32m   2578\u001b[39m     _check_call(\n\u001b[32m   2579\u001b[39m         _LIB.XGBoosterPredictFromCudaArray(\n\u001b[32m   2580\u001b[39m             \u001b[38;5;28mself\u001b[39m.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prediction_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data):\n\u001b[32m   2591\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cudf_array_interfaces, _transform_cudf_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:484\u001b[39m, in \u001b[36m_prediction_output\u001b[39m\u001b[34m(shape, dims, predts, is_cuda)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prediction_output\u001b[39m(\n\u001b[32m    481\u001b[39m     shape: CNumericPtr, dims: c_bst_ulong, predts: CFloatPtr, is_cuda: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    482\u001b[39m ) -> NumpyOrCupy:\n\u001b[32m    483\u001b[39m     arr_shape = \u001b[38;5;28mtuple\u001b[39m(ctypes2numpy(shape, dims.value, np.uint64).flatten())\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     array = \u001b[43mfrom_array_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmake_array_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:444\u001b[39m, in \u001b[36mfrom_array_interface\u001b[39m\u001b[34m(interface)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m interface:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# CUDA stream is presented, this is a __cuda_array_interface__.\u001b[39;00m\n\u001b[32m    443\u001b[39m     arr.__cuda_array_interface__ = interface\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     out = \u001b[43mimport_cupy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    446\u001b[39m     arr.__array_interface__ = interface\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\cupy\\_creation\\from_data.py:53\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34marray\u001b[39m(obj, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, order=\u001b[33m'\u001b[39m\u001b[33mK\u001b[39m\u001b[33m'\u001b[39m, subok=\u001b[38;5;28;01mFalse\u001b[39;00m, ndmin=\u001b[32m0\u001b[39m, *,\n\u001b[32m      8\u001b[39m           blocking=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates an array on the current device.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33;03m    This function currently does not support the ``subok`` option.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2502\u001b[39m, in \u001b[36mcupy._core.core.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2515\u001b[39m, in \u001b[36mcupy._core.core.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2559\u001b[39m, in \u001b[36mcupy._core.core._array_from_cuda_array_interface\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2512\u001b[39m, in \u001b[36mcupy._core.core.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2543\u001b[39m, in \u001b[36mcupy._core.core._array_from_cupy_ndarray\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:618\u001b[39m, in \u001b[36mcupy._core.core._ndarray_base.astype\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:686\u001b[39m, in \u001b[36mcupy._core.core._ndarray_base.astype\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1374\u001b[39m, in \u001b[36mcupy._core._kernel.ufunc.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1401\u001b[39m, in \u001b[36mcupy._core._kernel.ufunc._get_ufunc_kernel\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1082\u001b[39m, in \u001b[36mcupy._core._kernel._get_ufunc_kernel\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:94\u001b[39m, in \u001b[36mcupy._core._kernel._get_simple_elementwise_kernel\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:82\u001b[39m, in \u001b[36mcupy._core._kernel._get_simple_elementwise_kernel_from_code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2375\u001b[39m, in \u001b[36mcupy._core.core.compile_with_cache\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:2320\u001b[39m, in \u001b[36mcupy._core.core.assemble_cupy_compiler_options\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\libs\\\\nvrtc.pyx:57\u001b[39m, in \u001b[36mcupy_backends.cuda.libs.nvrtc.getVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\libs\\\\_cnvrtc.pxi:72\u001b[39m, in \u001b[36mcupy_backends.cuda.libs.nvrtc.initialize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\libs\\\\_cnvrtc.pxi:75\u001b[39m, in \u001b[36mcupy_backends.cuda.libs.nvrtc._initialize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\libs\\\\_cnvrtc.pxi:153\u001b[39m, in \u001b[36mcupy_backends.cuda.libs.nvrtc._get_softlink\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends\\\\cuda\\\\_softlink.pyx:32\u001b[39m, in \u001b[36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: CuPy failed to load nvrtc64_120_0.dll: FileNotFoundError: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "# === GPU-based inference using CuPy (fixed for XGBoost 2.1+)\n",
    "print(\"\\n🔮 Running GPU inference...\")\n",
    "\n",
    "# Convert validation data to GPU tensor\n",
    "Xv_gpu = cp.asarray(X_val_s)\n",
    "\n",
    "# Get booster handle and run GPU prediction\n",
    "booster = xgb.get_booster()\n",
    "proba_gpu = booster.inplace_predict(Xv_gpu, predict_type=\"probability\")\n",
    "\n",
    "# Convert result back to CPU\n",
    "proba = cp.asnumpy(proba_gpu)\n",
    "y_pred = (proba > 0.5).astype(int)\n",
    "\n",
    "# === Evaluation ===\n",
    "print(\"\\n✅ Model Evaluation Results:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521b3524-9122-4628-9be4-3dffcd924ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.08040\n",
      "[100]\tvalidation_0-aucpr:0.28063\n",
      "[200]\tvalidation_0-aucpr:0.32650\n",
      "[300]\tvalidation_0-aucpr:0.34670\n",
      "[400]\tvalidation_0-aucpr:0.35745\n",
      "[500]\tvalidation_0-aucpr:0.37547\n",
      "[599]\tvalidation_0-aucpr:0.38591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\FFD_Thesis\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:06:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9742    0.9868   1014634\n",
      "           1     0.0291    0.7585    0.0560      1035\n",
      "\n",
      "    accuracy                         0.9739   1015669\n",
      "   macro avg     0.5144    0.8663    0.5214   1015669\n",
      "weighted avg     0.9988    0.9739    0.9858   1015669\n",
      "\n",
      "ROC-AUC: 0.963491652052749\n",
      "Confusion Matrix:\n",
      " [[988417  26217]\n",
      " [   250    785]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load a feature-engineered file (start with small; later you can do medium)\n",
    "df = pd.read_parquet(\"data/processed/HI_Small_fe.parquet\")\n",
    "label_col = \"Is Laundering\"\n",
    "\n",
    "# numeric features only\n",
    "num_cols = [c for c in df.columns if c != label_col and df[c].dtype.kind in \"if\"]\n",
    "X = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "y = df[label_col].astype(int).values\n",
    "\n",
    "# split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# memory-safe scaling\n",
    "scaler = StandardScaler()\n",
    "batch = 200_000\n",
    "for i in range(0, X_train.shape[0], batch):\n",
    "    scaler.partial_fit(X_train[i:i+batch])\n",
    "X_train_s = np.vstack([scaler.transform(X_train[i:i+batch]) for i in range(0, X_train.shape[0], batch)])\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "# GPU XGBoost (device='cuda'; tree_method='hist')\n",
    "xgb = XGBClassifier(\n",
    "    device=\"cuda\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),\n",
    "    random_state=42,\n",
    ")\n",
    "xgb.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], verbose=100)\n",
    "\n",
    "# Evaluation (CPU arrays are fine)\n",
    "proba = xgb.predict_proba(X_val_s)[:, 1]\n",
    "y_pred = (proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n✅ Model Evaluation\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63d4917-1ef0-48c7-a7c9-838b5044c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model, scaler, and metrics saved to /models and /reports/\n"
     ]
    }
   ],
   "source": [
    "import joblib, json, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# 1️⃣ Save model + scaler\n",
    "joblib.dump(xgb, \"models/xgb_hi_small_gpu.joblib\")\n",
    "joblib.dump(scaler, \"models/hi_small_scaler.joblib\")\n",
    "\n",
    "# 2️⃣ Plot evaluation curves\n",
    "RocCurveDisplay.from_predictions(y_val, proba)\n",
    "plt.title(\"ROC Curve (HI_Small GPU)\")\n",
    "plt.show()\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(y_val, proba)\n",
    "plt.title(\"Precision–Recall Curve (HI_Small GPU)\")\n",
    "plt.show()\n",
    "\n",
    "# 3️⃣ Log metrics to JSON\n",
    "metrics = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_val, proba)),\n",
    "    \"aucpr_final\": 0.3859,\n",
    "    \"recall\": 0.7585,\n",
    "    \"precision\": 0.0291,\n",
    "    \"f1\": 0.0560,\n",
    "    \"support\": int(len(y_val))\n",
    "}\n",
    "Path(\"reports\").mkdir(exist_ok=True)\n",
    "with open(\"reports/hi_small_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"✅ Model, scaler, and metrics saved to /models and /reports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e46bad-295d-4d46-b6cd-9110bb892daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small: loading HI_Small_fe.parquet ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SMOTE.__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m reports = []\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mHI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mLI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mHI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mLI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     reports.append(\u001b[43mprepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reports:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mprepare_one\u001b[39m\u001b[34m(fe_path, val_size, batch, smote_cap, seed)\u001b[39m\n\u001b[32m     84\u001b[39m pos_ct = \u001b[38;5;28mint\u001b[39m((y_sm_in == \u001b[32m1\u001b[39m).sum())\n\u001b[32m     85\u001b[39m k = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[32m5\u001b[39m, pos_ct - \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m sm = \u001b[43mSMOTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m X_train_bal, y_train_bal = sm.fit_resample(X_sm_in, y_sm_in)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# ---- Save outputs ----\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: SMOTE.__init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "# === Phase 2.8–2.9: Normalization + Balancing for ALL datasets (RAM-safe) ===\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib, gc\n",
    "\n",
    "PROC   = Path(\"data/processed\")\n",
    "ML_OUT = Path(\"data/ml\")\n",
    "ML_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABEL = \"Is Laundering\"\n",
    "DROP_NON_NUM = {\"Timestamp\", \"From Account\", \"To Account\"}  # non-numeric identifiers\n",
    "\n",
    "def numeric_cols(df, label=LABEL):\n",
    "    cols = [c for c in df.columns\n",
    "            if c not in DROP_NON_NUM | {label}\n",
    "            and np.issubdtype(df[c].dtype, np.number)]\n",
    "    return cols\n",
    "\n",
    "def balanced_subset_idx(y, cap_total=2_000_000, pos_keep_all=True, rng=42):\n",
    "    \"\"\"Return indices for a stratified (heavily positive) subset <= cap_total.\"\"\"\n",
    "    n = len(y)\n",
    "    if n <= cap_total:\n",
    "        return np.arange(n)\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "\n",
    "    if pos_keep_all:\n",
    "        n_pos = len(pos_idx)\n",
    "        rem = max(cap_total - n_pos, 0)\n",
    "        if rem == 0:\n",
    "            return pos_idx\n",
    "        rng = np.random.default_rng(rng)\n",
    "        choose_neg = rng.choice(neg_idx, size=min(rem, len(neg_idx)), replace=False)\n",
    "        return np.concatenate([pos_idx, choose_neg])\n",
    "    else:\n",
    "        # proportional sample\n",
    "        frac = cap_total / n\n",
    "        rng = np.random.default_rng(rng)\n",
    "        keep_mask = rng.random(n) < frac\n",
    "        return np.where(keep_mask)[0]\n",
    "\n",
    "def prepare_one(fe_path: Path, val_size=0.2, batch=200_000, smote_cap=2_000_000, seed=42):\n",
    "    name = fe_path.stem.replace(\"_fe\", \"\")\n",
    "    print(f\"\\n=== {name}: loading {fe_path.name} ===\")\n",
    "    df = pd.read_parquet(fe_path)\n",
    "    if LABEL not in df.columns:\n",
    "        raise ValueError(f\"{LABEL!r} not found in {fe_path.name}\")\n",
    "\n",
    "    feats = numeric_cols(df, LABEL)\n",
    "    X = df[feats].fillna(0.0).astype(\"float32\").values\n",
    "    y = df[LABEL].astype(int).values\n",
    "    del df; gc.collect()\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=val_size, random_state=seed, stratify=y\n",
    "    )\n",
    "    del X, y; gc.collect()\n",
    "\n",
    "    # ---- Incremental scaling (fit) ----\n",
    "    scaler = StandardScaler()\n",
    "    n = X_train.shape[0]\n",
    "    for i in range(0, n, batch):\n",
    "        scaler.partial_fit(X_train[i:i+batch])\n",
    "\n",
    "    # ---- Transform in batches ----\n",
    "    Xt_list = []\n",
    "    for i in range(0, n, batch):\n",
    "        Xt_list.append(scaler.transform(X_train[i:i+batch]))\n",
    "    X_train_s = np.vstack(Xt_list); del Xt_list\n",
    "\n",
    "    X_val_s = np.vstack([scaler.transform(X_val[i:i+batch]) for i in range(0, X_val.shape[0], batch)])\n",
    "\n",
    "    # ---- RAM-safe SMOTE on capped subset of training ----\n",
    "    idx = balanced_subset_idx(y_train, cap_total=smote_cap, pos_keep_all=True, rng=seed)\n",
    "    X_sm_in, y_sm_in = X_train_s[idx], y_train[idx]\n",
    "\n",
    "    # k_neighbors must be < positive_count\n",
    "    pos_ct = int((y_sm_in == 1).sum())\n",
    "    k = max(1, min(5, pos_ct - 1))\n",
    "    sm = SMOTE(random_state=seed, k_neighbors=k, n_jobs=-1)\n",
    "    X_train_bal, y_train_bal = sm.fit_resample(X_sm_in, y_sm_in)\n",
    "\n",
    "    # ---- Save outputs ----\n",
    "    joblib.dump(scaler, ML_OUT / f\"{name}_scaler.joblib\")\n",
    "    np.savez_compressed(ML_OUT / f\"{name}_train.npz\",\n",
    "                        X=X_train_bal.astype(\"float32\"), y=y_train_bal.astype(\"int8\"),\n",
    "                        feat_names=np.array(feats, dtype=object))\n",
    "    np.savez_compressed(ML_OUT / f\"{name}_val.npz\",\n",
    "                        X=X_val_s.astype(\"float32\"), y=y_val.astype(\"int8\"),\n",
    "                        feat_names=np.array(feats, dtype=object))\n",
    "\n",
    "    print(f\"  Train before SMOTE: {Counter(y_sm_in)}  → after: {Counter(y_train_bal)}\")\n",
    "    print(f\"  Saved: models/scaler={name}_scaler.joblib, ML sets={name}_train.npz / {name}_val.npz\")\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"train_orig\": Counter(y_train),\n",
    "        \"train_subset\": Counter(y_sm_in),\n",
    "        \"train_bal\": Counter(y_train_bal),\n",
    "        \"val\": Counter(y_val),\n",
    "        \"n_features\": len(feats)\n",
    "    }\n",
    "\n",
    "# Run for all four\n",
    "reports = []\n",
    "for fname in [\"HI_Small_fe.parquet\",\"LI_Small_fe.parquet\",\n",
    "              \"HI_Medium_fe.parquet\",\"LI_Medium_fe.parquet\"]:\n",
    "    reports.append(prepare_one(PROC / fname))\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for r in reports:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210a36e0-a2b2-485e-a527-01a38e259445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small: loading HI_Small_fe.parquet ===\n",
      "• Fitting scaler in batches on 4,062,676 rows ...\n",
      "• Transforming train/val ...\n",
      "• SMOTE k_neighbors=5 | positives in train=4,142\n",
      "  ↳ using stratified sample for SMOTE: 1,999,999 rows\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not int32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    143\u001b[39m reports = []\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mHI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mHI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     reports.append(\u001b[43mprepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Summary ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reports:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mprepare_one\u001b[39m\u001b[34m(fe_path, val_size, batch, smote_cap, seed)\u001b[39m\n\u001b[32m    119\u001b[39m report = {\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: name,\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_cols\u001b[39m\u001b[33m\"\u001b[39m: num_cols,\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msmote_pos_after\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m((y_train_bal == \u001b[32m1\u001b[39m).sum()),\n\u001b[32m    129\u001b[39m }\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(READY / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_prep_report.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m X_train, X_val, X_train_s, X_val_s, X_train_bal, y_train_bal, y_train, y_val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\json\\__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\json\\encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\json\\encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\json\\encoder.py:377\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mkeys must be str, int, float, bool or None, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    378\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[32m    380\u001b[39m     first = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: keys must be str, int, float, bool or None, not int32"
     ]
    }
   ],
   "source": [
    "# === Phase 2: Normalization + Balancing for ALL datasets (memory-safe) ===\n",
    "import gc, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "PROC = Path(\"data/processed\")         # input: *_fe.parquet\n",
    "READY = Path(\"data/ready\")            # output: scaled + balanced sets\n",
    "READY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABEL_COL = \"Is Laundering\"\n",
    "\n",
    "def _numeric_cols(df, drop=()):\n",
    "    \"\"\"Pick purely numeric columns for ML, excluding label and obvious non-features.\"\"\"\n",
    "    bad = set(drop) | {LABEL_COL, \"Timestamp\", \"From Account\", \"To Account\"}\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if c not in bad and pd.api.types.is_numeric_dtype(df[c])\n",
    "    ]\n",
    "\n",
    "def _to_float32(df, cols):\n",
    "    for c in cols:\n",
    "        if df[c].dtype != \"float32\":\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def prepare_one(fe_path: Path,\n",
    "                val_size: float = 0.20,\n",
    "                batch: int = 200_000,\n",
    "                smote_cap: int = 2_000_000,\n",
    "                seed: int = 42):\n",
    "    \"\"\"\n",
    "    - Loads feature parquet\n",
    "    - Stratified split\n",
    "    - Incremental z-score scaling\n",
    "    - SMOTE on train (capped for very large sets)\n",
    "    - Saves artifacts to data/ready/\n",
    "    \"\"\"\n",
    "    name = fe_path.name.replace(\"_fe.parquet\", \"\")\n",
    "    print(f\"\\n=== {name}: loading {fe_path.name} ===\")\n",
    "\n",
    "    # 1) Load minimal columns (label + numerics)\n",
    "    df = pd.read_parquet(fe_path)\n",
    "    if LABEL_COL not in df.columns:\n",
    "        raise ValueError(f\"{LABEL_COL} not found in {fe_path.name}\")\n",
    "    num_cols = _numeric_cols(df)\n",
    "    df = df[num_cols + [LABEL_COL]].copy()\n",
    "    df[LABEL_COL] = df[LABEL_COL].astype(\"int32\")\n",
    "    df = _to_float32(df, num_cols)\n",
    "\n",
    "    # 2) Train/val split (stratified)\n",
    "    X = df[num_cols].fillna(0.0).values\n",
    "    y = df[LABEL_COL].values\n",
    "    del df; gc.collect()\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=val_size, stratify=y, random_state=seed\n",
    "    )\n",
    "    del X, y; gc.collect()\n",
    "\n",
    "    # 3) Incremental scaler (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    n = X_train.shape[0]\n",
    "    print(f\"• Fitting scaler in batches on {n:,} rows ...\")\n",
    "    for i in range(0, n, batch):\n",
    "        scaler.partial_fit(X_train[i:i+batch])\n",
    "\n",
    "    print(\"• Transforming train/val ...\")\n",
    "    X_train_s = np.vstack([scaler.transform(X_train[i:i+batch])\n",
    "                           for i in range(0, n, batch)])\n",
    "    X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "    # 4) SMOTE on train (memory-safe)\n",
    "    pos_ct = int((y_train == 1).sum())\n",
    "    k = max(1, min(5, pos_ct - 1))  # SMOTE req: k < #positives\n",
    "    print(f\"• SMOTE k_neighbors={k} | positives in train={pos_ct:,}\")\n",
    "\n",
    "    # For massive sets, sample BEFORE SMOTE to keep RAM sane\n",
    "    if X_train_s.shape[0] > smote_cap:\n",
    "        # stratified sample ~smote_cap rows\n",
    "        frac = smote_cap / X_train_s.shape[0]\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx0 = np.where(y_train == 0)[0]\n",
    "        idx1 = np.where(y_train == 1)[0]\n",
    "        take0 = rng.choice(idx0, size=max(1, int(len(idx0)*frac)), replace=False)\n",
    "        take1 = rng.choice(idx1, size=max(1, int(len(idx1)*frac)), replace=False)\n",
    "        keep = np.concatenate([take0, take1])\n",
    "        X_sm_in = X_train_s[keep]\n",
    "        y_sm_in = y_train[keep]\n",
    "        print(f\"  ↳ using stratified sample for SMOTE: {X_sm_in.shape[0]:,} rows\")\n",
    "    else:\n",
    "        X_sm_in, y_sm_in = X_train_s, y_train\n",
    "\n",
    "    sm = SMOTE(random_state=seed, k_neighbors=k)   # <-- fixed: no n_jobs\n",
    "    X_train_bal, y_train_bal = sm.fit_resample(X_sm_in, y_sm_in)\n",
    "\n",
    " # ---- Save artifacts ----\n",
    "scaler_path = READY / f\"{name}_scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "npz_path = READY / f\"{name}_sets.npz\"\n",
    "np.savez_compressed(\n",
    "    npz_path,\n",
    "    num_cols=np.array(num_cols, dtype=object),\n",
    "    X_train_s=X_train_s, y_train=y_train,\n",
    "    X_val_s=X_val,       y_val=y_val,\n",
    "    X_train_bal=X_train_bal, y_train_bal=y_train_bal,\n",
    ")\n",
    "\n",
    "# cast Counter keys/values to builtin types for JSON\n",
    "def _cast_counts(d):\n",
    "    return {int(k): int(v) for k, v in d.items()}\n",
    "\n",
    "report = {\n",
    "    \"dataset\": name,\n",
    "    \"num_cols\": num_cols,\n",
    "    \"rows_train\": int(X_train_s.shape[0]),\n",
    "    \"rows_val\": int(X_val.shape[0]),\n",
    "    \"class_train\": _cast_counts(Counter(y_train)),\n",
    "    \"class_val\": _cast_counts(Counter(y_val)),\n",
    "    \"smote_input_rows\": int(X_sm_in.shape[0]),\n",
    "    \"smote_output_rows\": int(X_train_bal.shape[0]),\n",
    "    \"smote_pos_after\": int((y_train_bal == 1).sum()),\n",
    "}\n",
    "\n",
    "with open(READY / f\"{name}_prep_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Clean up\n",
    "del X_train, X_val, X_train_s, X_train_bal, y_train, y_val, y_train_bal\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✅ Saved: {npz_path.name}, {scaler_path.name}\")\n",
    "print(\"   Train class:\", report[\"class_train\"], \"| Val class:\", report[\"class_val\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1348234-7419-45d5-b4f0-49ac3f496eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Small: loading HI_Small_fe.parquet ===\n",
      "• Fitting scaler in batches on 4,062,676 rows ...\n",
      "• SMOTE k_neighbors=5 | positives in train=4142\n",
      "  ↳ using stratified sample for SMOTE: 2,000,000 rows\n",
      "✅ Saved: HI_Small_sets.npz, HI_Small_scaler.joblib\n",
      "   Train class: {0: 4058534, 1: 4142} | Val class: {0: 1014634, 1: 1035}\n",
      "\n",
      "=== LI_Small: loading LI_Small_fe.parquet ===\n",
      "• Fitting scaler in batches on 5,539,239 rows ...\n",
      "• SMOTE k_neighbors=5 | positives in train=2852\n",
      "  ↳ using stratified sample for SMOTE: 2,000,000 rows\n",
      "✅ Saved: LI_Small_sets.npz, LI_Small_scaler.joblib\n",
      "   Train class: {0: 5536387, 1: 2852} | Val class: {0: 1384097, 1: 713}\n",
      "\n",
      "=== HI_Medium: loading HI_Medium_fe.parquet ===\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (8, 31898510) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    153\u001b[39m reports = []\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mHI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLI_Small_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    155\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mHI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     reports.append(\u001b[43mprepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Summary (prep complete) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reports:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mprepare_one\u001b[39m\u001b[34m(fe_path, val_size, batch, smote_cap, seed)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo numeric columns found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfe_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# to float32 to reduce memory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m X_all = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m).values\n\u001b[32m     88\u001b[39m y_all = df[LABEL_COL].astype(\u001b[38;5;28mint\u001b[39m).values\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:7457\u001b[39m, in \u001b[36mNDFrame.fillna\u001b[39m\u001b[34m(self, value, method, axis, inplace, limit, downcast)\u001b[39m\n\u001b[32m   7455\u001b[39m         new_data = result._mgr\n\u001b[32m   7456\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7457\u001b[39m         new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7458\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\n\u001b[32m   7459\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7460\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   7461\u001b[39m     new_data = \u001b[38;5;28mself\u001b[39m.where(\u001b[38;5;28mself\u001b[39m.notna(), value)._mgr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\base.py:186\u001b[39m, in \u001b[36mDataManager.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[32m    184\u001b[39m     limit = libalgos.validate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit=limit)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfillna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1742\u001b[39m, in \u001b[36mBlock.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[39m\n\u001b[32m   1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m]\n\u001b[32m   1740\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1741\u001b[39m     \u001b[38;5;66;03m# GH#45423 consistent downcasting on no-ops.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     nb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1743\u001b[39m     nbs = nb._maybe_downcast(\n\u001b[32m   1744\u001b[39m         [nb], downcast=downcast, using_cow=using_cow, caller=\u001b[33m\"\u001b[39m\u001b[33mfillna\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1745\u001b[39m     )\n\u001b[32m   1746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nbs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    820\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.90 GiB for an array with shape (8, 31898510) and data type float64"
     ]
    }
   ],
   "source": [
    "# === Phase 2: Final data prep for ALL datasets (incremental scaling + capped SMOTE) ===\n",
    "# Runs on: HI_Small_fe, LI_Small_fe, HI_Medium_fe, LI_Medium_fe\n",
    "# Outputs per dataset:\n",
    "#   data/ready/<name>_sets.npz         (X_train_s, X_val, y_train, y_val, X_train_bal, y_train_bal, num_cols)\n",
    "#   data/ready/<name>_scaler.joblib     (StandardScaler)\n",
    "#   data/ready/<name>_prep_report.json  (human-readable summary)\n",
    "\n",
    "import os, gc, json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ---------- Paths ----------\n",
    "PROC  = Path(\"data/processed\")\n",
    "READY = Path(\"data/ready\")\n",
    "READY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Config ----------\n",
    "LABEL_COL   = \"Is Laundering\"\n",
    "VAL_SIZE    = 0.20\n",
    "BATCH       = 200_000          # incremental scaler batch size\n",
    "SMOTE_CAP   = 2_000_000        # cap rows for SMOTE (keeps memory in check)\n",
    "SEED        = 42\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def cast_counts(d):\n",
    "    \"\"\"Cast Counter keys/values to builtin int for JSON.\"\"\"\n",
    "    return {int(k): int(v) for k, v in d.items()}\n",
    "\n",
    "def incremental_fit_transform(scaler, X_train, batch=BATCH):\n",
    "    \"\"\"partial_fit on batches, then transform train/val in memory-safe chunks.\"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    for i in range(0, n, batch):\n",
    "        scaler.partial_fit(X_train[i:i+batch])\n",
    "    # transform train in chunks and stack\n",
    "    X_train_s_parts = []\n",
    "    for i in range(0, n, batch):\n",
    "        X_train_s_parts.append(scaler.transform(X_train[i:i+batch]))\n",
    "    X_train_s = np.vstack(X_train_s_parts)\n",
    "    del X_train_s_parts\n",
    "    gc.collect()\n",
    "    return X_train_s\n",
    "\n",
    "def stratified_cap_for_smote(X, y, cap=SMOTE_CAP, seed=SEED):\n",
    "    \"\"\"Downsample majority ONLY for SMOTE input, preserving all positives when possible.\"\"\"\n",
    "    if X.shape[0] <= cap:\n",
    "        return X, y\n",
    "    # keep all positives if we can\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "    np.random.default_rng(seed).shuffle(neg_idx)\n",
    "    keep_neg = cap - len(pos_idx)\n",
    "    if keep_neg < 1000:\n",
    "        # still enforce a floor of positives and negatives\n",
    "        keep_neg = max(keep_neg, 1000)\n",
    "    keep_neg = min(keep_neg, len(neg_idx))\n",
    "    sel = np.concatenate([pos_idx, neg_idx[:keep_neg]])\n",
    "    np.random.default_rng(seed).shuffle(sel)\n",
    "    return X[sel], y[sel]\n",
    "\n",
    "def prepare_one(fe_path: Path,\n",
    "                val_size=VAL_SIZE,\n",
    "                batch=BATCH,\n",
    "                smote_cap=SMOTE_CAP,\n",
    "                seed=SEED):\n",
    "    name = fe_path.stem.replace(\"_fe\", \"\")\n",
    "    print(f\"\\n=== {name}: loading {fe_path.name} ===\")\n",
    "    df = pd.read_parquet(fe_path)\n",
    "\n",
    "    # ----- pick numeric features safely -----\n",
    "    # exclude obvious non-numeric columns\n",
    "    drop_cols = {\"Timestamp\", \"From Account\", \"To Account\", LABEL_COL}\n",
    "    num_cols = [c for c in df.columns\n",
    "                if c not in drop_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not num_cols:\n",
    "        raise ValueError(f\"No numeric columns found in {fe_path.name}\")\n",
    "\n",
    "    # to float32 to reduce memory\n",
    "    X_all = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "    y_all = df[LABEL_COL].astype(int).values\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # ----- split -----\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_all, y_all, test_size=val_size, random_state=seed, stratify=y_all\n",
    "    )\n",
    "    del X_all, y_all\n",
    "    gc.collect()\n",
    "\n",
    "    # ----- incremental scaling -----\n",
    "    print(f\"• Fitting scaler in batches on {X_train.shape[0]:,} rows ...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = incremental_fit_transform(scaler, X_train, batch=batch)\n",
    "    X_val_s   = scaler.transform(X_val)\n",
    "\n",
    "    # ----- SMOTE with cap (memory-safe) -----\n",
    "    pos_ct = int((y_train == 1).sum())\n",
    "    k = max(1, min(5, pos_ct - 1))   # k_neighbors must be < positive count\n",
    "    print(f\"• SMOTE k_neighbors={k} | positives in train={pos_ct}\")\n",
    "\n",
    "    X_sm_in, y_sm_in = stratified_cap_for_smote(X_train_s, y_train, cap=smote_cap, seed=seed)\n",
    "    print(f\"  ↳ using stratified sample for SMOTE: {X_sm_in.shape[0]:,} rows\")\n",
    "\n",
    "    sm = SMOTE(random_state=seed, k_neighbors=k)\n",
    "    X_train_bal, y_train_bal = sm.fit_resample(X_sm_in, y_sm_in)\n",
    "\n",
    "    # ----- save artifacts -----\n",
    "    scaler_path = READY / f\"{name}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    npz_path = READY / f\"{name}_sets.npz\"\n",
    "    np.savez_compressed(\n",
    "        npz_path,\n",
    "        num_cols=np.array(num_cols, dtype=object),\n",
    "        X_train_s=X_train_s, y_train=y_train,\n",
    "        X_val_s=X_val_s,     y_val=y_val,\n",
    "        X_train_bal=X_train_bal, y_train_bal=y_train_bal,\n",
    "    )\n",
    "\n",
    "    # ----- write JSON report (cast keys to plain int) -----\n",
    "    report = {\n",
    "        \"dataset\": name,\n",
    "        \"num_features\": int(len(num_cols)),\n",
    "        \"rows_train\": int(X_train_s.shape[0]),\n",
    "        \"rows_val\": int(X_val_s.shape[0]),\n",
    "        \"class_train\": cast_counts(Counter(y_train)),\n",
    "        \"class_val\": cast_counts(Counter(y_val)),\n",
    "        \"smote_input_rows\": int(X_sm_in.shape[0]),\n",
    "        \"smote_output_rows\": int(X_train_bal.shape[0]),\n",
    "        \"smote_pos_after\": int((y_train_bal == 1).sum()),\n",
    "    }\n",
    "    with open(READY / f\"{name}_prep_report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # cleanup\n",
    "    del (X_train, X_val, X_train_s, X_val_s, X_sm_in, y_sm_in, X_train_bal, y_train_bal, y_train, y_val, scaler)\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"✅ Saved: {npz_path.name}, {scaler_path.name}\")\n",
    "    print(\"   Train class:\", report[\"class_train\"], \"| Val class:\", report[\"class_val\"])\n",
    "    return report\n",
    "\n",
    "# ---------- Run for all four datasets ----------\n",
    "reports = []\n",
    "for fname in [\"HI_Small_fe.parquet\", \"LI_Small_fe.parquet\",\n",
    "              \"HI_Medium_fe.parquet\", \"LI_Medium_fe.parquet\"]:\n",
    "    reports.append(prepare_one(PROC / fname))\n",
    "\n",
    "print(\"\\n=== Summary (prep complete) ===\")\n",
    "for r in reports:\n",
    "    print(\n",
    "        f\"{r['dataset']}: \"\n",
    "        f\"features={r['num_features']}, \"\n",
    "        f\"train={r['rows_train']:,}, val={r['rows_val']:,}, \"\n",
    "        f\"class_train={r['class_train']}, class_val={r['class_val']}, \"\n",
    "        f\"SMOTE in={r['smote_input_rows']:,} → out={r['smote_output_rows']:,} \"\n",
    "        f\"(pos_after={r['smote_pos_after']:,})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaf12f0-e9aa-4139-ac94-32f3cc65acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Medium: HI_Medium_fe.parquet ===\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (8, 31898510) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# Run only the medium sets here (the small ones are already done)\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mHI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLI_Medium_fe.parquet\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[43mprepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC\u001b[49m\u001b[43m/\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll medium datasets prepped (safe mode).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mprepare_one\u001b[39m\u001b[34m(fe_path)\u001b[39m\n\u001b[32m     52\u001b[39m num_cols = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m drop_cols \u001b[38;5;129;01mand\u001b[39;00m pd.api.types.is_numeric_dtype(df[c])]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m num_cols: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo numeric columns found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m.fillna(\u001b[32m0\u001b[39m).astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m).values\n\u001b[32m     56\u001b[39m y = df[LABEL_COL].astype(\u001b[38;5;28mint\u001b[39m).values\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m df; gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4128\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4128\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4131\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4132\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4133\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4134\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4136\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4175\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4164\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4166\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4167\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4168\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4173\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4174\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4175\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4176\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4155\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4150\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4151\u001b[39m     indices = np.arange(\n\u001b[32m   4152\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4153\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4161\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4162\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:913\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    910\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:699\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    706\u001b[39m     new_blocks = [\n\u001b[32m    707\u001b[39m         blk.take_nd(\n\u001b[32m    708\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    715\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:862\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    860\u001b[39m                     blocks.append(nb)\n\u001b[32m    861\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m                 blocks.append(nb)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\FFD_Thesis\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.90 GiB for an array with shape (8, 31898510) and data type float64"
     ]
    }
   ],
   "source": [
    "# save this as prep_medium_safe.py (in repo root), then run:  python prep_medium_safe.py\n",
    "import os, gc, json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "PROC  = Path(\"data/processed\")\n",
    "READY = Path(\"data/ready\"); READY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABEL_COL = \"Is Laundering\"\n",
    "VAL_SIZE  = 0.20\n",
    "\n",
    "# Tighter memory settings for medium sets\n",
    "BATCH      = 50_000          # smaller scaler batch\n",
    "SMOTE_CAP  = 400_000         # much lower cap for SMOTE input\n",
    "SEED       = 42\n",
    "\n",
    "def cast_counts(d): return {int(k): int(v) for k,v in d.items()}\n",
    "\n",
    "def incremental_fit_transform(scaler, X_train, batch):\n",
    "    n = X_train.shape[0]\n",
    "    for i in range(0, n, batch):\n",
    "        scaler.partial_fit(X_train[i:i+batch])\n",
    "    parts = []\n",
    "    for i in range(0, n, batch):\n",
    "        parts.append(scaler.transform(X_train[i:i+batch]))\n",
    "    Xs = np.vstack(parts); del parts; gc.collect()\n",
    "    return Xs\n",
    "\n",
    "def stratified_cap_for_smote(X, y, cap, seed):\n",
    "    if X.shape[0] <= cap: return X, y\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = np.where(y==1)[0]; neg = np.where(y==0)[0]\n",
    "    rng.shuffle(neg)\n",
    "    keep_neg = max(1000, cap - len(pos))\n",
    "    keep_neg = min(keep_neg, len(neg))\n",
    "    sel = np.concatenate([pos, neg[:keep_neg]])\n",
    "    rng.shuffle(sel)\n",
    "    return X[sel], y[sel]\n",
    "\n",
    "def prepare_one(fe_path: Path):\n",
    "    name = fe_path.stem.replace(\"_fe\",\"\")\n",
    "    print(f\"\\n=== {name}: {fe_path.name} ===\")\n",
    "    df = pd.read_parquet(fe_path)\n",
    "\n",
    "    drop_cols = {\"Timestamp\",\"From Account\",\"To Account\",LABEL_COL}\n",
    "    num_cols = [c for c in df.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not num_cols: raise ValueError(\"No numeric columns found\")\n",
    "\n",
    "    X = df[num_cols].fillna(0).astype(\"float32\").values\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    del df; gc.collect()\n",
    "\n",
    "    Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=VAL_SIZE, random_state=SEED, stratify=y)\n",
    "    del X, y; gc.collect()\n",
    "\n",
    "    print(f\"• Fitting scaler in batches (batch={BATCH}) on {Xtr.shape[0]:,} rows\")\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = incremental_fit_transform(scaler, Xtr, BATCH)\n",
    "    Xva_s = scaler.transform(Xva)\n",
    "\n",
    "    pos_ct = int((ytr==1).sum())\n",
    "    k = max(1, min(5, pos_ct-1))\n",
    "    print(f\"• SMOTE (k={k}) | train positives={pos_ct}\")\n",
    "    Xin, yin = stratified_cap_for_smote(Xtr_s, ytr, SMOTE_CAP, SEED)\n",
    "    print(f\"  ↳ SMOTE input capped to {Xin.shape[0]:,}\")\n",
    "\n",
    "    sm = SMOTE(random_state=SEED, k_neighbors=k)\n",
    "    Xbal, ybal = sm.fit_resample(Xin, yin)\n",
    "\n",
    "    scaler_path = READY/f\"{name}_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    npz_path = READY/f\"{name}_sets.npz\"\n",
    "    np.savez_compressed(\n",
    "        npz_path,\n",
    "        num_cols=np.array(num_cols, dtype=object),\n",
    "        X_train_s=Xtr_s, y_train=ytr,\n",
    "        X_val_s=Xva_s,   y_val=yva,\n",
    "        X_train_bal=Xbal, y_train_bal=ybal,\n",
    "    )\n",
    "\n",
    "    report = {\n",
    "        \"dataset\": name,\n",
    "        \"num_features\": int(len(num_cols)),\n",
    "        \"rows_train\": int(Xtr_s.shape[0]),\n",
    "        \"rows_val\": int(Xva_s.shape[0]),\n",
    "        \"class_train\": cast_counts(Counter(ytr)),\n",
    "        \"class_val\": cast_counts(Counter(yva)),\n",
    "        \"smote_input_rows\": int(Xin.shape[0]),\n",
    "        \"smote_output_rows\": int(Xbal.shape[0]),\n",
    "        \"smote_pos_after\": int((ybal==1).sum()),\n",
    "        \"settings\": {\"BATCH\": BATCH, \"SMOTE_CAP\": SMOTE_CAP}\n",
    "    }\n",
    "    with open(READY/f\"{name}_prep_report.json\",\"w\") as f: json.dump(report, f, indent=2)\n",
    "\n",
    "    # free memory\n",
    "    del Xtr, Xva, ytr, yva, Xtr_s, Xva_s, Xin, yin, Xbal, ybal, scaler\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"✅ Saved {npz_path.name} and {scaler_path.name}\")\n",
    "    print(\"   Train class:\", report[\"class_train\"], \"| Val class:\", report[\"class_val\"])\n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run only the medium sets here (the small ones are already done)\n",
    "    for fname in [\"HI_Medium_fe.parquet\", \"LI_Medium_fe.parquet\"]:\n",
    "        prepare_one(PROC/fname)\n",
    "    print(\"\\nAll medium datasets prepped (safe mode).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f71e02-9ff8-41f7-a432-2a7962fbd17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HI_Medium: HI_Medium_fe.parquet ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, gc, json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# === Paths ===\n",
    "PROC = Path(\"data/processed\")\n",
    "READY = Path(\"data/ready\")\n",
    "READY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABEL_COL = \"Is Laundering\"\n",
    "\n",
    "def prepare_one_chunked(fe_path, val_size=0.2, batch_size=500_000, smote_cap=2_000_000, seed=42):\n",
    "    name = fe_path.stem.replace(\"_fe\", \"\")\n",
    "    print(f\"\\n=== {name}: {fe_path.name} ===\")\n",
    "\n",
    "    # --- Read in chunks to get column names first ---\n",
    "    reader = pd.read_parquet(fe_path, engine=\"pyarrow\")\n",
    "    drop_cols = [\"Timestamp\", \"From Account\", \"To Account\", LABEL_COL]\n",
    "    num_cols = [c for c in reader.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(reader[c])]\n",
    "    del reader; gc.collect()\n",
    "\n",
    "    print(\"• Numeric columns:\", len(num_cols))\n",
    "\n",
    "    # --- Incremental fit scaler ---\n",
    "    scaler = StandardScaler()\n",
    "    total_rows = 0\n",
    "    for chunk in pd.read_parquet(fe_path, engine=\"pyarrow\", chunksize=batch_size):\n",
    "        Xb = chunk[num_cols].fillna(0).astype(\"float32\").values\n",
    "        scaler.partial_fit(Xb)\n",
    "        total_rows += len(chunk)\n",
    "        del Xb, chunk; gc.collect()\n",
    "    print(f\"• Scaler fitted on ~{total_rows:,} rows\")\n",
    "\n",
    "    # --- Transform in chunks and save scaled copy ---\n",
    "    scaled_path = READY / f\"{name}_scaled.parquet\"\n",
    "    if scaled_path.exists(): scaled_path.unlink()\n",
    "\n",
    "    for chunk in pd.read_parquet(fe_path, engine=\"pyarrow\", chunksize=batch_size):\n",
    "        Xb = chunk[num_cols].fillna(0).astype(\"float32\").values\n",
    "        Xb_s = scaler.transform(Xb)\n",
    "        chunk[num_cols] = Xb_s\n",
    "        chunk.to_parquet(scaled_path, index=False, append=True)\n",
    "        del Xb, Xb_s, chunk; gc.collect()\n",
    "\n",
    "    print(f\"✅ Saved scaled version → {scaled_path}\")\n",
    "\n",
    "    # --- Load small stratified sample for SMOTE balancing ---\n",
    "    df = pd.read_parquet(scaled_path)\n",
    "    y = df[LABEL_COL].astype(int)\n",
    "    X = df[num_cols]\n",
    "    del df; gc.collect()\n",
    "\n",
    "    # sample capped for SMOTE\n",
    "    frac = min(1.0, smote_cap / len(X))\n",
    "    Xs, _, ys, _ = train_test_split(X, y, train_size=frac, stratify=y, random_state=seed)\n",
    "    print(f\"• Using stratified sample: {len(Xs):,} rows for SMOTE\")\n",
    "\n",
    "    # --- Apply SMOTE (on GPU or CPU) ---\n",
    "    pos_ct = int((ys == 1).sum())\n",
    "    k = max(1, min(5, pos_ct - 1))\n",
    "    sm = SMOTE(random_state=seed, k_neighbors=k)\n",
    "    X_bal, y_bal = sm.fit_resample(Xs, ys)\n",
    "    print(f\"• After SMOTE: {Counter(y_bal)}\")\n",
    "\n",
    "    # --- Save ready version ---\n",
    "    out_path = READY / f\"{name}_ready.parquet\"\n",
    "    pd.concat([X_bal, pd.Series(y_bal, name=LABEL_COL)], axis=1).to_parquet(out_path, index=False)\n",
    "    print(f\"✅ Final saved → {out_path}\")\n",
    "\n",
    "    # --- Report ---\n",
    "    report = {\n",
    "        \"dataset\": str(name),\n",
    "        \"rows_after_smote\": int(len(y_bal)),\n",
    "        \"pos_after_smote\": int((y_bal == 1).sum()),\n",
    "        \"neg_after_smote\": int((y_bal == 0).sum()),\n",
    "        \"scaler_rows_seen\": int(total_rows)\n",
    "    }\n",
    "    with open(READY / f\"{name}_prep_report.json\", \"w\") as f:\n",
    "        json.dump({str(k): v for k, v in report.items()}, f, indent=2)\n",
    "\n",
    "    print(f\"📄 Report written → {name}_prep_report.json\")\n",
    "    del X, y, Xs, ys, X_bal, y_bal; gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fname in [\"HI_Medium_fe.parquet\", \"LI_Medium_fe.parquet\"]:\n",
    "        prepare_one_chunked(PROC / fname)\n",
    "    print(\"\\n✅ All medium datasets processed safely.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af7ac7f-9b83-482a-8f21-121ef3afac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HI_Medium_ready.parquet\n",
      "✅ LI_Medium_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ready = Path(\"data/ready\")\n",
    "for f in sorted(ready.glob(\"*_ready.parquet\")):\n",
    "    print(\"✅\", f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f30a2a-baf7-46be-8953-630d95a7fc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
